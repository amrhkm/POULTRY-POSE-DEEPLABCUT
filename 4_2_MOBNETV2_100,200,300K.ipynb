{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.2 MOBNETV2@100,200,300K.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxssaR45M3M72g1qC7DMFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrhkm/POULTRY-POSE-DEEPLABCUTV2/blob/main/4_2_MOBNETV2_100%2C200%2C300K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrcdtLPTZvzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f934c53-1224-4162-d7a6-9dd8222dd578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deeplabcut\n",
            "  Downloading deeplabcut-2.2.0.3-py3-none-any.whl (542 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 102 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 163 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 184 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 235 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 256 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 276 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 307 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 317 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 327 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 348 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 368 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 378 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 399 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 409 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 419 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 460 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 471 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 481 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 501 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 512 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 522 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 532 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.51.2)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.0.1)\n",
            "Collecting scikit-image<=0.18.1,>=0.17\n",
            "  Downloading scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.6.3)\n",
            "Collecting tensorpack\n",
            "  Downloading tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.4.4)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 76 kB/s \n",
            "\u001b[?25hCollecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 72.9 MB/s \n",
            "\u001b[?25hCollecting statsmodels>=0.11\n",
            "  Downloading statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.1.5)\n",
            "Collecting ruamel.yaml>=0.15.0\n",
            "  Downloading ruamel.yaml-0.17.19-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.2.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.62.3)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.7.0)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (5.5.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 68.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=0.18.1,>=0.17->deeplabcut) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=0.18.1,>=0.17->deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (0.11.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11->deeplabcut) (0.5.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.42.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (12.0.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.22.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->deeplabcut) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0->deeplabcut) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->deeplabcut) (0.34.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->deeplabcut) (0.7.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deeplabcut) (2.7.3)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (22.3.0)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (1.0.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (0.8.9)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=96efe04d8778831d322943b1440d541da3098bcddb3ab1dc9905c0b42da72800\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: scikit-image, ruamel.yaml.clib, msgpack-numpy, tf-slim, tensorpack, statsmodels, ruamel.yaml, opencv-python-headless, imgaug, filterpy, deeplabcut\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed deeplabcut-2.2.0.3 filterpy-1.4.5 imgaug-0.4.0 msgpack-numpy-0.4.7.1 opencv-python-headless-4.5.5.62 ruamel.yaml-0.17.19 ruamel.yaml.clib-0.2.6 scikit-image-0.18.1 statsmodels-0.13.1 tensorpack-0.11 tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install deeplabcut"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeSLdU4wm8-0",
        "outputId": "32afec09-5a18-44b5-b7a2-511017874c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "metadata": {
        "id": "dqddxMKhm_uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABcnmv8h2Tdt",
        "outputId": "ed4f2af6-9da9-4b84-a840-7d08154533b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplabcut"
      ],
      "metadata": {
        "id": "yrlt1BJdnBFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWfhxZhInDPp",
        "outputId": "0ccc3f51-f134-420b-bb42-c136e7098fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive\"\n",
        "ProjectFolderName = 'largewithaug-amir-2021-12-20'\n",
        "\n",
        "VideoType = 'mp4' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPhJst5kn4ly",
        "outputId": "e7fc6bf8-469c-4fc4-cca0-d4f8dc6e4279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AYAMFINAL-AMIR-2021-10-07      MULTITEST1-AMIR-2021-11-03\n",
            "'belum di trained'\t        temp\n",
            " CHICKEN5-AMIR-2021-10-09      'temp 2'\n",
            "'Colab Notebooks'\t       'TRAINED BACKUP1'\n",
            "'Copy of learning_stats.csv'   'UNTRAINED BU1 AND 2'\n",
            " DMANIP\t\t\t        VIDDATASET\n",
            " largewithaug-amir-2021-12-20   VIDDATASET2\n",
            "'MESSED UP'\t\t        VIDDATASET3\n",
            " MISC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = '/content/drive/MyDrive/'+ ProjectFolderName\n",
        "videofile_path = ['/content/drive/MyDrive/'+ProjectFolderName+'/videos/']\n",
        "path_config_file = '/content/drive/MyDrive/'+ProjectFolderName+'/config.yaml'\n",
        "\n",
        "print('NEW project_path: ' + project_path)\n",
        "print('NEW path_config_file: ' + path_config_file)\n",
        "print('NEW videofile_path: ' + videofile_path[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Rcv1SZn576",
        "outputId": "a5d5c97a-f8a8-4320-a139-1f050898da2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEW project_path: /content/drive/MyDrive/largewithaug-amir-2021-12-20\n",
            "NEW path_config_file: /content/drive/MyDrive/largewithaug-amir-2021-12-20/config.yaml\n",
            "NEW videofile_path: /content/drive/MyDrive/largewithaug-amir-2021-12-20/videos/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE EVERYTIME U START A NEW PROJECT\n",
        "!ls '/content/drive/MyDrive/largewithaug-amir-2021-12-20'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTck0T4poKKd",
        "outputId": "afc5066b-d51b-4300-86a5-8ec1e61f091b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.yaml  labeled-data  videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.create_training_dataset(path_config_file, net_type='mobilenet_v2_1.0', augmenter_type='imgaug')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eZ7b47coNDx",
        "outputId": "4104537b-1c9e-48a9-dc83-c5974a5968f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It appears that the images were labeled on a Windows system, but you are currently trying to create a training set on a Unix system. \n",
            " In this case the paths should be converted. Do you want to proceed with the conversion?\n",
            "yes/noyes\n",
            "Annotation data converted to unix format...\n",
            "Downloading a ImageNet-pretrained model from https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.8,\n",
              "  1,\n",
              "  (array([ 35, 108,  83, 239,  14, 152,  94, 120,  38, 144, 236, 150, 106,\n",
              "          151, 198,  61, 112,  63, 159, 230, 165, 101, 173, 157,  99, 251,\n",
              "           74,  77, 247, 105, 235, 199,  69, 241,   2, 200,  50, 234, 240,\n",
              "          102, 111, 219,  48,  88,  78,  43, 190, 141,  84, 185,  40,  65,\n",
              "          249,  45, 121, 140, 178,  98,  56, 147, 124, 184, 232,  59, 179,\n",
              "          195,  95, 207,  46, 228, 164,  11, 161, 139,  81, 134, 194,  15,\n",
              "           12, 254, 246,  10,  30, 245,  89, 129, 171, 188, 172, 116,  23,\n",
              "            6,   7, 107,  62, 220,  57, 169, 231,  76, 128,  44, 186, 183,\n",
              "          104, 131,  82,  66,  55, 145, 167,  75,  70, 238, 163, 202,  51,\n",
              "          224, 233, 181, 208, 115, 177, 182, 221, 209, 215,  27, 250, 160,\n",
              "           90, 138, 126,  47, 193, 253,  39, 109,  42, 225, 244,  31, 154,\n",
              "            9,   3, 136, 119, 162, 248,  17, 242, 197, 125,  49, 133, 187,\n",
              "          229,  53,  67,  13,  86,  97, 114, 203,  24, 122,  72,   1,  73,\n",
              "          170, 214,   5, 216, 166, 110,  60,  16,  87,  92, 222, 148, 174,\n",
              "            4, 132, 205, 153, 156, 226,  34,  18, 237, 217, 137, 168,  41,\n",
              "           71, 142, 255,  29,  96, 243,  20,  54, 135]),\n",
              "   array([223, 213, 180, 191, 176,  33,   0,  19,  22,  25,  64, 227,  37,\n",
              "           36,  79, 192,  28, 149, 204, 143, 103,  21, 218, 117, 175,  91,\n",
              "           58, 211, 206,  52,  26, 158,  85, 155,  32, 210, 130,   8, 127,\n",
              "          100, 113, 123, 252,  68, 189,  80,  93, 212, 118, 196, 201, 146])))]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=100, saveiters=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HBdr0vMhoaLQ",
        "outputId": "a864f154-981f-4337-a46a-586104793507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'clahe': True,\n",
            " 'claheratio': 0.1,\n",
            " 'crop_pad': 0,\n",
            " 'crop_sampling': 'hybrid',\n",
            " 'crop_size': [400, 400],\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'edge': False,\n",
            " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'histeq': True,\n",
            " 'histeqratio': 0.1,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'max_shift': 0.4,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/Documentation_data-largewithaug_80shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'pre_resize': [],\n",
            " 'project_path': '/content/drive/MyDrive/largewithaug-amir-2021-12-20',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'sharpen': False,\n",
            " 'sharpenratio': 0.3,\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting single-animal trainer\n",
            "Batch Size is 1\n",
            "Loading ImageNet-pretrained mobilenet_v2_1.0\n",
            "Display_iters overwritten as 100\n",
            "Save_iters overwritten as 100\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4]], 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'crop_sampling': 'hybrid', 'crop_size': [400, 400], 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'max_shift': 0.4, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/Documentation_data-largewithaug_80shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'mobilenet_v2_1.0', 'num_joints': 5, 'pos_dist_thresh': 17, 'pre_resize': [], 'project_path': '/content/drive/MyDrive/largewithaug-amir-2021-12-20', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "iteration: 100 loss: 0.1719 lr: 0.005\n",
            "iteration: 200 loss: 0.0562 lr: 0.005\n",
            "iteration: 300 loss: 0.0474 lr: 0.005\n",
            "iteration: 400 loss: 0.0530 lr: 0.005\n",
            "iteration: 500 loss: 0.0467 lr: 0.005\n",
            "iteration: 600 loss: 0.0408 lr: 0.005\n",
            "iteration: 700 loss: 0.0430 lr: 0.005\n",
            "iteration: 800 loss: 0.0485 lr: 0.005\n",
            "iteration: 900 loss: 0.0378 lr: 0.005\n",
            "iteration: 1000 loss: 0.0404 lr: 0.005\n",
            "iteration: 1100 loss: 0.0372 lr: 0.005\n",
            "iteration: 1200 loss: 0.0381 lr: 0.005\n",
            "iteration: 1300 loss: 0.0375 lr: 0.005\n",
            "iteration: 1400 loss: 0.0355 lr: 0.005\n",
            "iteration: 1500 loss: 0.0365 lr: 0.005\n",
            "iteration: 1600 loss: 0.0384 lr: 0.005\n",
            "iteration: 1700 loss: 0.0364 lr: 0.005\n",
            "iteration: 1800 loss: 0.0332 lr: 0.005\n",
            "iteration: 1900 loss: 0.0371 lr: 0.005\n",
            "iteration: 2000 loss: 0.0423 lr: 0.005\n",
            "iteration: 2100 loss: 0.0353 lr: 0.005\n",
            "iteration: 2200 loss: 0.0362 lr: 0.005\n",
            "iteration: 2300 loss: 0.0322 lr: 0.005\n",
            "iteration: 2400 loss: 0.0379 lr: 0.005\n",
            "iteration: 2500 loss: 0.0324 lr: 0.005\n",
            "iteration: 2600 loss: 0.0320 lr: 0.005\n",
            "iteration: 2700 loss: 0.0366 lr: 0.005\n",
            "iteration: 2800 loss: 0.0272 lr: 0.005\n",
            "iteration: 2900 loss: 0.0321 lr: 0.005\n",
            "iteration: 3000 loss: 0.0323 lr: 0.005\n",
            "iteration: 3100 loss: 0.0342 lr: 0.005\n",
            "iteration: 3200 loss: 0.0289 lr: 0.005\n",
            "iteration: 3300 loss: 0.0337 lr: 0.005\n",
            "iteration: 3400 loss: 0.0275 lr: 0.005\n",
            "iteration: 3500 loss: 0.0332 lr: 0.005\n",
            "iteration: 3600 loss: 0.0306 lr: 0.005\n",
            "iteration: 3700 loss: 0.0317 lr: 0.005\n",
            "iteration: 3800 loss: 0.0328 lr: 0.005\n",
            "iteration: 3900 loss: 0.0331 lr: 0.005\n",
            "iteration: 4000 loss: 0.0255 lr: 0.005\n",
            "iteration: 4100 loss: 0.0299 lr: 0.005\n",
            "iteration: 4200 loss: 0.0267 lr: 0.005\n",
            "iteration: 4300 loss: 0.0290 lr: 0.005\n",
            "iteration: 4400 loss: 0.0254 lr: 0.005\n",
            "iteration: 4500 loss: 0.0316 lr: 0.005\n",
            "iteration: 4600 loss: 0.0266 lr: 0.005\n",
            "iteration: 4700 loss: 0.0247 lr: 0.005\n",
            "iteration: 4800 loss: 0.0280 lr: 0.005\n",
            "iteration: 4900 loss: 0.0296 lr: 0.005\n",
            "iteration: 5000 loss: 0.0269 lr: 0.005\n",
            "iteration: 5100 loss: 0.0270 lr: 0.005\n",
            "iteration: 5200 loss: 0.0264 lr: 0.005\n",
            "iteration: 5300 loss: 0.0305 lr: 0.005\n",
            "iteration: 5400 loss: 0.0260 lr: 0.005\n",
            "iteration: 5500 loss: 0.0286 lr: 0.005\n",
            "iteration: 5600 loss: 0.0250 lr: 0.005\n",
            "iteration: 5700 loss: 0.0242 lr: 0.005\n",
            "iteration: 5800 loss: 0.0264 lr: 0.005\n",
            "iteration: 5900 loss: 0.0231 lr: 0.005\n",
            "iteration: 6000 loss: 0.0246 lr: 0.005\n",
            "iteration: 6100 loss: 0.0250 lr: 0.005\n",
            "iteration: 6200 loss: 0.0263 lr: 0.005\n",
            "iteration: 6300 loss: 0.0264 lr: 0.005\n",
            "iteration: 6400 loss: 0.0251 lr: 0.005\n",
            "iteration: 6500 loss: 0.0251 lr: 0.005\n",
            "iteration: 6600 loss: 0.0226 lr: 0.005\n",
            "iteration: 6700 loss: 0.0258 lr: 0.005\n",
            "iteration: 6800 loss: 0.0250 lr: 0.005\n",
            "iteration: 6900 loss: 0.0282 lr: 0.005\n",
            "iteration: 7000 loss: 0.0248 lr: 0.005\n",
            "iteration: 7100 loss: 0.0229 lr: 0.005\n",
            "iteration: 7200 loss: 0.0285 lr: 0.005\n",
            "iteration: 7300 loss: 0.0236 lr: 0.005\n",
            "iteration: 7400 loss: 0.0243 lr: 0.005\n",
            "iteration: 7500 loss: 0.0237 lr: 0.005\n",
            "iteration: 7600 loss: 0.0226 lr: 0.005\n",
            "iteration: 7700 loss: 0.0227 lr: 0.005\n",
            "iteration: 7800 loss: 0.0230 lr: 0.005\n",
            "iteration: 7900 loss: 0.0218 lr: 0.005\n",
            "iteration: 8000 loss: 0.0218 lr: 0.005\n",
            "iteration: 8100 loss: 0.0257 lr: 0.005\n",
            "iteration: 8200 loss: 0.0197 lr: 0.005\n",
            "iteration: 8300 loss: 0.0255 lr: 0.005\n",
            "iteration: 8400 loss: 0.0243 lr: 0.005\n",
            "iteration: 8500 loss: 0.0233 lr: 0.005\n",
            "iteration: 8600 loss: 0.0241 lr: 0.005\n",
            "iteration: 8700 loss: 0.0194 lr: 0.005\n",
            "iteration: 8800 loss: 0.0212 lr: 0.005\n",
            "iteration: 8900 loss: 0.0260 lr: 0.005\n",
            "iteration: 9000 loss: 0.0230 lr: 0.005\n",
            "iteration: 9100 loss: 0.0212 lr: 0.005\n",
            "iteration: 9200 loss: 0.0242 lr: 0.005\n",
            "iteration: 9300 loss: 0.0239 lr: 0.005\n",
            "iteration: 9400 loss: 0.0250 lr: 0.005\n",
            "iteration: 9500 loss: 0.0234 lr: 0.005\n",
            "iteration: 9600 loss: 0.0224 lr: 0.005\n",
            "iteration: 9700 loss: 0.0220 lr: 0.005\n",
            "iteration: 9800 loss: 0.0211 lr: 0.005\n",
            "iteration: 9900 loss: 0.0219 lr: 0.005\n",
            "iteration: 10000 loss: 0.0207 lr: 0.005\n",
            "iteration: 10100 loss: 0.0217 lr: 0.02\n",
            "iteration: 10200 loss: 0.0246 lr: 0.02\n",
            "iteration: 10300 loss: 0.0230 lr: 0.02\n",
            "iteration: 10400 loss: 0.0272 lr: 0.02\n",
            "iteration: 10500 loss: 0.0222 lr: 0.02\n",
            "iteration: 10600 loss: 0.0235 lr: 0.02\n",
            "iteration: 10700 loss: 0.0250 lr: 0.02\n",
            "iteration: 10800 loss: 0.0242 lr: 0.02\n",
            "iteration: 10900 loss: 0.0240 lr: 0.02\n",
            "iteration: 11000 loss: 0.0234 lr: 0.02\n",
            "iteration: 11100 loss: 0.0222 lr: 0.02\n",
            "iteration: 11200 loss: 0.0205 lr: 0.02\n",
            "iteration: 11300 loss: 0.0205 lr: 0.02\n",
            "iteration: 11400 loss: 0.0204 lr: 0.02\n",
            "iteration: 11500 loss: 0.0185 lr: 0.02\n",
            "iteration: 11600 loss: 0.0193 lr: 0.02\n",
            "iteration: 11700 loss: 0.0195 lr: 0.02\n",
            "iteration: 11800 loss: 0.0231 lr: 0.02\n",
            "iteration: 11900 loss: 0.0200 lr: 0.02\n",
            "iteration: 12000 loss: 0.0209 lr: 0.02\n",
            "iteration: 12100 loss: 0.0183 lr: 0.02\n",
            "iteration: 12200 loss: 0.0186 lr: 0.02\n",
            "iteration: 12300 loss: 0.0205 lr: 0.02\n",
            "iteration: 12400 loss: 0.0211 lr: 0.02\n",
            "iteration: 12500 loss: 0.0180 lr: 0.02\n",
            "iteration: 12600 loss: 0.0190 lr: 0.02\n",
            "iteration: 12700 loss: 0.0211 lr: 0.02\n",
            "iteration: 12800 loss: 0.0180 lr: 0.02\n",
            "iteration: 12900 loss: 0.0191 lr: 0.02\n",
            "iteration: 13000 loss: 0.0193 lr: 0.02\n",
            "iteration: 13100 loss: 0.0208 lr: 0.02\n",
            "iteration: 13200 loss: 0.0188 lr: 0.02\n",
            "iteration: 13300 loss: 0.0164 lr: 0.02\n",
            "iteration: 13400 loss: 0.0194 lr: 0.02\n",
            "iteration: 13500 loss: 0.0187 lr: 0.02\n",
            "iteration: 13600 loss: 0.0190 lr: 0.02\n",
            "iteration: 13700 loss: 0.0185 lr: 0.02\n",
            "iteration: 13800 loss: 0.0192 lr: 0.02\n",
            "iteration: 13900 loss: 0.0183 lr: 0.02\n",
            "iteration: 14000 loss: 0.0181 lr: 0.02\n",
            "iteration: 14100 loss: 0.0172 lr: 0.02\n",
            "iteration: 14200 loss: 0.0168 lr: 0.02\n",
            "iteration: 14300 loss: 0.0146 lr: 0.02\n",
            "iteration: 14400 loss: 0.0164 lr: 0.02\n",
            "iteration: 14500 loss: 0.0171 lr: 0.02\n",
            "iteration: 14600 loss: 0.0142 lr: 0.02\n",
            "iteration: 14700 loss: 0.0157 lr: 0.02\n",
            "iteration: 14800 loss: 0.0150 lr: 0.02\n",
            "iteration: 14900 loss: 0.0153 lr: 0.02\n",
            "iteration: 15000 loss: 0.0155 lr: 0.02\n",
            "iteration: 15100 loss: 0.0166 lr: 0.02\n",
            "iteration: 15200 loss: 0.0162 lr: 0.02\n",
            "iteration: 15300 loss: 0.0146 lr: 0.02\n",
            "iteration: 15400 loss: 0.0154 lr: 0.02\n",
            "iteration: 15500 loss: 0.0138 lr: 0.02\n",
            "iteration: 15600 loss: 0.0158 lr: 0.02\n",
            "iteration: 15700 loss: 0.0180 lr: 0.02\n",
            "iteration: 15800 loss: 0.0135 lr: 0.02\n",
            "iteration: 15900 loss: 0.0158 lr: 0.02\n",
            "iteration: 16000 loss: 0.0157 lr: 0.02\n",
            "iteration: 16100 loss: 0.0153 lr: 0.02\n",
            "iteration: 16200 loss: 0.0143 lr: 0.02\n",
            "iteration: 16300 loss: 0.0148 lr: 0.02\n",
            "iteration: 16400 loss: 0.0160 lr: 0.02\n",
            "iteration: 16500 loss: 0.0140 lr: 0.02\n",
            "iteration: 16600 loss: 0.0133 lr: 0.02\n",
            "iteration: 16700 loss: 0.0133 lr: 0.02\n",
            "iteration: 16800 loss: 0.0130 lr: 0.02\n",
            "iteration: 16900 loss: 0.0141 lr: 0.02\n",
            "iteration: 17000 loss: 0.0142 lr: 0.02\n",
            "iteration: 17100 loss: 0.0115 lr: 0.02\n",
            "iteration: 17200 loss: 0.0126 lr: 0.02\n",
            "iteration: 17300 loss: 0.0135 lr: 0.02\n",
            "iteration: 17400 loss: 0.0148 lr: 0.02\n",
            "iteration: 17500 loss: 0.0131 lr: 0.02\n",
            "iteration: 17600 loss: 0.0143 lr: 0.02\n",
            "iteration: 17700 loss: 0.0119 lr: 0.02\n",
            "iteration: 17800 loss: 0.0159 lr: 0.02\n",
            "iteration: 17900 loss: 0.0140 lr: 0.02\n",
            "iteration: 18000 loss: 0.0130 lr: 0.02\n",
            "iteration: 18100 loss: 0.0135 lr: 0.02\n",
            "iteration: 18200 loss: 0.0131 lr: 0.02\n",
            "iteration: 18300 loss: 0.0133 lr: 0.02\n",
            "iteration: 18400 loss: 0.0133 lr: 0.02\n",
            "iteration: 18500 loss: 0.0153 lr: 0.02\n",
            "iteration: 18600 loss: 0.0135 lr: 0.02\n",
            "iteration: 18700 loss: 0.0147 lr: 0.02\n",
            "iteration: 18800 loss: 0.0138 lr: 0.02\n",
            "iteration: 18900 loss: 0.0132 lr: 0.02\n",
            "iteration: 19000 loss: 0.0130 lr: 0.02\n",
            "iteration: 19100 loss: 0.0142 lr: 0.02\n",
            "iteration: 19200 loss: 0.0132 lr: 0.02\n",
            "iteration: 19300 loss: 0.0135 lr: 0.02\n",
            "iteration: 19400 loss: 0.0120 lr: 0.02\n",
            "iteration: 19500 loss: 0.0138 lr: 0.02\n",
            "iteration: 19600 loss: 0.0120 lr: 0.02\n",
            "iteration: 19700 loss: 0.0119 lr: 0.02\n",
            "iteration: 19800 loss: 0.0116 lr: 0.02\n",
            "iteration: 19900 loss: 0.0130 lr: 0.02\n",
            "iteration: 20000 loss: 0.0128 lr: 0.02\n",
            "iteration: 20100 loss: 0.0109 lr: 0.02\n",
            "iteration: 20200 loss: 0.0129 lr: 0.02\n",
            "iteration: 20300 loss: 0.0133 lr: 0.02\n",
            "iteration: 20400 loss: 0.0148 lr: 0.02\n",
            "iteration: 20500 loss: 0.0120 lr: 0.02\n",
            "iteration: 20600 loss: 0.0156 lr: 0.02\n",
            "iteration: 20700 loss: 0.0137 lr: 0.02\n",
            "iteration: 20800 loss: 0.0129 lr: 0.02\n",
            "iteration: 20900 loss: 0.0117 lr: 0.02\n",
            "iteration: 21000 loss: 0.0113 lr: 0.02\n",
            "iteration: 21100 loss: 0.0145 lr: 0.02\n",
            "iteration: 21200 loss: 0.0121 lr: 0.02\n",
            "iteration: 21300 loss: 0.0141 lr: 0.02\n",
            "iteration: 21400 loss: 0.0112 lr: 0.02\n",
            "iteration: 21500 loss: 0.0126 lr: 0.02\n",
            "iteration: 21600 loss: 0.0110 lr: 0.02\n",
            "iteration: 21700 loss: 0.0115 lr: 0.02\n",
            "iteration: 21800 loss: 0.0111 lr: 0.02\n",
            "iteration: 21900 loss: 0.0129 lr: 0.02\n",
            "iteration: 22000 loss: 0.0144 lr: 0.02\n",
            "iteration: 22100 loss: 0.0113 lr: 0.02\n",
            "iteration: 22200 loss: 0.0136 lr: 0.02\n",
            "iteration: 22300 loss: 0.0121 lr: 0.02\n",
            "iteration: 22400 loss: 0.0114 lr: 0.02\n",
            "iteration: 22500 loss: 0.0118 lr: 0.02\n",
            "iteration: 22600 loss: 0.0116 lr: 0.02\n",
            "iteration: 22700 loss: 0.0109 lr: 0.02\n",
            "iteration: 22800 loss: 0.0125 lr: 0.02\n",
            "iteration: 22900 loss: 0.0108 lr: 0.02\n",
            "iteration: 23000 loss: 0.0105 lr: 0.02\n",
            "iteration: 23100 loss: 0.0110 lr: 0.02\n",
            "iteration: 23200 loss: 0.0114 lr: 0.02\n",
            "iteration: 23300 loss: 0.0116 lr: 0.02\n",
            "iteration: 23400 loss: 0.0110 lr: 0.02\n",
            "iteration: 23500 loss: 0.0110 lr: 0.02\n",
            "iteration: 23600 loss: 0.0119 lr: 0.02\n",
            "iteration: 23700 loss: 0.0098 lr: 0.02\n",
            "iteration: 23800 loss: 0.0103 lr: 0.02\n",
            "iteration: 23900 loss: 0.0129 lr: 0.02\n",
            "iteration: 24000 loss: 0.0097 lr: 0.02\n",
            "iteration: 24100 loss: 0.0099 lr: 0.02\n",
            "iteration: 24200 loss: 0.0104 lr: 0.02\n",
            "iteration: 24300 loss: 0.0107 lr: 0.02\n",
            "iteration: 24400 loss: 0.0111 lr: 0.02\n",
            "iteration: 24500 loss: 0.0115 lr: 0.02\n",
            "iteration: 24600 loss: 0.0113 lr: 0.02\n",
            "iteration: 24700 loss: 0.0100 lr: 0.02\n",
            "iteration: 24800 loss: 0.0112 lr: 0.02\n",
            "iteration: 24900 loss: 0.0111 lr: 0.02\n",
            "iteration: 25000 loss: 0.0107 lr: 0.02\n",
            "iteration: 25100 loss: 0.0104 lr: 0.02\n",
            "iteration: 25200 loss: 0.0100 lr: 0.02\n",
            "iteration: 25300 loss: 0.0099 lr: 0.02\n",
            "iteration: 25400 loss: 0.0098 lr: 0.02\n",
            "iteration: 25500 loss: 0.0114 lr: 0.02\n",
            "iteration: 25600 loss: 0.0096 lr: 0.02\n",
            "iteration: 25700 loss: 0.0102 lr: 0.02\n",
            "iteration: 25800 loss: 0.0108 lr: 0.02\n",
            "iteration: 25900 loss: 0.0112 lr: 0.02\n",
            "iteration: 26000 loss: 0.0099 lr: 0.02\n",
            "iteration: 26100 loss: 0.0102 lr: 0.02\n",
            "iteration: 26200 loss: 0.0111 lr: 0.02\n",
            "iteration: 26300 loss: 0.0099 lr: 0.02\n",
            "iteration: 26400 loss: 0.0103 lr: 0.02\n",
            "iteration: 26500 loss: 0.0091 lr: 0.02\n",
            "iteration: 26600 loss: 0.0109 lr: 0.02\n",
            "iteration: 26700 loss: 0.0111 lr: 0.02\n",
            "iteration: 26800 loss: 0.0095 lr: 0.02\n",
            "iteration: 26900 loss: 0.0123 lr: 0.02\n",
            "iteration: 27000 loss: 0.0095 lr: 0.02\n",
            "iteration: 27100 loss: 0.0096 lr: 0.02\n",
            "iteration: 27200 loss: 0.0107 lr: 0.02\n",
            "iteration: 27300 loss: 0.0121 lr: 0.02\n",
            "iteration: 27400 loss: 0.0113 lr: 0.02\n",
            "iteration: 27500 loss: 0.0089 lr: 0.02\n",
            "iteration: 27600 loss: 0.0095 lr: 0.02\n",
            "iteration: 27700 loss: 0.0101 lr: 0.02\n",
            "iteration: 27800 loss: 0.0105 lr: 0.02\n",
            "iteration: 27900 loss: 0.0111 lr: 0.02\n",
            "iteration: 28000 loss: 0.0098 lr: 0.02\n",
            "iteration: 28100 loss: 0.0095 lr: 0.02\n",
            "iteration: 28200 loss: 0.0086 lr: 0.02\n",
            "iteration: 28300 loss: 0.0085 lr: 0.02\n",
            "iteration: 28400 loss: 0.0089 lr: 0.02\n",
            "iteration: 28500 loss: 0.0093 lr: 0.02\n",
            "iteration: 28600 loss: 0.0087 lr: 0.02\n",
            "iteration: 28700 loss: 0.0091 lr: 0.02\n",
            "iteration: 28800 loss: 0.0094 lr: 0.02\n",
            "iteration: 28900 loss: 0.0095 lr: 0.02\n",
            "iteration: 29000 loss: 0.0096 lr: 0.02\n",
            "iteration: 29100 loss: 0.0096 lr: 0.02\n",
            "iteration: 29200 loss: 0.0124 lr: 0.02\n",
            "iteration: 29300 loss: 0.0113 lr: 0.02\n",
            "iteration: 29400 loss: 0.0090 lr: 0.02\n",
            "iteration: 29500 loss: 0.0089 lr: 0.02\n",
            "iteration: 29600 loss: 0.0113 lr: 0.02\n",
            "iteration: 29700 loss: 0.0103 lr: 0.02\n",
            "iteration: 29800 loss: 0.0092 lr: 0.02\n",
            "iteration: 29900 loss: 0.0098 lr: 0.02\n",
            "iteration: 30000 loss: 0.0099 lr: 0.02\n",
            "iteration: 30100 loss: 0.0078 lr: 0.02\n",
            "iteration: 30200 loss: 0.0094 lr: 0.02\n",
            "iteration: 30300 loss: 0.0105 lr: 0.02\n",
            "iteration: 30400 loss: 0.0092 lr: 0.02\n",
            "iteration: 30500 loss: 0.0112 lr: 0.02\n",
            "iteration: 30600 loss: 0.0107 lr: 0.02\n",
            "iteration: 30700 loss: 0.0092 lr: 0.02\n",
            "iteration: 30800 loss: 0.0107 lr: 0.02\n",
            "iteration: 30900 loss: 0.0092 lr: 0.02\n",
            "iteration: 31000 loss: 0.0102 lr: 0.02\n",
            "iteration: 31100 loss: 0.0085 lr: 0.02\n",
            "iteration: 31200 loss: 0.0091 lr: 0.02\n",
            "iteration: 31300 loss: 0.0098 lr: 0.02\n",
            "iteration: 31400 loss: 0.0083 lr: 0.02\n",
            "iteration: 31500 loss: 0.0093 lr: 0.02\n",
            "iteration: 31600 loss: 0.0089 lr: 0.02\n",
            "iteration: 31700 loss: 0.0092 lr: 0.02\n",
            "iteration: 31800 loss: 0.0094 lr: 0.02\n",
            "iteration: 31900 loss: 0.0095 lr: 0.02\n",
            "iteration: 32000 loss: 0.0089 lr: 0.02\n",
            "iteration: 32100 loss: 0.0096 lr: 0.02\n",
            "iteration: 32200 loss: 0.0104 lr: 0.02\n",
            "iteration: 32300 loss: 0.0108 lr: 0.02\n",
            "iteration: 32400 loss: 0.0087 lr: 0.02\n",
            "iteration: 32500 loss: 0.0092 lr: 0.02\n",
            "iteration: 32600 loss: 0.0081 lr: 0.02\n",
            "iteration: 32700 loss: 0.0096 lr: 0.02\n",
            "iteration: 32800 loss: 0.0088 lr: 0.02\n",
            "iteration: 32900 loss: 0.0080 lr: 0.02\n",
            "iteration: 33000 loss: 0.0074 lr: 0.02\n",
            "iteration: 33100 loss: 0.0087 lr: 0.02\n",
            "iteration: 33200 loss: 0.0081 lr: 0.02\n",
            "iteration: 33300 loss: 0.0102 lr: 0.02\n",
            "iteration: 33400 loss: 0.0100 lr: 0.02\n",
            "iteration: 33500 loss: 0.0088 lr: 0.02\n",
            "iteration: 33600 loss: 0.0081 lr: 0.02\n",
            "iteration: 33700 loss: 0.0079 lr: 0.02\n",
            "iteration: 33800 loss: 0.0085 lr: 0.02\n",
            "iteration: 33900 loss: 0.0082 lr: 0.02\n",
            "iteration: 34000 loss: 0.0099 lr: 0.02\n",
            "iteration: 34100 loss: 0.0090 lr: 0.02\n",
            "iteration: 34200 loss: 0.0103 lr: 0.02\n",
            "iteration: 34300 loss: 0.0085 lr: 0.02\n",
            "iteration: 34400 loss: 0.0085 lr: 0.02\n",
            "iteration: 34500 loss: 0.0076 lr: 0.02\n",
            "iteration: 34600 loss: 0.0082 lr: 0.02\n",
            "iteration: 34700 loss: 0.0085 lr: 0.02\n",
            "iteration: 34800 loss: 0.0079 lr: 0.02\n",
            "iteration: 34900 loss: 0.0089 lr: 0.02\n",
            "iteration: 35000 loss: 0.0087 lr: 0.02\n",
            "iteration: 35100 loss: 0.0091 lr: 0.02\n",
            "iteration: 35200 loss: 0.0094 lr: 0.02\n",
            "iteration: 35300 loss: 0.0097 lr: 0.02\n",
            "iteration: 35400 loss: 0.0073 lr: 0.02\n",
            "iteration: 35500 loss: 0.0088 lr: 0.02\n",
            "iteration: 35600 loss: 0.0089 lr: 0.02\n",
            "iteration: 35700 loss: 0.0081 lr: 0.02\n",
            "iteration: 35800 loss: 0.0075 lr: 0.02\n",
            "iteration: 35900 loss: 0.0077 lr: 0.02\n",
            "iteration: 36000 loss: 0.0091 lr: 0.02\n",
            "iteration: 36100 loss: 0.0075 lr: 0.02\n",
            "iteration: 36200 loss: 0.0086 lr: 0.02\n",
            "iteration: 36300 loss: 0.0094 lr: 0.02\n",
            "iteration: 36400 loss: 0.0083 lr: 0.02\n",
            "iteration: 36500 loss: 0.0084 lr: 0.02\n",
            "iteration: 36600 loss: 0.0079 lr: 0.02\n",
            "iteration: 36700 loss: 0.0086 lr: 0.02\n",
            "iteration: 36800 loss: 0.0089 lr: 0.02\n",
            "iteration: 36900 loss: 0.0079 lr: 0.02\n",
            "iteration: 37000 loss: 0.0087 lr: 0.02\n",
            "iteration: 37100 loss: 0.0085 lr: 0.02\n",
            "iteration: 37200 loss: 0.0094 lr: 0.02\n",
            "iteration: 37300 loss: 0.0086 lr: 0.02\n",
            "iteration: 37400 loss: 0.0088 lr: 0.02\n",
            "iteration: 37500 loss: 0.0082 lr: 0.02\n",
            "iteration: 37600 loss: 0.0075 lr: 0.02\n",
            "iteration: 37700 loss: 0.0083 lr: 0.02\n",
            "iteration: 37800 loss: 0.0076 lr: 0.02\n",
            "iteration: 37900 loss: 0.0079 lr: 0.02\n",
            "iteration: 38000 loss: 0.0092 lr: 0.02\n",
            "iteration: 38100 loss: 0.0086 lr: 0.02\n",
            "iteration: 38200 loss: 0.0081 lr: 0.02\n",
            "iteration: 38300 loss: 0.0077 lr: 0.02\n",
            "iteration: 38400 loss: 0.0094 lr: 0.02\n",
            "iteration: 38500 loss: 0.0085 lr: 0.02\n",
            "iteration: 38600 loss: 0.0076 lr: 0.02\n",
            "iteration: 38700 loss: 0.0084 lr: 0.02\n",
            "iteration: 38800 loss: 0.0084 lr: 0.02\n",
            "iteration: 38900 loss: 0.0089 lr: 0.02\n",
            "iteration: 39000 loss: 0.0068 lr: 0.02\n",
            "iteration: 39100 loss: 0.0081 lr: 0.02\n",
            "iteration: 39200 loss: 0.0079 lr: 0.02\n",
            "iteration: 39300 loss: 0.0080 lr: 0.02\n",
            "iteration: 39400 loss: 0.0076 lr: 0.02\n",
            "iteration: 39500 loss: 0.0092 lr: 0.02\n",
            "iteration: 39600 loss: 0.0086 lr: 0.02\n",
            "iteration: 39700 loss: 0.0085 lr: 0.02\n",
            "iteration: 39800 loss: 0.0082 lr: 0.02\n",
            "iteration: 39900 loss: 0.0077 lr: 0.02\n",
            "iteration: 40000 loss: 0.0080 lr: 0.02\n",
            "iteration: 40100 loss: 0.0091 lr: 0.02\n",
            "iteration: 40200 loss: 0.0081 lr: 0.02\n",
            "iteration: 40300 loss: 0.0092 lr: 0.02\n",
            "iteration: 40400 loss: 0.0082 lr: 0.02\n",
            "iteration: 40500 loss: 0.0078 lr: 0.02\n",
            "iteration: 40600 loss: 0.0084 lr: 0.02\n",
            "iteration: 40700 loss: 0.0093 lr: 0.02\n",
            "iteration: 40800 loss: 0.0075 lr: 0.02\n",
            "iteration: 40900 loss: 0.0083 lr: 0.02\n",
            "iteration: 41000 loss: 0.0072 lr: 0.02\n",
            "iteration: 41100 loss: 0.0070 lr: 0.02\n",
            "iteration: 41200 loss: 0.0073 lr: 0.02\n",
            "iteration: 41300 loss: 0.0072 lr: 0.02\n",
            "iteration: 41400 loss: 0.0082 lr: 0.02\n",
            "iteration: 41500 loss: 0.0078 lr: 0.02\n",
            "iteration: 41600 loss: 0.0091 lr: 0.02\n",
            "iteration: 41700 loss: 0.0078 lr: 0.02\n",
            "iteration: 41800 loss: 0.0082 lr: 0.02\n",
            "iteration: 41900 loss: 0.0067 lr: 0.02\n",
            "iteration: 42000 loss: 0.0081 lr: 0.02\n",
            "iteration: 42100 loss: 0.0084 lr: 0.02\n",
            "iteration: 42200 loss: 0.0087 lr: 0.02\n",
            "iteration: 42300 loss: 0.0081 lr: 0.02\n",
            "iteration: 42400 loss: 0.0085 lr: 0.02\n",
            "iteration: 42500 loss: 0.0071 lr: 0.02\n",
            "iteration: 42600 loss: 0.0075 lr: 0.02\n",
            "iteration: 42700 loss: 0.0077 lr: 0.02\n",
            "iteration: 42800 loss: 0.0082 lr: 0.02\n",
            "iteration: 42900 loss: 0.0081 lr: 0.02\n",
            "iteration: 43000 loss: 0.0075 lr: 0.02\n",
            "iteration: 43100 loss: 0.0076 lr: 0.02\n",
            "iteration: 43200 loss: 0.0078 lr: 0.02\n",
            "iteration: 43300 loss: 0.0079 lr: 0.02\n",
            "iteration: 43400 loss: 0.0076 lr: 0.02\n",
            "iteration: 43500 loss: 0.0081 lr: 0.02\n",
            "iteration: 43600 loss: 0.0071 lr: 0.02\n",
            "iteration: 43700 loss: 0.0070 lr: 0.02\n",
            "iteration: 43800 loss: 0.0062 lr: 0.02\n",
            "iteration: 43900 loss: 0.0081 lr: 0.02\n",
            "iteration: 44000 loss: 0.0075 lr: 0.02\n",
            "iteration: 44100 loss: 0.0095 lr: 0.02\n",
            "iteration: 44200 loss: 0.0088 lr: 0.02\n",
            "iteration: 44300 loss: 0.0070 lr: 0.02\n",
            "iteration: 44400 loss: 0.0092 lr: 0.02\n",
            "iteration: 44500 loss: 0.0068 lr: 0.02\n",
            "iteration: 44600 loss: 0.0076 lr: 0.02\n",
            "iteration: 44700 loss: 0.0072 lr: 0.02\n",
            "iteration: 44800 loss: 0.0071 lr: 0.02\n",
            "iteration: 44900 loss: 0.0071 lr: 0.02\n",
            "iteration: 45000 loss: 0.0064 lr: 0.02\n",
            "iteration: 45100 loss: 0.0064 lr: 0.02\n",
            "iteration: 45200 loss: 0.0070 lr: 0.02\n",
            "iteration: 45300 loss: 0.0073 lr: 0.02\n",
            "iteration: 45400 loss: 0.0080 lr: 0.02\n",
            "iteration: 45500 loss: 0.0077 lr: 0.02\n",
            "iteration: 45600 loss: 0.0073 lr: 0.02\n",
            "iteration: 45700 loss: 0.0078 lr: 0.02\n",
            "iteration: 45800 loss: 0.0076 lr: 0.02\n",
            "iteration: 45900 loss: 0.0075 lr: 0.02\n",
            "iteration: 46000 loss: 0.0066 lr: 0.02\n",
            "iteration: 46100 loss: 0.0078 lr: 0.02\n",
            "iteration: 46200 loss: 0.0075 lr: 0.02\n",
            "iteration: 46300 loss: 0.0068 lr: 0.02\n",
            "iteration: 46400 loss: 0.0069 lr: 0.02\n",
            "iteration: 46500 loss: 0.0077 lr: 0.02\n",
            "iteration: 46600 loss: 0.0075 lr: 0.02\n",
            "iteration: 46700 loss: 0.0077 lr: 0.02\n",
            "iteration: 46800 loss: 0.0072 lr: 0.02\n",
            "iteration: 46900 loss: 0.0074 lr: 0.02\n",
            "iteration: 47000 loss: 0.0078 lr: 0.02\n",
            "iteration: 47100 loss: 0.0073 lr: 0.02\n",
            "iteration: 47200 loss: 0.0071 lr: 0.02\n",
            "iteration: 47300 loss: 0.0068 lr: 0.02\n",
            "iteration: 47400 loss: 0.0074 lr: 0.02\n",
            "iteration: 47500 loss: 0.0066 lr: 0.02\n",
            "iteration: 47600 loss: 0.0070 lr: 0.02\n",
            "iteration: 47700 loss: 0.0064 lr: 0.02\n",
            "iteration: 47800 loss: 0.0074 lr: 0.02\n",
            "iteration: 47900 loss: 0.0060 lr: 0.02\n",
            "iteration: 48000 loss: 0.0064 lr: 0.02\n",
            "iteration: 48100 loss: 0.0076 lr: 0.02\n",
            "iteration: 48200 loss: 0.0069 lr: 0.02\n",
            "iteration: 48300 loss: 0.0073 lr: 0.02\n",
            "iteration: 48400 loss: 0.0070 lr: 0.02\n",
            "iteration: 48500 loss: 0.0060 lr: 0.02\n",
            "iteration: 48600 loss: 0.0071 lr: 0.02\n",
            "iteration: 48700 loss: 0.0064 lr: 0.02\n",
            "iteration: 48800 loss: 0.0071 lr: 0.02\n",
            "iteration: 48900 loss: 0.0063 lr: 0.02\n",
            "iteration: 49000 loss: 0.0066 lr: 0.02\n",
            "iteration: 49100 loss: 0.0059 lr: 0.02\n",
            "iteration: 49200 loss: 0.0067 lr: 0.02\n",
            "iteration: 49300 loss: 0.0080 lr: 0.02\n",
            "iteration: 49400 loss: 0.0075 lr: 0.02\n",
            "iteration: 49500 loss: 0.0069 lr: 0.02\n",
            "iteration: 49600 loss: 0.0075 lr: 0.02\n",
            "iteration: 49700 loss: 0.0078 lr: 0.02\n",
            "iteration: 49800 loss: 0.0070 lr: 0.02\n",
            "iteration: 49900 loss: 0.0072 lr: 0.02\n",
            "iteration: 50000 loss: 0.0073 lr: 0.02\n",
            "iteration: 50100 loss: 0.0058 lr: 0.02\n",
            "iteration: 50200 loss: 0.0071 lr: 0.02\n",
            "iteration: 50300 loss: 0.0076 lr: 0.02\n",
            "iteration: 50400 loss: 0.0070 lr: 0.02\n",
            "iteration: 50500 loss: 0.0088 lr: 0.02\n",
            "iteration: 50600 loss: 0.0071 lr: 0.02\n",
            "iteration: 50700 loss: 0.0072 lr: 0.02\n",
            "iteration: 50800 loss: 0.0066 lr: 0.02\n",
            "iteration: 50900 loss: 0.0063 lr: 0.02\n",
            "iteration: 51000 loss: 0.0067 lr: 0.02\n",
            "iteration: 51100 loss: 0.0075 lr: 0.02\n",
            "iteration: 51200 loss: 0.0076 lr: 0.02\n",
            "iteration: 51300 loss: 0.0069 lr: 0.02\n",
            "iteration: 51400 loss: 0.0067 lr: 0.02\n",
            "iteration: 51500 loss: 0.0069 lr: 0.02\n",
            "iteration: 51600 loss: 0.0071 lr: 0.02\n",
            "iteration: 51700 loss: 0.0063 lr: 0.02\n",
            "iteration: 51800 loss: 0.0070 lr: 0.02\n",
            "iteration: 51900 loss: 0.0063 lr: 0.02\n",
            "iteration: 52000 loss: 0.0058 lr: 0.02\n",
            "iteration: 52100 loss: 0.0063 lr: 0.02\n",
            "iteration: 52200 loss: 0.0074 lr: 0.02\n",
            "iteration: 52300 loss: 0.0070 lr: 0.02\n",
            "iteration: 52400 loss: 0.0074 lr: 0.02\n",
            "iteration: 52500 loss: 0.0068 lr: 0.02\n",
            "iteration: 52600 loss: 0.0067 lr: 0.02\n",
            "iteration: 52700 loss: 0.0071 lr: 0.02\n",
            "iteration: 52800 loss: 0.0076 lr: 0.02\n",
            "iteration: 52900 loss: 0.0061 lr: 0.02\n",
            "iteration: 53000 loss: 0.0070 lr: 0.02\n",
            "iteration: 53100 loss: 0.0068 lr: 0.02\n",
            "iteration: 53200 loss: 0.0068 lr: 0.02\n",
            "iteration: 53300 loss: 0.0065 lr: 0.02\n",
            "iteration: 53400 loss: 0.0067 lr: 0.02\n",
            "iteration: 53500 loss: 0.0064 lr: 0.02\n",
            "iteration: 53600 loss: 0.0060 lr: 0.02\n",
            "iteration: 53700 loss: 0.0060 lr: 0.02\n",
            "iteration: 53800 loss: 0.0064 lr: 0.02\n",
            "iteration: 53900 loss: 0.0072 lr: 0.02\n",
            "iteration: 54000 loss: 0.0077 lr: 0.02\n",
            "iteration: 54100 loss: 0.0061 lr: 0.02\n",
            "iteration: 54200 loss: 0.0066 lr: 0.02\n",
            "iteration: 54300 loss: 0.0072 lr: 0.02\n",
            "iteration: 54400 loss: 0.0059 lr: 0.02\n",
            "iteration: 54500 loss: 0.0066 lr: 0.02\n",
            "iteration: 54600 loss: 0.0062 lr: 0.02\n",
            "iteration: 54700 loss: 0.0062 lr: 0.02\n",
            "iteration: 54800 loss: 0.0065 lr: 0.02\n",
            "iteration: 54900 loss: 0.0066 lr: 0.02\n",
            "iteration: 55000 loss: 0.0067 lr: 0.02\n",
            "iteration: 55100 loss: 0.0056 lr: 0.02\n",
            "iteration: 55200 loss: 0.0056 lr: 0.02\n",
            "iteration: 55300 loss: 0.0063 lr: 0.02\n",
            "iteration: 55400 loss: 0.0068 lr: 0.02\n",
            "iteration: 55500 loss: 0.0069 lr: 0.02\n",
            "iteration: 55600 loss: 0.0056 lr: 0.02\n",
            "iteration: 55700 loss: 0.0055 lr: 0.02\n",
            "iteration: 55800 loss: 0.0068 lr: 0.02\n",
            "iteration: 55900 loss: 0.0067 lr: 0.02\n",
            "iteration: 56000 loss: 0.0061 lr: 0.02\n",
            "iteration: 56100 loss: 0.0061 lr: 0.02\n",
            "iteration: 56200 loss: 0.0071 lr: 0.02\n",
            "iteration: 56300 loss: 0.0062 lr: 0.02\n",
            "iteration: 56400 loss: 0.0063 lr: 0.02\n",
            "iteration: 56500 loss: 0.0067 lr: 0.02\n",
            "iteration: 56600 loss: 0.0056 lr: 0.02\n",
            "iteration: 56700 loss: 0.0056 lr: 0.02\n",
            "iteration: 56800 loss: 0.0069 lr: 0.02\n",
            "iteration: 56900 loss: 0.0070 lr: 0.02\n",
            "iteration: 57000 loss: 0.0059 lr: 0.02\n",
            "iteration: 57100 loss: 0.0072 lr: 0.02\n",
            "iteration: 57200 loss: 0.0060 lr: 0.02\n",
            "iteration: 57300 loss: 0.0054 lr: 0.02\n",
            "iteration: 57400 loss: 0.0060 lr: 0.02\n",
            "iteration: 57500 loss: 0.0060 lr: 0.02\n",
            "iteration: 57600 loss: 0.0058 lr: 0.02\n",
            "iteration: 57700 loss: 0.0056 lr: 0.02\n",
            "iteration: 57800 loss: 0.0060 lr: 0.02\n",
            "iteration: 57900 loss: 0.0058 lr: 0.02\n",
            "iteration: 58000 loss: 0.0061 lr: 0.02\n",
            "iteration: 58100 loss: 0.0048 lr: 0.02\n",
            "iteration: 58200 loss: 0.0058 lr: 0.02\n",
            "iteration: 58300 loss: 0.0060 lr: 0.02\n",
            "iteration: 58400 loss: 0.0059 lr: 0.02\n",
            "iteration: 58500 loss: 0.0061 lr: 0.02\n",
            "iteration: 58600 loss: 0.0064 lr: 0.02\n",
            "iteration: 58700 loss: 0.0063 lr: 0.02\n",
            "iteration: 58800 loss: 0.0068 lr: 0.02\n",
            "iteration: 58900 loss: 0.0059 lr: 0.02\n",
            "iteration: 59000 loss: 0.0063 lr: 0.02\n",
            "iteration: 59100 loss: 0.0063 lr: 0.02\n",
            "iteration: 59200 loss: 0.0060 lr: 0.02\n",
            "iteration: 59300 loss: 0.0073 lr: 0.02\n",
            "iteration: 59400 loss: 0.0063 lr: 0.02\n",
            "iteration: 59500 loss: 0.0065 lr: 0.02\n",
            "iteration: 59600 loss: 0.0066 lr: 0.02\n",
            "iteration: 59700 loss: 0.0065 lr: 0.02\n",
            "iteration: 59800 loss: 0.0059 lr: 0.02\n",
            "iteration: 59900 loss: 0.0071 lr: 0.02\n",
            "iteration: 60000 loss: 0.0059 lr: 0.02\n",
            "iteration: 60100 loss: 0.0061 lr: 0.02\n",
            "iteration: 60200 loss: 0.0067 lr: 0.02\n",
            "iteration: 60300 loss: 0.0067 lr: 0.02\n",
            "iteration: 60400 loss: 0.0066 lr: 0.02\n",
            "iteration: 60500 loss: 0.0060 lr: 0.02\n",
            "iteration: 60600 loss: 0.0060 lr: 0.02\n",
            "iteration: 60700 loss: 0.0058 lr: 0.02\n",
            "iteration: 60800 loss: 0.0059 lr: 0.02\n",
            "iteration: 60900 loss: 0.0057 lr: 0.02\n",
            "iteration: 61000 loss: 0.0065 lr: 0.02\n",
            "iteration: 61100 loss: 0.0061 lr: 0.02\n",
            "iteration: 61200 loss: 0.0068 lr: 0.02\n",
            "iteration: 61300 loss: 0.0059 lr: 0.02\n",
            "iteration: 61400 loss: 0.0060 lr: 0.02\n",
            "iteration: 61500 loss: 0.0061 lr: 0.02\n",
            "iteration: 61600 loss: 0.0056 lr: 0.02\n",
            "iteration: 61700 loss: 0.0056 lr: 0.02\n",
            "iteration: 61800 loss: 0.0060 lr: 0.02\n",
            "iteration: 61900 loss: 0.0068 lr: 0.02\n",
            "iteration: 62000 loss: 0.0064 lr: 0.02\n",
            "iteration: 62100 loss: 0.0066 lr: 0.02\n",
            "iteration: 62200 loss: 0.0072 lr: 0.02\n",
            "iteration: 62300 loss: 0.0065 lr: 0.02\n",
            "iteration: 62400 loss: 0.0054 lr: 0.02\n",
            "iteration: 62500 loss: 0.0065 lr: 0.02\n",
            "iteration: 62600 loss: 0.0064 lr: 0.02\n",
            "iteration: 62700 loss: 0.0058 lr: 0.02\n",
            "iteration: 62800 loss: 0.0059 lr: 0.02\n",
            "iteration: 62900 loss: 0.0057 lr: 0.02\n",
            "iteration: 63000 loss: 0.0055 lr: 0.02\n",
            "iteration: 63100 loss: 0.0064 lr: 0.02\n",
            "iteration: 63200 loss: 0.0063 lr: 0.02\n",
            "iteration: 63300 loss: 0.0063 lr: 0.02\n",
            "iteration: 63400 loss: 0.0059 lr: 0.02\n",
            "iteration: 63500 loss: 0.0077 lr: 0.02\n",
            "iteration: 63600 loss: 0.0060 lr: 0.02\n",
            "iteration: 63700 loss: 0.0061 lr: 0.02\n",
            "iteration: 63800 loss: 0.0060 lr: 0.02\n",
            "iteration: 63900 loss: 0.0051 lr: 0.02\n",
            "iteration: 64000 loss: 0.0056 lr: 0.02\n",
            "iteration: 64100 loss: 0.0063 lr: 0.02\n",
            "iteration: 64200 loss: 0.0055 lr: 0.02\n",
            "iteration: 64300 loss: 0.0056 lr: 0.02\n",
            "iteration: 64400 loss: 0.0060 lr: 0.02\n",
            "iteration: 64500 loss: 0.0059 lr: 0.02\n",
            "iteration: 64600 loss: 0.0062 lr: 0.02\n",
            "iteration: 64700 loss: 0.0053 lr: 0.02\n",
            "iteration: 64800 loss: 0.0059 lr: 0.02\n",
            "iteration: 64900 loss: 0.0055 lr: 0.02\n",
            "iteration: 65000 loss: 0.0061 lr: 0.02\n",
            "iteration: 65100 loss: 0.0049 lr: 0.02\n",
            "iteration: 65200 loss: 0.0053 lr: 0.02\n",
            "iteration: 65300 loss: 0.0064 lr: 0.02\n",
            "iteration: 65400 loss: 0.0058 lr: 0.02\n",
            "iteration: 65500 loss: 0.0056 lr: 0.02\n",
            "iteration: 65600 loss: 0.0059 lr: 0.02\n",
            "iteration: 65700 loss: 0.0054 lr: 0.02\n",
            "iteration: 65800 loss: 0.0053 lr: 0.02\n",
            "iteration: 65900 loss: 0.0060 lr: 0.02\n",
            "iteration: 66000 loss: 0.0053 lr: 0.02\n",
            "iteration: 66100 loss: 0.0063 lr: 0.02\n",
            "iteration: 66200 loss: 0.0057 lr: 0.02\n",
            "iteration: 66300 loss: 0.0066 lr: 0.02\n",
            "iteration: 66400 loss: 0.0063 lr: 0.02\n",
            "iteration: 66500 loss: 0.0058 lr: 0.02\n",
            "iteration: 66600 loss: 0.0068 lr: 0.02\n",
            "iteration: 66700 loss: 0.0053 lr: 0.02\n",
            "iteration: 66800 loss: 0.0050 lr: 0.02\n",
            "iteration: 66900 loss: 0.0063 lr: 0.02\n",
            "iteration: 67000 loss: 0.0057 lr: 0.02\n",
            "iteration: 67100 loss: 0.0055 lr: 0.02\n",
            "iteration: 67200 loss: 0.0057 lr: 0.02\n",
            "iteration: 67300 loss: 0.0048 lr: 0.02\n",
            "iteration: 67400 loss: 0.0054 lr: 0.02\n",
            "iteration: 67500 loss: 0.0060 lr: 0.02\n",
            "iteration: 67600 loss: 0.0056 lr: 0.02\n",
            "iteration: 67700 loss: 0.0055 lr: 0.02\n",
            "iteration: 67800 loss: 0.0058 lr: 0.02\n",
            "iteration: 67900 loss: 0.0052 lr: 0.02\n",
            "iteration: 68000 loss: 0.0055 lr: 0.02\n",
            "iteration: 68100 loss: 0.0067 lr: 0.02\n",
            "iteration: 68200 loss: 0.0054 lr: 0.02\n",
            "iteration: 68300 loss: 0.0060 lr: 0.02\n",
            "iteration: 68400 loss: 0.0065 lr: 0.02\n",
            "iteration: 68500 loss: 0.0055 lr: 0.02\n",
            "iteration: 68600 loss: 0.0063 lr: 0.02\n",
            "iteration: 68700 loss: 0.0060 lr: 0.02\n",
            "iteration: 68800 loss: 0.0059 lr: 0.02\n",
            "iteration: 68900 loss: 0.0059 lr: 0.02\n",
            "iteration: 69000 loss: 0.0055 lr: 0.02\n",
            "iteration: 69100 loss: 0.0064 lr: 0.02\n",
            "iteration: 69200 loss: 0.0061 lr: 0.02\n",
            "iteration: 69300 loss: 0.0058 lr: 0.02\n",
            "iteration: 69400 loss: 0.0056 lr: 0.02\n",
            "iteration: 69500 loss: 0.0068 lr: 0.02\n",
            "iteration: 69600 loss: 0.0057 lr: 0.02\n",
            "iteration: 69700 loss: 0.0049 lr: 0.02\n",
            "iteration: 69800 loss: 0.0058 lr: 0.02\n",
            "iteration: 69900 loss: 0.0060 lr: 0.02\n",
            "iteration: 70000 loss: 0.0053 lr: 0.02\n",
            "iteration: 70100 loss: 0.0065 lr: 0.02\n",
            "iteration: 70200 loss: 0.0057 lr: 0.02\n",
            "iteration: 70300 loss: 0.0054 lr: 0.02\n",
            "iteration: 70400 loss: 0.0058 lr: 0.02\n",
            "iteration: 70500 loss: 0.0058 lr: 0.02\n",
            "iteration: 70600 loss: 0.0063 lr: 0.02\n",
            "iteration: 70700 loss: 0.0048 lr: 0.02\n",
            "iteration: 70800 loss: 0.0053 lr: 0.02\n",
            "iteration: 70900 loss: 0.0054 lr: 0.02\n",
            "iteration: 71000 loss: 0.0054 lr: 0.02\n",
            "iteration: 71100 loss: 0.0052 lr: 0.02\n",
            "iteration: 71200 loss: 0.0054 lr: 0.02\n",
            "iteration: 71300 loss: 0.0055 lr: 0.02\n",
            "iteration: 71400 loss: 0.0057 lr: 0.02\n",
            "iteration: 71500 loss: 0.0060 lr: 0.02\n",
            "iteration: 71600 loss: 0.0053 lr: 0.02\n",
            "iteration: 71700 loss: 0.0044 lr: 0.02\n",
            "iteration: 71800 loss: 0.0052 lr: 0.02\n",
            "iteration: 71900 loss: 0.0060 lr: 0.02\n",
            "iteration: 72000 loss: 0.0054 lr: 0.02\n",
            "iteration: 72100 loss: 0.0049 lr: 0.02\n",
            "iteration: 72200 loss: 0.0056 lr: 0.02\n",
            "iteration: 72300 loss: 0.0054 lr: 0.02\n",
            "iteration: 72400 loss: 0.0047 lr: 0.02\n",
            "iteration: 72500 loss: 0.0054 lr: 0.02\n",
            "iteration: 72600 loss: 0.0052 lr: 0.02\n",
            "iteration: 72700 loss: 0.0052 lr: 0.02\n",
            "iteration: 72800 loss: 0.0057 lr: 0.02\n",
            "iteration: 72900 loss: 0.0052 lr: 0.02\n",
            "iteration: 73000 loss: 0.0056 lr: 0.02\n",
            "iteration: 73100 loss: 0.0052 lr: 0.02\n",
            "iteration: 73200 loss: 0.0047 lr: 0.02\n",
            "iteration: 73300 loss: 0.0057 lr: 0.02\n",
            "iteration: 73400 loss: 0.0054 lr: 0.02\n",
            "iteration: 73500 loss: 0.0052 lr: 0.02\n",
            "iteration: 73600 loss: 0.0057 lr: 0.02\n",
            "iteration: 73700 loss: 0.0054 lr: 0.02\n",
            "iteration: 73800 loss: 0.0056 lr: 0.02\n",
            "iteration: 73900 loss: 0.0056 lr: 0.02\n",
            "iteration: 74000 loss: 0.0051 lr: 0.02\n",
            "iteration: 74100 loss: 0.0062 lr: 0.02\n",
            "iteration: 74200 loss: 0.0054 lr: 0.02\n",
            "iteration: 74300 loss: 0.0059 lr: 0.02\n",
            "iteration: 74400 loss: 0.0048 lr: 0.02\n",
            "iteration: 74500 loss: 0.0051 lr: 0.02\n",
            "iteration: 74600 loss: 0.0052 lr: 0.02\n",
            "iteration: 74700 loss: 0.0048 lr: 0.02\n",
            "iteration: 74800 loss: 0.0051 lr: 0.02\n",
            "iteration: 74900 loss: 0.0057 lr: 0.02\n",
            "iteration: 75000 loss: 0.0056 lr: 0.02\n",
            "iteration: 75100 loss: 0.0049 lr: 0.02\n",
            "iteration: 75200 loss: 0.0053 lr: 0.02\n",
            "iteration: 75300 loss: 0.0057 lr: 0.02\n",
            "iteration: 75400 loss: 0.0053 lr: 0.02\n",
            "iteration: 75500 loss: 0.0058 lr: 0.02\n",
            "iteration: 75600 loss: 0.0058 lr: 0.02\n",
            "iteration: 75700 loss: 0.0050 lr: 0.02\n",
            "iteration: 75800 loss: 0.0061 lr: 0.02\n",
            "iteration: 75900 loss: 0.0063 lr: 0.02\n",
            "iteration: 76000 loss: 0.0054 lr: 0.02\n",
            "iteration: 76100 loss: 0.0055 lr: 0.02\n",
            "iteration: 76200 loss: 0.0056 lr: 0.02\n",
            "iteration: 76300 loss: 0.0050 lr: 0.02\n",
            "iteration: 76400 loss: 0.0057 lr: 0.02\n",
            "iteration: 76500 loss: 0.0048 lr: 0.02\n",
            "iteration: 76600 loss: 0.0050 lr: 0.02\n",
            "iteration: 76700 loss: 0.0053 lr: 0.02\n",
            "iteration: 76800 loss: 0.0057 lr: 0.02\n",
            "iteration: 76900 loss: 0.0052 lr: 0.02\n",
            "iteration: 77000 loss: 0.0054 lr: 0.02\n",
            "iteration: 77100 loss: 0.0055 lr: 0.02\n",
            "iteration: 77200 loss: 0.0058 lr: 0.02\n",
            "iteration: 77300 loss: 0.0050 lr: 0.02\n",
            "iteration: 77400 loss: 0.0049 lr: 0.02\n",
            "iteration: 77500 loss: 0.0051 lr: 0.02\n",
            "iteration: 77600 loss: 0.0056 lr: 0.02\n",
            "iteration: 77700 loss: 0.0054 lr: 0.02\n",
            "iteration: 77800 loss: 0.0059 lr: 0.02\n",
            "iteration: 77900 loss: 0.0061 lr: 0.02\n",
            "iteration: 78000 loss: 0.0062 lr: 0.02\n",
            "iteration: 78100 loss: 0.0053 lr: 0.02\n",
            "iteration: 78200 loss: 0.0062 lr: 0.02\n",
            "iteration: 78300 loss: 0.0047 lr: 0.02\n",
            "iteration: 78400 loss: 0.0056 lr: 0.02\n",
            "iteration: 78500 loss: 0.0050 lr: 0.02\n",
            "iteration: 78600 loss: 0.0047 lr: 0.02\n",
            "iteration: 78700 loss: 0.0053 lr: 0.02\n",
            "iteration: 78800 loss: 0.0045 lr: 0.02\n",
            "iteration: 78900 loss: 0.0051 lr: 0.02\n",
            "iteration: 79000 loss: 0.0060 lr: 0.02\n",
            "iteration: 79100 loss: 0.0056 lr: 0.02\n",
            "iteration: 79200 loss: 0.0052 lr: 0.02\n",
            "iteration: 79300 loss: 0.0054 lr: 0.02\n",
            "iteration: 79400 loss: 0.0051 lr: 0.02\n",
            "iteration: 79500 loss: 0.0052 lr: 0.02\n",
            "iteration: 79600 loss: 0.0054 lr: 0.02\n",
            "iteration: 79700 loss: 0.0055 lr: 0.02\n",
            "iteration: 79800 loss: 0.0052 lr: 0.02\n",
            "iteration: 79900 loss: 0.0049 lr: 0.02\n",
            "iteration: 80000 loss: 0.0045 lr: 0.02\n",
            "iteration: 80100 loss: 0.0048 lr: 0.02\n",
            "iteration: 80200 loss: 0.0053 lr: 0.02\n",
            "iteration: 80300 loss: 0.0047 lr: 0.02\n",
            "iteration: 80400 loss: 0.0057 lr: 0.02\n",
            "iteration: 80500 loss: 0.0049 lr: 0.02\n",
            "iteration: 80600 loss: 0.0047 lr: 0.02\n",
            "iteration: 80700 loss: 0.0050 lr: 0.02\n",
            "iteration: 80800 loss: 0.0052 lr: 0.02\n",
            "iteration: 80900 loss: 0.0052 lr: 0.02\n",
            "iteration: 81000 loss: 0.0047 lr: 0.02\n",
            "iteration: 81100 loss: 0.0055 lr: 0.02\n",
            "iteration: 81200 loss: 0.0051 lr: 0.02\n",
            "iteration: 81300 loss: 0.0052 lr: 0.02\n",
            "iteration: 81400 loss: 0.0052 lr: 0.02\n",
            "iteration: 81500 loss: 0.0051 lr: 0.02\n",
            "iteration: 81600 loss: 0.0052 lr: 0.02\n",
            "iteration: 81700 loss: 0.0051 lr: 0.02\n",
            "iteration: 81800 loss: 0.0047 lr: 0.02\n",
            "iteration: 81900 loss: 0.0050 lr: 0.02\n",
            "iteration: 82000 loss: 0.0059 lr: 0.02\n",
            "iteration: 82100 loss: 0.0048 lr: 0.02\n",
            "iteration: 82200 loss: 0.0054 lr: 0.02\n",
            "iteration: 82300 loss: 0.0056 lr: 0.02\n",
            "iteration: 82400 loss: 0.0051 lr: 0.02\n",
            "iteration: 82500 loss: 0.0044 lr: 0.02\n",
            "iteration: 82600 loss: 0.0052 lr: 0.02\n",
            "iteration: 82700 loss: 0.0052 lr: 0.02\n",
            "iteration: 82800 loss: 0.0052 lr: 0.02\n",
            "iteration: 82900 loss: 0.0060 lr: 0.02\n",
            "iteration: 83000 loss: 0.0045 lr: 0.02\n",
            "iteration: 83100 loss: 0.0043 lr: 0.02\n",
            "iteration: 83200 loss: 0.0055 lr: 0.02\n",
            "iteration: 83300 loss: 0.0053 lr: 0.02\n",
            "iteration: 83400 loss: 0.0050 lr: 0.02\n",
            "iteration: 83500 loss: 0.0052 lr: 0.02\n",
            "iteration: 83600 loss: 0.0051 lr: 0.02\n",
            "iteration: 83700 loss: 0.0049 lr: 0.02\n",
            "iteration: 83800 loss: 0.0047 lr: 0.02\n",
            "iteration: 83900 loss: 0.0056 lr: 0.02\n",
            "iteration: 84000 loss: 0.0052 lr: 0.02\n",
            "iteration: 84100 loss: 0.0057 lr: 0.02\n",
            "iteration: 84200 loss: 0.0058 lr: 0.02\n",
            "iteration: 84300 loss: 0.0045 lr: 0.02\n",
            "iteration: 84400 loss: 0.0052 lr: 0.02\n",
            "iteration: 84500 loss: 0.0047 lr: 0.02\n",
            "iteration: 84600 loss: 0.0048 lr: 0.02\n",
            "iteration: 84700 loss: 0.0050 lr: 0.02\n",
            "iteration: 84800 loss: 0.0049 lr: 0.02\n",
            "iteration: 84900 loss: 0.0051 lr: 0.02\n",
            "iteration: 85000 loss: 0.0049 lr: 0.02\n",
            "iteration: 85100 loss: 0.0047 lr: 0.02\n",
            "iteration: 85200 loss: 0.0052 lr: 0.02\n",
            "iteration: 85300 loss: 0.0053 lr: 0.02\n",
            "iteration: 85400 loss: 0.0046 lr: 0.02\n",
            "iteration: 85500 loss: 0.0045 lr: 0.02\n",
            "iteration: 85600 loss: 0.0046 lr: 0.02\n",
            "iteration: 85700 loss: 0.0051 lr: 0.02\n",
            "iteration: 85800 loss: 0.0057 lr: 0.02\n",
            "iteration: 85900 loss: 0.0040 lr: 0.02\n",
            "iteration: 86000 loss: 0.0040 lr: 0.02\n",
            "iteration: 86100 loss: 0.0042 lr: 0.02\n",
            "iteration: 86200 loss: 0.0051 lr: 0.02\n",
            "iteration: 86300 loss: 0.0052 lr: 0.02\n",
            "iteration: 86400 loss: 0.0045 lr: 0.02\n",
            "iteration: 86500 loss: 0.0049 lr: 0.02\n",
            "iteration: 86600 loss: 0.0050 lr: 0.02\n",
            "iteration: 86700 loss: 0.0048 lr: 0.02\n",
            "iteration: 86800 loss: 0.0050 lr: 0.02\n",
            "iteration: 86900 loss: 0.0057 lr: 0.02\n",
            "iteration: 87000 loss: 0.0053 lr: 0.02\n",
            "iteration: 87100 loss: 0.0047 lr: 0.02\n",
            "iteration: 87200 loss: 0.0045 lr: 0.02\n",
            "iteration: 87300 loss: 0.0055 lr: 0.02\n",
            "iteration: 87400 loss: 0.0052 lr: 0.02\n",
            "iteration: 87500 loss: 0.0048 lr: 0.02\n",
            "iteration: 87600 loss: 0.0053 lr: 0.02\n",
            "iteration: 87700 loss: 0.0041 lr: 0.02\n",
            "iteration: 87800 loss: 0.0046 lr: 0.02\n",
            "iteration: 87900 loss: 0.0050 lr: 0.02\n",
            "iteration: 88000 loss: 0.0046 lr: 0.02\n",
            "iteration: 88100 loss: 0.0049 lr: 0.02\n",
            "iteration: 88200 loss: 0.0054 lr: 0.02\n",
            "iteration: 88300 loss: 0.0050 lr: 0.02\n",
            "iteration: 88400 loss: 0.0045 lr: 0.02\n",
            "iteration: 88500 loss: 0.0047 lr: 0.02\n",
            "iteration: 88600 loss: 0.0048 lr: 0.02\n",
            "iteration: 88700 loss: 0.0057 lr: 0.02\n",
            "iteration: 88800 loss: 0.0047 lr: 0.02\n",
            "iteration: 88900 loss: 0.0043 lr: 0.02\n",
            "iteration: 89000 loss: 0.0040 lr: 0.02\n",
            "iteration: 89100 loss: 0.0049 lr: 0.02\n",
            "iteration: 89200 loss: 0.0053 lr: 0.02\n",
            "iteration: 89300 loss: 0.0041 lr: 0.02\n",
            "iteration: 89400 loss: 0.0048 lr: 0.02\n",
            "iteration: 89500 loss: 0.0047 lr: 0.02\n",
            "iteration: 89600 loss: 0.0054 lr: 0.02\n",
            "iteration: 89700 loss: 0.0047 lr: 0.02\n",
            "iteration: 89800 loss: 0.0051 lr: 0.02\n",
            "iteration: 89900 loss: 0.0054 lr: 0.02\n",
            "iteration: 90000 loss: 0.0053 lr: 0.02\n",
            "iteration: 90100 loss: 0.0049 lr: 0.02\n",
            "iteration: 90200 loss: 0.0042 lr: 0.02\n",
            "iteration: 90300 loss: 0.0055 lr: 0.02\n",
            "iteration: 90400 loss: 0.0051 lr: 0.02\n",
            "iteration: 90500 loss: 0.0044 lr: 0.02\n",
            "iteration: 90600 loss: 0.0052 lr: 0.02\n",
            "iteration: 90700 loss: 0.0045 lr: 0.02\n",
            "iteration: 90800 loss: 0.0045 lr: 0.02\n",
            "iteration: 90900 loss: 0.0056 lr: 0.02\n",
            "iteration: 91000 loss: 0.0049 lr: 0.02\n",
            "iteration: 91100 loss: 0.0046 lr: 0.02\n",
            "iteration: 91200 loss: 0.0049 lr: 0.02\n",
            "iteration: 91300 loss: 0.0047 lr: 0.02\n",
            "iteration: 91400 loss: 0.0046 lr: 0.02\n",
            "iteration: 91500 loss: 0.0046 lr: 0.02\n",
            "iteration: 91600 loss: 0.0048 lr: 0.02\n",
            "iteration: 91700 loss: 0.0050 lr: 0.02\n",
            "iteration: 91800 loss: 0.0055 lr: 0.02\n",
            "iteration: 91900 loss: 0.0050 lr: 0.02\n",
            "iteration: 92000 loss: 0.0050 lr: 0.02\n",
            "iteration: 92100 loss: 0.0051 lr: 0.02\n",
            "iteration: 92200 loss: 0.0051 lr: 0.02\n",
            "iteration: 92300 loss: 0.0050 lr: 0.02\n",
            "iteration: 92400 loss: 0.0048 lr: 0.02\n",
            "iteration: 92500 loss: 0.0054 lr: 0.02\n",
            "iteration: 92600 loss: 0.0048 lr: 0.02\n",
            "iteration: 92700 loss: 0.0044 lr: 0.02\n",
            "iteration: 92800 loss: 0.0046 lr: 0.02\n",
            "iteration: 92900 loss: 0.0050 lr: 0.02\n",
            "iteration: 93000 loss: 0.0042 lr: 0.02\n",
            "iteration: 93100 loss: 0.0053 lr: 0.02\n",
            "iteration: 93200 loss: 0.0046 lr: 0.02\n",
            "iteration: 93300 loss: 0.0056 lr: 0.02\n",
            "iteration: 93400 loss: 0.0051 lr: 0.02\n",
            "iteration: 93500 loss: 0.0057 lr: 0.02\n",
            "iteration: 93600 loss: 0.0043 lr: 0.02\n",
            "iteration: 93700 loss: 0.0049 lr: 0.02\n",
            "iteration: 93800 loss: 0.0046 lr: 0.02\n",
            "iteration: 93900 loss: 0.0046 lr: 0.02\n",
            "iteration: 94000 loss: 0.0049 lr: 0.02\n",
            "iteration: 94100 loss: 0.0055 lr: 0.02\n",
            "iteration: 94200 loss: 0.0045 lr: 0.02\n",
            "iteration: 94300 loss: 0.0050 lr: 0.02\n",
            "iteration: 94400 loss: 0.0050 lr: 0.02\n",
            "iteration: 94500 loss: 0.0046 lr: 0.02\n",
            "iteration: 94600 loss: 0.0044 lr: 0.02\n",
            "iteration: 94700 loss: 0.0048 lr: 0.02\n",
            "iteration: 94800 loss: 0.0046 lr: 0.02\n",
            "iteration: 94900 loss: 0.0050 lr: 0.02\n",
            "iteration: 95000 loss: 0.0049 lr: 0.02\n",
            "iteration: 95100 loss: 0.0050 lr: 0.02\n",
            "iteration: 95200 loss: 0.0045 lr: 0.02\n",
            "iteration: 95300 loss: 0.0050 lr: 0.02\n",
            "iteration: 95400 loss: 0.0039 lr: 0.02\n",
            "iteration: 95500 loss: 0.0052 lr: 0.02\n",
            "iteration: 95600 loss: 0.0047 lr: 0.02\n",
            "iteration: 95700 loss: 0.0053 lr: 0.02\n",
            "iteration: 95800 loss: 0.0049 lr: 0.02\n",
            "iteration: 95900 loss: 0.0050 lr: 0.02\n",
            "iteration: 96000 loss: 0.0042 lr: 0.02\n",
            "iteration: 96100 loss: 0.0049 lr: 0.02\n",
            "iteration: 96200 loss: 0.0046 lr: 0.02\n",
            "iteration: 96300 loss: 0.0043 lr: 0.02\n",
            "iteration: 96400 loss: 0.0046 lr: 0.02\n",
            "iteration: 96500 loss: 0.0047 lr: 0.02\n",
            "iteration: 96600 loss: 0.0050 lr: 0.02\n",
            "iteration: 96700 loss: 0.0054 lr: 0.02\n",
            "iteration: 96800 loss: 0.0051 lr: 0.02\n",
            "iteration: 96900 loss: 0.0045 lr: 0.02\n",
            "iteration: 97000 loss: 0.0049 lr: 0.02\n",
            "iteration: 97100 loss: 0.0048 lr: 0.02\n",
            "iteration: 97200 loss: 0.0040 lr: 0.02\n",
            "iteration: 97300 loss: 0.0052 lr: 0.02\n",
            "iteration: 97400 loss: 0.0046 lr: 0.02\n",
            "iteration: 97500 loss: 0.0054 lr: 0.02\n",
            "iteration: 97600 loss: 0.0058 lr: 0.02\n",
            "iteration: 97700 loss: 0.0053 lr: 0.02\n",
            "iteration: 97800 loss: 0.0048 lr: 0.02\n",
            "iteration: 97900 loss: 0.0044 lr: 0.02\n",
            "iteration: 98000 loss: 0.0050 lr: 0.02\n",
            "iteration: 98100 loss: 0.0041 lr: 0.02\n",
            "iteration: 98200 loss: 0.0044 lr: 0.02\n",
            "iteration: 98300 loss: 0.0042 lr: 0.02\n",
            "iteration: 98400 loss: 0.0051 lr: 0.02\n",
            "iteration: 98500 loss: 0.0052 lr: 0.02\n",
            "iteration: 98600 loss: 0.0044 lr: 0.02\n",
            "iteration: 98700 loss: 0.0045 lr: 0.02\n",
            "iteration: 98800 loss: 0.0048 lr: 0.02\n",
            "iteration: 98900 loss: 0.0052 lr: 0.02\n",
            "iteration: 99000 loss: 0.0055 lr: 0.02\n",
            "iteration: 99100 loss: 0.0046 lr: 0.02\n",
            "iteration: 99200 loss: 0.0049 lr: 0.02\n",
            "iteration: 99300 loss: 0.0045 lr: 0.02\n",
            "iteration: 99400 loss: 0.0044 lr: 0.02\n",
            "iteration: 99500 loss: 0.0044 lr: 0.02\n",
            "iteration: 99600 loss: 0.0047 lr: 0.02\n",
            "iteration: 99700 loss: 0.0044 lr: 0.02\n",
            "iteration: 99800 loss: 0.0049 lr: 0.02\n",
            "iteration: 99900 loss: 0.0049 lr: 0.02\n",
            "iteration: 100000 loss: 0.0045 lr: 0.02\n",
            "iteration: 100100 loss: 0.0058 lr: 0.02\n",
            "iteration: 100200 loss: 0.0049 lr: 0.02\n",
            "iteration: 100300 loss: 0.0047 lr: 0.02\n",
            "iteration: 100400 loss: 0.0039 lr: 0.02\n",
            "iteration: 100500 loss: 0.0049 lr: 0.02\n",
            "iteration: 100600 loss: 0.0048 lr: 0.02\n",
            "iteration: 100700 loss: 0.0041 lr: 0.02\n",
            "iteration: 100800 loss: 0.0049 lr: 0.02\n",
            "iteration: 100900 loss: 0.0047 lr: 0.02\n",
            "iteration: 101000 loss: 0.0048 lr: 0.02\n",
            "iteration: 101100 loss: 0.0048 lr: 0.02\n",
            "iteration: 101200 loss: 0.0048 lr: 0.02\n",
            "iteration: 101300 loss: 0.0047 lr: 0.02\n",
            "iteration: 101400 loss: 0.0051 lr: 0.02\n",
            "iteration: 101500 loss: 0.0048 lr: 0.02\n",
            "iteration: 101600 loss: 0.0046 lr: 0.02\n",
            "iteration: 101700 loss: 0.0041 lr: 0.02\n",
            "iteration: 101800 loss: 0.0048 lr: 0.02\n",
            "iteration: 101900 loss: 0.0055 lr: 0.02\n",
            "iteration: 102000 loss: 0.0051 lr: 0.02\n",
            "iteration: 102100 loss: 0.0047 lr: 0.02\n",
            "iteration: 102200 loss: 0.0051 lr: 0.02\n",
            "iteration: 102300 loss: 0.0054 lr: 0.02\n",
            "iteration: 102400 loss: 0.0043 lr: 0.02\n",
            "iteration: 102500 loss: 0.0041 lr: 0.02\n",
            "iteration: 102600 loss: 0.0055 lr: 0.02\n",
            "iteration: 102700 loss: 0.0045 lr: 0.02\n",
            "iteration: 102800 loss: 0.0044 lr: 0.02\n",
            "iteration: 102900 loss: 0.0047 lr: 0.02\n",
            "iteration: 103000 loss: 0.0046 lr: 0.02\n",
            "iteration: 103100 loss: 0.0048 lr: 0.02\n",
            "iteration: 103200 loss: 0.0042 lr: 0.02\n",
            "iteration: 103300 loss: 0.0043 lr: 0.02\n",
            "iteration: 103400 loss: 0.0047 lr: 0.02\n",
            "iteration: 103500 loss: 0.0043 lr: 0.02\n",
            "iteration: 103600 loss: 0.0046 lr: 0.02\n",
            "iteration: 103700 loss: 0.0058 lr: 0.02\n",
            "iteration: 103800 loss: 0.0044 lr: 0.02\n",
            "iteration: 103900 loss: 0.0043 lr: 0.02\n",
            "iteration: 104000 loss: 0.0051 lr: 0.02\n",
            "iteration: 104100 loss: 0.0053 lr: 0.02\n",
            "iteration: 104200 loss: 0.0044 lr: 0.02\n",
            "iteration: 104300 loss: 0.0042 lr: 0.02\n",
            "iteration: 104400 loss: 0.0043 lr: 0.02\n",
            "iteration: 104500 loss: 0.0039 lr: 0.02\n",
            "iteration: 104600 loss: 0.0043 lr: 0.02\n",
            "iteration: 104700 loss: 0.0043 lr: 0.02\n",
            "iteration: 104800 loss: 0.0049 lr: 0.02\n",
            "iteration: 104900 loss: 0.0044 lr: 0.02\n",
            "iteration: 105000 loss: 0.0044 lr: 0.02\n",
            "iteration: 105100 loss: 0.0051 lr: 0.02\n",
            "iteration: 105200 loss: 0.0048 lr: 0.02\n",
            "iteration: 105300 loss: 0.0044 lr: 0.02\n",
            "iteration: 105400 loss: 0.0041 lr: 0.02\n",
            "iteration: 105500 loss: 0.0051 lr: 0.02\n",
            "iteration: 105600 loss: 0.0046 lr: 0.02\n",
            "iteration: 105700 loss: 0.0051 lr: 0.02\n",
            "iteration: 105800 loss: 0.0049 lr: 0.02\n",
            "iteration: 105900 loss: 0.0043 lr: 0.02\n",
            "iteration: 106000 loss: 0.0056 lr: 0.02\n",
            "iteration: 106100 loss: 0.0045 lr: 0.02\n",
            "iteration: 106200 loss: 0.0040 lr: 0.02\n",
            "iteration: 106300 loss: 0.0051 lr: 0.02\n",
            "iteration: 106400 loss: 0.0046 lr: 0.02\n",
            "iteration: 106500 loss: 0.0048 lr: 0.02\n",
            "iteration: 106600 loss: 0.0043 lr: 0.02\n",
            "iteration: 106700 loss: 0.0048 lr: 0.02\n",
            "iteration: 106800 loss: 0.0048 lr: 0.02\n",
            "iteration: 106900 loss: 0.0043 lr: 0.02\n",
            "iteration: 107000 loss: 0.0038 lr: 0.02\n",
            "iteration: 107100 loss: 0.0042 lr: 0.02\n",
            "iteration: 107200 loss: 0.0043 lr: 0.02\n",
            "iteration: 107300 loss: 0.0045 lr: 0.02\n",
            "iteration: 107400 loss: 0.0049 lr: 0.02\n",
            "iteration: 107500 loss: 0.0049 lr: 0.02\n",
            "iteration: 107600 loss: 0.0047 lr: 0.02\n",
            "iteration: 107700 loss: 0.0043 lr: 0.02\n",
            "iteration: 107800 loss: 0.0049 lr: 0.02\n",
            "iteration: 107900 loss: 0.0051 lr: 0.02\n",
            "iteration: 108000 loss: 0.0041 lr: 0.02\n",
            "iteration: 108100 loss: 0.0040 lr: 0.02\n",
            "iteration: 108200 loss: 0.0046 lr: 0.02\n",
            "iteration: 108300 loss: 0.0046 lr: 0.02\n",
            "iteration: 108400 loss: 0.0038 lr: 0.02\n",
            "iteration: 108500 loss: 0.0047 lr: 0.02\n",
            "iteration: 108600 loss: 0.0043 lr: 0.02\n",
            "iteration: 108700 loss: 0.0044 lr: 0.02\n",
            "iteration: 108800 loss: 0.0047 lr: 0.02\n",
            "iteration: 108900 loss: 0.0054 lr: 0.02\n",
            "iteration: 109000 loss: 0.0041 lr: 0.02\n",
            "iteration: 109100 loss: 0.0036 lr: 0.02\n",
            "iteration: 109200 loss: 0.0040 lr: 0.02\n",
            "iteration: 109300 loss: 0.0045 lr: 0.02\n",
            "iteration: 109400 loss: 0.0037 lr: 0.02\n",
            "iteration: 109500 loss: 0.0037 lr: 0.02\n",
            "iteration: 109600 loss: 0.0046 lr: 0.02\n",
            "iteration: 109700 loss: 0.0048 lr: 0.02\n",
            "iteration: 109800 loss: 0.0049 lr: 0.02\n",
            "iteration: 109900 loss: 0.0044 lr: 0.02\n",
            "iteration: 110000 loss: 0.0050 lr: 0.02\n",
            "iteration: 110100 loss: 0.0040 lr: 0.02\n",
            "iteration: 110200 loss: 0.0046 lr: 0.02\n",
            "iteration: 110300 loss: 0.0043 lr: 0.02\n",
            "iteration: 110400 loss: 0.0048 lr: 0.02\n",
            "iteration: 110500 loss: 0.0045 lr: 0.02\n",
            "iteration: 110600 loss: 0.0051 lr: 0.02\n",
            "iteration: 110700 loss: 0.0044 lr: 0.02\n",
            "iteration: 110800 loss: 0.0042 lr: 0.02\n",
            "iteration: 110900 loss: 0.0045 lr: 0.02\n",
            "iteration: 111000 loss: 0.0041 lr: 0.02\n",
            "iteration: 111100 loss: 0.0042 lr: 0.02\n",
            "iteration: 111200 loss: 0.0046 lr: 0.02\n",
            "iteration: 111300 loss: 0.0041 lr: 0.02\n",
            "iteration: 111400 loss: 0.0044 lr: 0.02\n",
            "iteration: 111500 loss: 0.0039 lr: 0.02\n",
            "iteration: 111600 loss: 0.0043 lr: 0.02\n",
            "iteration: 111700 loss: 0.0038 lr: 0.02\n",
            "iteration: 111800 loss: 0.0044 lr: 0.02\n",
            "iteration: 111900 loss: 0.0044 lr: 0.02\n",
            "iteration: 112000 loss: 0.0048 lr: 0.02\n",
            "iteration: 112100 loss: 0.0041 lr: 0.02\n",
            "iteration: 112200 loss: 0.0043 lr: 0.02\n",
            "iteration: 112300 loss: 0.0039 lr: 0.02\n",
            "iteration: 112400 loss: 0.0048 lr: 0.02\n",
            "iteration: 112500 loss: 0.0047 lr: 0.02\n",
            "iteration: 112600 loss: 0.0043 lr: 0.02\n",
            "iteration: 112700 loss: 0.0044 lr: 0.02\n",
            "iteration: 112800 loss: 0.0046 lr: 0.02\n",
            "iteration: 112900 loss: 0.0041 lr: 0.02\n",
            "iteration: 113000 loss: 0.0045 lr: 0.02\n",
            "iteration: 113100 loss: 0.0041 lr: 0.02\n",
            "iteration: 113200 loss: 0.0047 lr: 0.02\n",
            "iteration: 113300 loss: 0.0045 lr: 0.02\n",
            "iteration: 113400 loss: 0.0040 lr: 0.02\n",
            "iteration: 113500 loss: 0.0037 lr: 0.02\n",
            "iteration: 113600 loss: 0.0039 lr: 0.02\n",
            "iteration: 113700 loss: 0.0048 lr: 0.02\n",
            "iteration: 113800 loss: 0.0048 lr: 0.02\n",
            "iteration: 113900 loss: 0.0043 lr: 0.02\n",
            "iteration: 114000 loss: 0.0047 lr: 0.02\n",
            "iteration: 114100 loss: 0.0047 lr: 0.02\n",
            "iteration: 114200 loss: 0.0043 lr: 0.02\n",
            "iteration: 114300 loss: 0.0040 lr: 0.02\n",
            "iteration: 114400 loss: 0.0040 lr: 0.02\n",
            "iteration: 114500 loss: 0.0040 lr: 0.02\n",
            "iteration: 114600 loss: 0.0042 lr: 0.02\n",
            "iteration: 114700 loss: 0.0036 lr: 0.02\n",
            "iteration: 114800 loss: 0.0057 lr: 0.02\n",
            "iteration: 114900 loss: 0.0043 lr: 0.02\n",
            "iteration: 115000 loss: 0.0048 lr: 0.02\n",
            "iteration: 115100 loss: 0.0041 lr: 0.02\n",
            "iteration: 115200 loss: 0.0043 lr: 0.02\n",
            "iteration: 115300 loss: 0.0042 lr: 0.02\n",
            "iteration: 115400 loss: 0.0042 lr: 0.02\n",
            "iteration: 115500 loss: 0.0044 lr: 0.02\n",
            "iteration: 115600 loss: 0.0046 lr: 0.02\n",
            "iteration: 115700 loss: 0.0045 lr: 0.02\n",
            "iteration: 115800 loss: 0.0046 lr: 0.02\n",
            "iteration: 115900 loss: 0.0037 lr: 0.02\n",
            "iteration: 116000 loss: 0.0041 lr: 0.02\n",
            "iteration: 116100 loss: 0.0042 lr: 0.02\n",
            "iteration: 116200 loss: 0.0046 lr: 0.02\n",
            "iteration: 116300 loss: 0.0046 lr: 0.02\n",
            "iteration: 116400 loss: 0.0043 lr: 0.02\n",
            "iteration: 116500 loss: 0.0039 lr: 0.02\n",
            "iteration: 116600 loss: 0.0038 lr: 0.02\n",
            "iteration: 116700 loss: 0.0051 lr: 0.02\n",
            "iteration: 116800 loss: 0.0038 lr: 0.02\n",
            "iteration: 116900 loss: 0.0042 lr: 0.02\n",
            "iteration: 117000 loss: 0.0037 lr: 0.02\n",
            "iteration: 117100 loss: 0.0042 lr: 0.02\n",
            "iteration: 117200 loss: 0.0044 lr: 0.02\n",
            "iteration: 117300 loss: 0.0044 lr: 0.02\n",
            "iteration: 117400 loss: 0.0046 lr: 0.02\n",
            "iteration: 117500 loss: 0.0046 lr: 0.02\n",
            "iteration: 117600 loss: 0.0037 lr: 0.02\n",
            "iteration: 117700 loss: 0.0040 lr: 0.02\n",
            "iteration: 117800 loss: 0.0038 lr: 0.02\n",
            "iteration: 117900 loss: 0.0035 lr: 0.02\n",
            "iteration: 118000 loss: 0.0043 lr: 0.02\n",
            "iteration: 118100 loss: 0.0040 lr: 0.02\n",
            "iteration: 118200 loss: 0.0044 lr: 0.02\n",
            "iteration: 118300 loss: 0.0039 lr: 0.02\n",
            "iteration: 118400 loss: 0.0047 lr: 0.02\n",
            "iteration: 118500 loss: 0.0041 lr: 0.02\n",
            "iteration: 118600 loss: 0.0044 lr: 0.02\n",
            "iteration: 118700 loss: 0.0044 lr: 0.02\n",
            "iteration: 118800 loss: 0.0045 lr: 0.02\n",
            "iteration: 118900 loss: 0.0045 lr: 0.02\n",
            "iteration: 119000 loss: 0.0048 lr: 0.02\n",
            "iteration: 119100 loss: 0.0046 lr: 0.02\n",
            "iteration: 119200 loss: 0.0048 lr: 0.02\n",
            "iteration: 119300 loss: 0.0038 lr: 0.02\n",
            "iteration: 119400 loss: 0.0042 lr: 0.02\n",
            "iteration: 119500 loss: 0.0042 lr: 0.02\n",
            "iteration: 119600 loss: 0.0042 lr: 0.02\n",
            "iteration: 119700 loss: 0.0046 lr: 0.02\n",
            "iteration: 119800 loss: 0.0049 lr: 0.02\n",
            "iteration: 119900 loss: 0.0049 lr: 0.02\n",
            "iteration: 120000 loss: 0.0039 lr: 0.02\n",
            "iteration: 120100 loss: 0.0046 lr: 0.02\n",
            "iteration: 120200 loss: 0.0045 lr: 0.02\n",
            "iteration: 120300 loss: 0.0042 lr: 0.02\n",
            "iteration: 120400 loss: 0.0039 lr: 0.02\n",
            "iteration: 120500 loss: 0.0042 lr: 0.02\n",
            "iteration: 120600 loss: 0.0039 lr: 0.02\n",
            "iteration: 120700 loss: 0.0041 lr: 0.02\n",
            "iteration: 120800 loss: 0.0041 lr: 0.02\n",
            "iteration: 120900 loss: 0.0038 lr: 0.02\n",
            "iteration: 121000 loss: 0.0040 lr: 0.02\n",
            "iteration: 121100 loss: 0.0042 lr: 0.02\n",
            "iteration: 121200 loss: 0.0042 lr: 0.02\n",
            "iteration: 121300 loss: 0.0045 lr: 0.02\n",
            "iteration: 121400 loss: 0.0042 lr: 0.02\n",
            "iteration: 121500 loss: 0.0041 lr: 0.02\n",
            "iteration: 121600 loss: 0.0045 lr: 0.02\n",
            "iteration: 121700 loss: 0.0040 lr: 0.02\n",
            "iteration: 121800 loss: 0.0043 lr: 0.02\n",
            "iteration: 121900 loss: 0.0043 lr: 0.02\n",
            "iteration: 122000 loss: 0.0044 lr: 0.02\n",
            "iteration: 122100 loss: 0.0045 lr: 0.02\n",
            "iteration: 122200 loss: 0.0043 lr: 0.02\n",
            "iteration: 122300 loss: 0.0040 lr: 0.02\n",
            "iteration: 122400 loss: 0.0043 lr: 0.02\n",
            "iteration: 122500 loss: 0.0040 lr: 0.02\n",
            "iteration: 122600 loss: 0.0032 lr: 0.02\n",
            "iteration: 122700 loss: 0.0039 lr: 0.02\n",
            "iteration: 122800 loss: 0.0039 lr: 0.02\n",
            "iteration: 122900 loss: 0.0039 lr: 0.02\n",
            "iteration: 123000 loss: 0.0037 lr: 0.02\n",
            "iteration: 123100 loss: 0.0047 lr: 0.02\n",
            "iteration: 123200 loss: 0.0038 lr: 0.02\n",
            "iteration: 123300 loss: 0.0040 lr: 0.02\n",
            "iteration: 123400 loss: 0.0044 lr: 0.02\n",
            "iteration: 123500 loss: 0.0039 lr: 0.02\n",
            "iteration: 123600 loss: 0.0039 lr: 0.02\n",
            "iteration: 123700 loss: 0.0048 lr: 0.02\n",
            "iteration: 123800 loss: 0.0035 lr: 0.02\n",
            "iteration: 123900 loss: 0.0035 lr: 0.02\n",
            "iteration: 124000 loss: 0.0045 lr: 0.02\n",
            "iteration: 124100 loss: 0.0043 lr: 0.02\n",
            "iteration: 124200 loss: 0.0043 lr: 0.02\n",
            "iteration: 124300 loss: 0.0041 lr: 0.02\n",
            "iteration: 124400 loss: 0.0043 lr: 0.02\n",
            "iteration: 124500 loss: 0.0044 lr: 0.02\n",
            "iteration: 124600 loss: 0.0042 lr: 0.02\n",
            "iteration: 124700 loss: 0.0041 lr: 0.02\n",
            "iteration: 124800 loss: 0.0046 lr: 0.02\n",
            "iteration: 124900 loss: 0.0045 lr: 0.02\n",
            "iteration: 125000 loss: 0.0050 lr: 0.02\n",
            "iteration: 125100 loss: 0.0040 lr: 0.02\n",
            "iteration: 125200 loss: 0.0042 lr: 0.02\n",
            "iteration: 125300 loss: 0.0041 lr: 0.02\n",
            "iteration: 125400 loss: 0.0045 lr: 0.02\n",
            "iteration: 125500 loss: 0.0042 lr: 0.02\n",
            "iteration: 125600 loss: 0.0043 lr: 0.02\n",
            "iteration: 125700 loss: 0.0041 lr: 0.02\n",
            "iteration: 125800 loss: 0.0044 lr: 0.02\n",
            "iteration: 125900 loss: 0.0040 lr: 0.02\n",
            "iteration: 126000 loss: 0.0040 lr: 0.02\n",
            "iteration: 126100 loss: 0.0040 lr: 0.02\n",
            "iteration: 126200 loss: 0.0043 lr: 0.02\n",
            "iteration: 126300 loss: 0.0040 lr: 0.02\n",
            "iteration: 126400 loss: 0.0041 lr: 0.02\n",
            "iteration: 126500 loss: 0.0037 lr: 0.02\n",
            "iteration: 126600 loss: 0.0039 lr: 0.02\n",
            "iteration: 126700 loss: 0.0042 lr: 0.02\n",
            "iteration: 126800 loss: 0.0039 lr: 0.02\n",
            "iteration: 126900 loss: 0.0041 lr: 0.02\n",
            "iteration: 127000 loss: 0.0038 lr: 0.02\n",
            "iteration: 127100 loss: 0.0037 lr: 0.02\n",
            "iteration: 127200 loss: 0.0040 lr: 0.02\n",
            "iteration: 127300 loss: 0.0045 lr: 0.02\n",
            "iteration: 127400 loss: 0.0042 lr: 0.02\n",
            "iteration: 127500 loss: 0.0047 lr: 0.02\n",
            "iteration: 127600 loss: 0.0040 lr: 0.02\n",
            "iteration: 127700 loss: 0.0037 lr: 0.02\n",
            "iteration: 127800 loss: 0.0041 lr: 0.02\n",
            "iteration: 127900 loss: 0.0039 lr: 0.02\n",
            "iteration: 128000 loss: 0.0036 lr: 0.02\n",
            "iteration: 128100 loss: 0.0042 lr: 0.02\n",
            "iteration: 128200 loss: 0.0043 lr: 0.02\n",
            "iteration: 128300 loss: 0.0046 lr: 0.02\n",
            "iteration: 128400 loss: 0.0040 lr: 0.02\n",
            "iteration: 128500 loss: 0.0039 lr: 0.02\n",
            "iteration: 128600 loss: 0.0044 lr: 0.02\n",
            "iteration: 128700 loss: 0.0046 lr: 0.02\n",
            "iteration: 128800 loss: 0.0045 lr: 0.02\n",
            "iteration: 128900 loss: 0.0036 lr: 0.02\n",
            "iteration: 129000 loss: 0.0047 lr: 0.02\n",
            "iteration: 129100 loss: 0.0040 lr: 0.02\n",
            "iteration: 129200 loss: 0.0045 lr: 0.02\n",
            "iteration: 129300 loss: 0.0039 lr: 0.02\n",
            "iteration: 129400 loss: 0.0040 lr: 0.02\n",
            "iteration: 129500 loss: 0.0039 lr: 0.02\n",
            "iteration: 129600 loss: 0.0039 lr: 0.02\n",
            "iteration: 129700 loss: 0.0039 lr: 0.02\n",
            "iteration: 129800 loss: 0.0034 lr: 0.02\n",
            "iteration: 129900 loss: 0.0039 lr: 0.02\n",
            "iteration: 130000 loss: 0.0042 lr: 0.02\n",
            "iteration: 130100 loss: 0.0039 lr: 0.02\n",
            "iteration: 130200 loss: 0.0037 lr: 0.02\n",
            "iteration: 130300 loss: 0.0037 lr: 0.02\n",
            "iteration: 130400 loss: 0.0036 lr: 0.02\n",
            "iteration: 130500 loss: 0.0044 lr: 0.02\n",
            "iteration: 130600 loss: 0.0051 lr: 0.02\n",
            "iteration: 130700 loss: 0.0037 lr: 0.02\n",
            "iteration: 130800 loss: 0.0046 lr: 0.02\n",
            "iteration: 130900 loss: 0.0043 lr: 0.02\n",
            "iteration: 131000 loss: 0.0037 lr: 0.02\n",
            "iteration: 131100 loss: 0.0039 lr: 0.02\n",
            "iteration: 131200 loss: 0.0042 lr: 0.02\n",
            "iteration: 131300 loss: 0.0045 lr: 0.02\n",
            "iteration: 131400 loss: 0.0045 lr: 0.02\n",
            "iteration: 131500 loss: 0.0041 lr: 0.02\n",
            "iteration: 131600 loss: 0.0042 lr: 0.02\n",
            "iteration: 131700 loss: 0.0044 lr: 0.02\n",
            "iteration: 131800 loss: 0.0044 lr: 0.02\n",
            "iteration: 131900 loss: 0.0039 lr: 0.02\n",
            "iteration: 132000 loss: 0.0037 lr: 0.02\n",
            "iteration: 132100 loss: 0.0041 lr: 0.02\n",
            "iteration: 132200 loss: 0.0039 lr: 0.02\n",
            "iteration: 132300 loss: 0.0039 lr: 0.02\n",
            "iteration: 132400 loss: 0.0041 lr: 0.02\n",
            "iteration: 132500 loss: 0.0042 lr: 0.02\n",
            "iteration: 132600 loss: 0.0048 lr: 0.02\n",
            "iteration: 132700 loss: 0.0038 lr: 0.02\n",
            "iteration: 132800 loss: 0.0040 lr: 0.02\n",
            "iteration: 132900 loss: 0.0042 lr: 0.02\n",
            "iteration: 133000 loss: 0.0043 lr: 0.02\n",
            "iteration: 133100 loss: 0.0046 lr: 0.02\n",
            "iteration: 133200 loss: 0.0041 lr: 0.02\n",
            "iteration: 133300 loss: 0.0039 lr: 0.02\n",
            "iteration: 133400 loss: 0.0040 lr: 0.02\n",
            "iteration: 133500 loss: 0.0045 lr: 0.02\n",
            "iteration: 133600 loss: 0.0046 lr: 0.02\n",
            "iteration: 133700 loss: 0.0049 lr: 0.02\n",
            "iteration: 133800 loss: 0.0044 lr: 0.02\n",
            "iteration: 133900 loss: 0.0043 lr: 0.02\n",
            "iteration: 134000 loss: 0.0042 lr: 0.02\n",
            "iteration: 134100 loss: 0.0038 lr: 0.02\n",
            "iteration: 134200 loss: 0.0044 lr: 0.02\n",
            "iteration: 134300 loss: 0.0042 lr: 0.02\n",
            "iteration: 134400 loss: 0.0046 lr: 0.02\n",
            "iteration: 134500 loss: 0.0044 lr: 0.02\n",
            "iteration: 134600 loss: 0.0036 lr: 0.02\n",
            "iteration: 134700 loss: 0.0039 lr: 0.02\n",
            "iteration: 134800 loss: 0.0037 lr: 0.02\n",
            "iteration: 134900 loss: 0.0041 lr: 0.02\n",
            "iteration: 135000 loss: 0.0043 lr: 0.02\n",
            "iteration: 135100 loss: 0.0042 lr: 0.02\n",
            "iteration: 135200 loss: 0.0040 lr: 0.02\n",
            "iteration: 135300 loss: 0.0040 lr: 0.02\n",
            "iteration: 135400 loss: 0.0048 lr: 0.02\n",
            "iteration: 135500 loss: 0.0037 lr: 0.02\n",
            "iteration: 135600 loss: 0.0041 lr: 0.02\n",
            "iteration: 135700 loss: 0.0040 lr: 0.02\n",
            "iteration: 135800 loss: 0.0041 lr: 0.02\n",
            "iteration: 135900 loss: 0.0040 lr: 0.02\n",
            "iteration: 136000 loss: 0.0044 lr: 0.02\n",
            "iteration: 136100 loss: 0.0042 lr: 0.02\n",
            "iteration: 136200 loss: 0.0041 lr: 0.02\n",
            "iteration: 136300 loss: 0.0042 lr: 0.02\n",
            "iteration: 136400 loss: 0.0042 lr: 0.02\n",
            "iteration: 136500 loss: 0.0043 lr: 0.02\n",
            "iteration: 136600 loss: 0.0036 lr: 0.02\n",
            "iteration: 136700 loss: 0.0043 lr: 0.02\n",
            "iteration: 136800 loss: 0.0039 lr: 0.02\n",
            "iteration: 136900 loss: 0.0037 lr: 0.02\n",
            "iteration: 137000 loss: 0.0045 lr: 0.02\n",
            "iteration: 137100 loss: 0.0041 lr: 0.02\n",
            "iteration: 137200 loss: 0.0040 lr: 0.02\n",
            "iteration: 137300 loss: 0.0036 lr: 0.02\n",
            "iteration: 137400 loss: 0.0044 lr: 0.02\n",
            "iteration: 137500 loss: 0.0043 lr: 0.02\n",
            "iteration: 137600 loss: 0.0040 lr: 0.02\n",
            "iteration: 137700 loss: 0.0043 lr: 0.02\n",
            "iteration: 137800 loss: 0.0043 lr: 0.02\n",
            "iteration: 137900 loss: 0.0039 lr: 0.02\n",
            "iteration: 138000 loss: 0.0039 lr: 0.02\n",
            "iteration: 138100 loss: 0.0047 lr: 0.02\n",
            "iteration: 138200 loss: 0.0040 lr: 0.02\n",
            "iteration: 138300 loss: 0.0042 lr: 0.02\n",
            "iteration: 138400 loss: 0.0042 lr: 0.02\n",
            "iteration: 138500 loss: 0.0035 lr: 0.02\n",
            "iteration: 138600 loss: 0.0037 lr: 0.02\n",
            "iteration: 138700 loss: 0.0035 lr: 0.02\n",
            "iteration: 138800 loss: 0.0035 lr: 0.02\n",
            "iteration: 138900 loss: 0.0039 lr: 0.02\n",
            "iteration: 139000 loss: 0.0041 lr: 0.02\n",
            "iteration: 139100 loss: 0.0037 lr: 0.02\n",
            "iteration: 139200 loss: 0.0043 lr: 0.02\n",
            "iteration: 139300 loss: 0.0043 lr: 0.02\n",
            "iteration: 139400 loss: 0.0047 lr: 0.02\n",
            "iteration: 139500 loss: 0.0037 lr: 0.02\n",
            "iteration: 139600 loss: 0.0034 lr: 0.02\n",
            "iteration: 139700 loss: 0.0039 lr: 0.02\n",
            "iteration: 139800 loss: 0.0046 lr: 0.02\n",
            "iteration: 139900 loss: 0.0041 lr: 0.02\n",
            "iteration: 140000 loss: 0.0043 lr: 0.02\n",
            "iteration: 140100 loss: 0.0041 lr: 0.02\n",
            "iteration: 140200 loss: 0.0042 lr: 0.02\n",
            "iteration: 140300 loss: 0.0043 lr: 0.02\n",
            "iteration: 140400 loss: 0.0038 lr: 0.02\n",
            "iteration: 140500 loss: 0.0042 lr: 0.02\n",
            "iteration: 140600 loss: 0.0041 lr: 0.02\n",
            "iteration: 140700 loss: 0.0039 lr: 0.02\n",
            "iteration: 140800 loss: 0.0049 lr: 0.02\n",
            "iteration: 140900 loss: 0.0035 lr: 0.02\n",
            "iteration: 141000 loss: 0.0039 lr: 0.02\n",
            "iteration: 141100 loss: 0.0038 lr: 0.02\n",
            "iteration: 141200 loss: 0.0040 lr: 0.02\n",
            "iteration: 141300 loss: 0.0036 lr: 0.02\n",
            "iteration: 141400 loss: 0.0036 lr: 0.02\n",
            "iteration: 141500 loss: 0.0034 lr: 0.02\n",
            "iteration: 141600 loss: 0.0039 lr: 0.02\n",
            "iteration: 141700 loss: 0.0044 lr: 0.02\n",
            "iteration: 141800 loss: 0.0036 lr: 0.02\n",
            "iteration: 141900 loss: 0.0036 lr: 0.02\n",
            "iteration: 142000 loss: 0.0038 lr: 0.02\n",
            "iteration: 142100 loss: 0.0037 lr: 0.02\n",
            "iteration: 142200 loss: 0.0039 lr: 0.02\n",
            "iteration: 142300 loss: 0.0046 lr: 0.02\n",
            "iteration: 142400 loss: 0.0036 lr: 0.02\n",
            "iteration: 142500 loss: 0.0042 lr: 0.02\n",
            "iteration: 142600 loss: 0.0037 lr: 0.02\n",
            "iteration: 142700 loss: 0.0041 lr: 0.02\n",
            "iteration: 142800 loss: 0.0034 lr: 0.02\n",
            "iteration: 142900 loss: 0.0036 lr: 0.02\n",
            "iteration: 143000 loss: 0.0037 lr: 0.02\n",
            "iteration: 143100 loss: 0.0034 lr: 0.02\n",
            "iteration: 143200 loss: 0.0043 lr: 0.02\n",
            "iteration: 143300 loss: 0.0040 lr: 0.02\n",
            "iteration: 143400 loss: 0.0043 lr: 0.02\n",
            "iteration: 143500 loss: 0.0038 lr: 0.02\n",
            "iteration: 143600 loss: 0.0043 lr: 0.02\n",
            "iteration: 143700 loss: 0.0036 lr: 0.02\n",
            "iteration: 143800 loss: 0.0038 lr: 0.02\n",
            "iteration: 143900 loss: 0.0037 lr: 0.02\n",
            "iteration: 144000 loss: 0.0040 lr: 0.02\n",
            "iteration: 144100 loss: 0.0041 lr: 0.02\n",
            "iteration: 144200 loss: 0.0036 lr: 0.02\n",
            "iteration: 144300 loss: 0.0036 lr: 0.02\n",
            "iteration: 144400 loss: 0.0034 lr: 0.02\n",
            "iteration: 144500 loss: 0.0037 lr: 0.02\n",
            "iteration: 144600 loss: 0.0038 lr: 0.02\n",
            "iteration: 144700 loss: 0.0047 lr: 0.02\n",
            "iteration: 144800 loss: 0.0033 lr: 0.02\n",
            "iteration: 144900 loss: 0.0030 lr: 0.02\n",
            "iteration: 145000 loss: 0.0040 lr: 0.02\n",
            "iteration: 145100 loss: 0.0034 lr: 0.02\n",
            "iteration: 145200 loss: 0.0036 lr: 0.02\n",
            "iteration: 145300 loss: 0.0036 lr: 0.02\n",
            "iteration: 145400 loss: 0.0037 lr: 0.02\n",
            "iteration: 145500 loss: 0.0039 lr: 0.02\n",
            "iteration: 145600 loss: 0.0042 lr: 0.02\n",
            "iteration: 145700 loss: 0.0032 lr: 0.02\n",
            "iteration: 145800 loss: 0.0037 lr: 0.02\n",
            "iteration: 145900 loss: 0.0036 lr: 0.02\n",
            "iteration: 146000 loss: 0.0038 lr: 0.02\n",
            "iteration: 146100 loss: 0.0037 lr: 0.02\n",
            "iteration: 146200 loss: 0.0038 lr: 0.02\n",
            "iteration: 146300 loss: 0.0033 lr: 0.02\n",
            "iteration: 146400 loss: 0.0040 lr: 0.02\n",
            "iteration: 146500 loss: 0.0039 lr: 0.02\n",
            "iteration: 146600 loss: 0.0041 lr: 0.02\n",
            "iteration: 146700 loss: 0.0047 lr: 0.02\n",
            "iteration: 146800 loss: 0.0037 lr: 0.02\n",
            "iteration: 146900 loss: 0.0034 lr: 0.02\n",
            "iteration: 147000 loss: 0.0042 lr: 0.02\n",
            "iteration: 147100 loss: 0.0042 lr: 0.02\n",
            "iteration: 147200 loss: 0.0041 lr: 0.02\n",
            "iteration: 147300 loss: 0.0041 lr: 0.02\n",
            "iteration: 147400 loss: 0.0039 lr: 0.02\n",
            "iteration: 147500 loss: 0.0038 lr: 0.02\n",
            "iteration: 147600 loss: 0.0044 lr: 0.02\n",
            "iteration: 147700 loss: 0.0044 lr: 0.02\n",
            "iteration: 147800 loss: 0.0041 lr: 0.02\n",
            "iteration: 147900 loss: 0.0042 lr: 0.02\n",
            "iteration: 148000 loss: 0.0048 lr: 0.02\n",
            "iteration: 148100 loss: 0.0038 lr: 0.02\n",
            "iteration: 148200 loss: 0.0035 lr: 0.02\n",
            "iteration: 148300 loss: 0.0039 lr: 0.02\n",
            "iteration: 148400 loss: 0.0037 lr: 0.02\n",
            "iteration: 148500 loss: 0.0042 lr: 0.02\n",
            "iteration: 148600 loss: 0.0042 lr: 0.02\n",
            "iteration: 148700 loss: 0.0034 lr: 0.02\n",
            "iteration: 148800 loss: 0.0042 lr: 0.02\n",
            "iteration: 148900 loss: 0.0044 lr: 0.02\n",
            "iteration: 149000 loss: 0.0036 lr: 0.02\n",
            "iteration: 149100 loss: 0.0032 lr: 0.02\n",
            "iteration: 149200 loss: 0.0037 lr: 0.02\n",
            "iteration: 149300 loss: 0.0038 lr: 0.02\n",
            "iteration: 149400 loss: 0.0041 lr: 0.02\n",
            "iteration: 149500 loss: 0.0043 lr: 0.02\n",
            "iteration: 149600 loss: 0.0039 lr: 0.02\n",
            "iteration: 149700 loss: 0.0038 lr: 0.02\n",
            "iteration: 149800 loss: 0.0036 lr: 0.02\n",
            "iteration: 149900 loss: 0.0042 lr: 0.02\n",
            "iteration: 150000 loss: 0.0038 lr: 0.02\n",
            "iteration: 150100 loss: 0.0037 lr: 0.02\n",
            "iteration: 150200 loss: 0.0039 lr: 0.02\n",
            "iteration: 150300 loss: 0.0035 lr: 0.02\n",
            "iteration: 150400 loss: 0.0034 lr: 0.02\n",
            "iteration: 150500 loss: 0.0043 lr: 0.02\n",
            "iteration: 150600 loss: 0.0042 lr: 0.02\n",
            "iteration: 150700 loss: 0.0036 lr: 0.02\n",
            "iteration: 150800 loss: 0.0042 lr: 0.02\n",
            "iteration: 150900 loss: 0.0037 lr: 0.02\n",
            "iteration: 151000 loss: 0.0044 lr: 0.02\n",
            "iteration: 151100 loss: 0.0033 lr: 0.02\n",
            "iteration: 151200 loss: 0.0039 lr: 0.02\n",
            "iteration: 151300 loss: 0.0033 lr: 0.02\n",
            "iteration: 151400 loss: 0.0041 lr: 0.02\n",
            "iteration: 151500 loss: 0.0038 lr: 0.02\n",
            "iteration: 151600 loss: 0.0032 lr: 0.02\n",
            "iteration: 151700 loss: 0.0039 lr: 0.02\n",
            "iteration: 151800 loss: 0.0039 lr: 0.02\n",
            "iteration: 151900 loss: 0.0041 lr: 0.02\n",
            "iteration: 152000 loss: 0.0039 lr: 0.02\n",
            "iteration: 152100 loss: 0.0041 lr: 0.02\n",
            "iteration: 152200 loss: 0.0040 lr: 0.02\n",
            "iteration: 152300 loss: 0.0035 lr: 0.02\n",
            "iteration: 152400 loss: 0.0042 lr: 0.02\n",
            "iteration: 152500 loss: 0.0043 lr: 0.02\n",
            "iteration: 152600 loss: 0.0037 lr: 0.02\n",
            "iteration: 152700 loss: 0.0042 lr: 0.02\n",
            "iteration: 152800 loss: 0.0036 lr: 0.02\n",
            "iteration: 152900 loss: 0.0039 lr: 0.02\n",
            "iteration: 153000 loss: 0.0043 lr: 0.02\n",
            "iteration: 153100 loss: 0.0040 lr: 0.02\n",
            "iteration: 153200 loss: 0.0034 lr: 0.02\n",
            "iteration: 153300 loss: 0.0034 lr: 0.02\n",
            "iteration: 153400 loss: 0.0038 lr: 0.02\n",
            "iteration: 153500 loss: 0.0041 lr: 0.02\n",
            "iteration: 153600 loss: 0.0041 lr: 0.02\n",
            "iteration: 153700 loss: 0.0037 lr: 0.02\n",
            "iteration: 153800 loss: 0.0040 lr: 0.02\n",
            "iteration: 153900 loss: 0.0035 lr: 0.02\n",
            "iteration: 154000 loss: 0.0037 lr: 0.02\n",
            "iteration: 154100 loss: 0.0035 lr: 0.02\n",
            "iteration: 154200 loss: 0.0037 lr: 0.02\n",
            "iteration: 154300 loss: 0.0038 lr: 0.02\n",
            "iteration: 154400 loss: 0.0050 lr: 0.02\n",
            "iteration: 154500 loss: 0.0039 lr: 0.02\n",
            "iteration: 154600 loss: 0.0036 lr: 0.02\n",
            "iteration: 154700 loss: 0.0035 lr: 0.02\n",
            "iteration: 154800 loss: 0.0046 lr: 0.02\n",
            "iteration: 154900 loss: 0.0043 lr: 0.02\n",
            "iteration: 155000 loss: 0.0040 lr: 0.02\n",
            "iteration: 155100 loss: 0.0040 lr: 0.02\n",
            "iteration: 155200 loss: 0.0034 lr: 0.02\n",
            "iteration: 155300 loss: 0.0036 lr: 0.02\n",
            "iteration: 155400 loss: 0.0042 lr: 0.02\n",
            "iteration: 155500 loss: 0.0041 lr: 0.02\n",
            "iteration: 155600 loss: 0.0039 lr: 0.02\n",
            "iteration: 155700 loss: 0.0036 lr: 0.02\n",
            "iteration: 155800 loss: 0.0035 lr: 0.02\n",
            "iteration: 155900 loss: 0.0039 lr: 0.02\n",
            "iteration: 156000 loss: 0.0036 lr: 0.02\n",
            "iteration: 156100 loss: 0.0045 lr: 0.02\n",
            "iteration: 156200 loss: 0.0045 lr: 0.02\n",
            "iteration: 156300 loss: 0.0034 lr: 0.02\n",
            "iteration: 156400 loss: 0.0039 lr: 0.02\n",
            "iteration: 156500 loss: 0.0039 lr: 0.02\n",
            "iteration: 156600 loss: 0.0038 lr: 0.02\n",
            "iteration: 156700 loss: 0.0037 lr: 0.02\n",
            "iteration: 156800 loss: 0.0039 lr: 0.02\n",
            "iteration: 156900 loss: 0.0036 lr: 0.02\n",
            "iteration: 157000 loss: 0.0040 lr: 0.02\n",
            "iteration: 157100 loss: 0.0040 lr: 0.02\n",
            "iteration: 157200 loss: 0.0037 lr: 0.02\n",
            "iteration: 157300 loss: 0.0036 lr: 0.02\n",
            "iteration: 157400 loss: 0.0036 lr: 0.02\n",
            "iteration: 157500 loss: 0.0037 lr: 0.02\n",
            "iteration: 157600 loss: 0.0043 lr: 0.02\n",
            "iteration: 157700 loss: 0.0038 lr: 0.02\n",
            "iteration: 157800 loss: 0.0026 lr: 0.02\n",
            "iteration: 157900 loss: 0.0041 lr: 0.02\n",
            "iteration: 158000 loss: 0.0035 lr: 0.02\n",
            "iteration: 158100 loss: 0.0037 lr: 0.02\n",
            "iteration: 158200 loss: 0.0034 lr: 0.02\n",
            "iteration: 158300 loss: 0.0042 lr: 0.02\n",
            "iteration: 158400 loss: 0.0043 lr: 0.02\n",
            "iteration: 158500 loss: 0.0044 lr: 0.02\n",
            "iteration: 158600 loss: 0.0039 lr: 0.02\n",
            "iteration: 158700 loss: 0.0041 lr: 0.02\n",
            "iteration: 158800 loss: 0.0033 lr: 0.02\n",
            "iteration: 158900 loss: 0.0041 lr: 0.02\n",
            "iteration: 159000 loss: 0.0040 lr: 0.02\n",
            "iteration: 159100 loss: 0.0033 lr: 0.02\n",
            "iteration: 159200 loss: 0.0034 lr: 0.02\n",
            "iteration: 159300 loss: 0.0035 lr: 0.02\n",
            "iteration: 159400 loss: 0.0036 lr: 0.02\n",
            "iteration: 159500 loss: 0.0037 lr: 0.02\n",
            "iteration: 159600 loss: 0.0032 lr: 0.02\n",
            "iteration: 159700 loss: 0.0039 lr: 0.02\n",
            "iteration: 159800 loss: 0.0040 lr: 0.02\n",
            "iteration: 159900 loss: 0.0036 lr: 0.02\n",
            "iteration: 160000 loss: 0.0036 lr: 0.02\n",
            "iteration: 160100 loss: 0.0039 lr: 0.02\n",
            "iteration: 160200 loss: 0.0033 lr: 0.02\n",
            "iteration: 160300 loss: 0.0041 lr: 0.02\n",
            "iteration: 160400 loss: 0.0036 lr: 0.02\n",
            "iteration: 160500 loss: 0.0036 lr: 0.02\n",
            "iteration: 160600 loss: 0.0036 lr: 0.02\n",
            "iteration: 160700 loss: 0.0034 lr: 0.02\n",
            "iteration: 160800 loss: 0.0043 lr: 0.02\n",
            "iteration: 160900 loss: 0.0041 lr: 0.02\n",
            "iteration: 161000 loss: 0.0035 lr: 0.02\n",
            "iteration: 161100 loss: 0.0037 lr: 0.02\n",
            "iteration: 161200 loss: 0.0034 lr: 0.02\n",
            "iteration: 161300 loss: 0.0036 lr: 0.02\n",
            "iteration: 161400 loss: 0.0039 lr: 0.02\n",
            "iteration: 161500 loss: 0.0042 lr: 0.02\n",
            "iteration: 161600 loss: 0.0035 lr: 0.02\n",
            "iteration: 161700 loss: 0.0044 lr: 0.02\n",
            "iteration: 161800 loss: 0.0042 lr: 0.02\n",
            "iteration: 161900 loss: 0.0038 lr: 0.02\n",
            "iteration: 162000 loss: 0.0035 lr: 0.02\n",
            "iteration: 162100 loss: 0.0033 lr: 0.02\n",
            "iteration: 162200 loss: 0.0034 lr: 0.02\n",
            "iteration: 162300 loss: 0.0037 lr: 0.02\n",
            "iteration: 162400 loss: 0.0030 lr: 0.02\n",
            "iteration: 162500 loss: 0.0038 lr: 0.02\n",
            "iteration: 162600 loss: 0.0032 lr: 0.02\n",
            "iteration: 162700 loss: 0.0033 lr: 0.02\n",
            "iteration: 162800 loss: 0.0030 lr: 0.02\n",
            "iteration: 162900 loss: 0.0033 lr: 0.02\n",
            "iteration: 163000 loss: 0.0033 lr: 0.02\n",
            "iteration: 163100 loss: 0.0033 lr: 0.02\n",
            "iteration: 163200 loss: 0.0036 lr: 0.02\n",
            "iteration: 163300 loss: 0.0037 lr: 0.02\n",
            "iteration: 163400 loss: 0.0033 lr: 0.02\n",
            "iteration: 163500 loss: 0.0036 lr: 0.02\n",
            "iteration: 163600 loss: 0.0035 lr: 0.02\n",
            "iteration: 163700 loss: 0.0039 lr: 0.02\n",
            "iteration: 163800 loss: 0.0035 lr: 0.02\n",
            "iteration: 163900 loss: 0.0031 lr: 0.02\n",
            "iteration: 164000 loss: 0.0038 lr: 0.02\n",
            "iteration: 164100 loss: 0.0038 lr: 0.02\n",
            "iteration: 164200 loss: 0.0033 lr: 0.02\n",
            "iteration: 164300 loss: 0.0036 lr: 0.02\n",
            "iteration: 164400 loss: 0.0033 lr: 0.02\n",
            "iteration: 164500 loss: 0.0045 lr: 0.02\n",
            "iteration: 164600 loss: 0.0033 lr: 0.02\n",
            "iteration: 164700 loss: 0.0029 lr: 0.02\n",
            "iteration: 164800 loss: 0.0039 lr: 0.02\n",
            "iteration: 164900 loss: 0.0033 lr: 0.02\n",
            "iteration: 165000 loss: 0.0040 lr: 0.02\n",
            "iteration: 165100 loss: 0.0040 lr: 0.02\n",
            "iteration: 165200 loss: 0.0030 lr: 0.02\n",
            "iteration: 165300 loss: 0.0037 lr: 0.02\n",
            "iteration: 165400 loss: 0.0036 lr: 0.02\n",
            "iteration: 165500 loss: 0.0037 lr: 0.02\n",
            "iteration: 165600 loss: 0.0042 lr: 0.02\n",
            "iteration: 165700 loss: 0.0037 lr: 0.02\n",
            "iteration: 165800 loss: 0.0035 lr: 0.02\n",
            "iteration: 165900 loss: 0.0038 lr: 0.02\n",
            "iteration: 166000 loss: 0.0042 lr: 0.02\n",
            "iteration: 166100 loss: 0.0038 lr: 0.02\n",
            "iteration: 166200 loss: 0.0041 lr: 0.02\n",
            "iteration: 166300 loss: 0.0038 lr: 0.02\n",
            "iteration: 166400 loss: 0.0036 lr: 0.02\n",
            "iteration: 166500 loss: 0.0038 lr: 0.02\n",
            "iteration: 166600 loss: 0.0034 lr: 0.02\n",
            "iteration: 166700 loss: 0.0038 lr: 0.02\n",
            "iteration: 166800 loss: 0.0036 lr: 0.02\n",
            "iteration: 166900 loss: 0.0036 lr: 0.02\n",
            "iteration: 167000 loss: 0.0039 lr: 0.02\n",
            "iteration: 167100 loss: 0.0038 lr: 0.02\n",
            "iteration: 167200 loss: 0.0039 lr: 0.02\n",
            "iteration: 167300 loss: 0.0033 lr: 0.02\n",
            "iteration: 167400 loss: 0.0040 lr: 0.02\n",
            "iteration: 167500 loss: 0.0043 lr: 0.02\n",
            "iteration: 167600 loss: 0.0033 lr: 0.02\n",
            "iteration: 167700 loss: 0.0033 lr: 0.02\n",
            "iteration: 167800 loss: 0.0039 lr: 0.02\n",
            "iteration: 167900 loss: 0.0038 lr: 0.02\n",
            "iteration: 168000 loss: 0.0031 lr: 0.02\n",
            "iteration: 168100 loss: 0.0041 lr: 0.02\n",
            "iteration: 168200 loss: 0.0038 lr: 0.02\n",
            "iteration: 168300 loss: 0.0037 lr: 0.02\n",
            "iteration: 168400 loss: 0.0037 lr: 0.02\n",
            "iteration: 168500 loss: 0.0040 lr: 0.02\n",
            "iteration: 168600 loss: 0.0036 lr: 0.02\n",
            "iteration: 168700 loss: 0.0035 lr: 0.02\n",
            "iteration: 168800 loss: 0.0036 lr: 0.02\n",
            "iteration: 168900 loss: 0.0040 lr: 0.02\n",
            "iteration: 169000 loss: 0.0040 lr: 0.02\n",
            "iteration: 169100 loss: 0.0037 lr: 0.02\n",
            "iteration: 169200 loss: 0.0037 lr: 0.02\n",
            "iteration: 169300 loss: 0.0036 lr: 0.02\n",
            "iteration: 169400 loss: 0.0035 lr: 0.02\n",
            "iteration: 169500 loss: 0.0034 lr: 0.02\n",
            "iteration: 169600 loss: 0.0037 lr: 0.02\n",
            "iteration: 169700 loss: 0.0036 lr: 0.02\n",
            "iteration: 169800 loss: 0.0037 lr: 0.02\n",
            "iteration: 169900 loss: 0.0033 lr: 0.02\n",
            "iteration: 170000 loss: 0.0031 lr: 0.02\n",
            "iteration: 170100 loss: 0.0033 lr: 0.02\n",
            "iteration: 170200 loss: 0.0033 lr: 0.02\n",
            "iteration: 170300 loss: 0.0028 lr: 0.02\n",
            "iteration: 170400 loss: 0.0039 lr: 0.02\n",
            "iteration: 170500 loss: 0.0034 lr: 0.02\n",
            "iteration: 170600 loss: 0.0038 lr: 0.02\n",
            "iteration: 170700 loss: 0.0041 lr: 0.02\n",
            "iteration: 170800 loss: 0.0042 lr: 0.02\n",
            "iteration: 170900 loss: 0.0036 lr: 0.02\n",
            "iteration: 171000 loss: 0.0037 lr: 0.02\n",
            "iteration: 171100 loss: 0.0032 lr: 0.02\n",
            "iteration: 171200 loss: 0.0036 lr: 0.02\n",
            "iteration: 171300 loss: 0.0034 lr: 0.02\n",
            "iteration: 171400 loss: 0.0037 lr: 0.02\n",
            "iteration: 171500 loss: 0.0032 lr: 0.02\n",
            "iteration: 171600 loss: 0.0032 lr: 0.02\n",
            "iteration: 171700 loss: 0.0041 lr: 0.02\n",
            "iteration: 171800 loss: 0.0038 lr: 0.02\n",
            "iteration: 171900 loss: 0.0035 lr: 0.02\n",
            "iteration: 172000 loss: 0.0033 lr: 0.02\n",
            "iteration: 172100 loss: 0.0040 lr: 0.02\n",
            "iteration: 172200 loss: 0.0038 lr: 0.02\n",
            "iteration: 172300 loss: 0.0035 lr: 0.02\n",
            "iteration: 172400 loss: 0.0040 lr: 0.02\n",
            "iteration: 172500 loss: 0.0035 lr: 0.02\n",
            "iteration: 172600 loss: 0.0037 lr: 0.02\n",
            "iteration: 172700 loss: 0.0036 lr: 0.02\n",
            "iteration: 172800 loss: 0.0035 lr: 0.02\n",
            "iteration: 172900 loss: 0.0034 lr: 0.02\n",
            "iteration: 173000 loss: 0.0038 lr: 0.02\n",
            "iteration: 173100 loss: 0.0038 lr: 0.02\n",
            "iteration: 173200 loss: 0.0040 lr: 0.02\n",
            "iteration: 173300 loss: 0.0034 lr: 0.02\n",
            "iteration: 173400 loss: 0.0037 lr: 0.02\n",
            "iteration: 173500 loss: 0.0041 lr: 0.02\n",
            "iteration: 173600 loss: 0.0033 lr: 0.02\n",
            "iteration: 173700 loss: 0.0039 lr: 0.02\n",
            "iteration: 173800 loss: 0.0039 lr: 0.02\n",
            "iteration: 173900 loss: 0.0040 lr: 0.02\n",
            "iteration: 174000 loss: 0.0041 lr: 0.02\n",
            "iteration: 174100 loss: 0.0038 lr: 0.02\n",
            "iteration: 174200 loss: 0.0033 lr: 0.02\n",
            "iteration: 174300 loss: 0.0040 lr: 0.02\n",
            "iteration: 174400 loss: 0.0037 lr: 0.02\n",
            "iteration: 174500 loss: 0.0037 lr: 0.02\n",
            "iteration: 174600 loss: 0.0037 lr: 0.02\n",
            "iteration: 174700 loss: 0.0029 lr: 0.02\n",
            "iteration: 174800 loss: 0.0038 lr: 0.02\n",
            "iteration: 174900 loss: 0.0040 lr: 0.02\n",
            "iteration: 175000 loss: 0.0038 lr: 0.02\n",
            "iteration: 175100 loss: 0.0035 lr: 0.02\n",
            "iteration: 175200 loss: 0.0038 lr: 0.02\n",
            "iteration: 175300 loss: 0.0038 lr: 0.02\n",
            "iteration: 175400 loss: 0.0040 lr: 0.02\n",
            "iteration: 175500 loss: 0.0037 lr: 0.02\n",
            "iteration: 175600 loss: 0.0038 lr: 0.02\n",
            "iteration: 175700 loss: 0.0032 lr: 0.02\n",
            "iteration: 175800 loss: 0.0035 lr: 0.02\n",
            "iteration: 175900 loss: 0.0040 lr: 0.02\n",
            "iteration: 176000 loss: 0.0033 lr: 0.02\n",
            "iteration: 176100 loss: 0.0033 lr: 0.02\n",
            "iteration: 176200 loss: 0.0033 lr: 0.02\n",
            "iteration: 176300 loss: 0.0034 lr: 0.02\n",
            "iteration: 176400 loss: 0.0037 lr: 0.02\n",
            "iteration: 176500 loss: 0.0034 lr: 0.02\n",
            "iteration: 176600 loss: 0.0032 lr: 0.02\n",
            "iteration: 176700 loss: 0.0034 lr: 0.02\n",
            "iteration: 176800 loss: 0.0034 lr: 0.02\n",
            "iteration: 176900 loss: 0.0035 lr: 0.02\n",
            "iteration: 177000 loss: 0.0031 lr: 0.02\n",
            "iteration: 177100 loss: 0.0038 lr: 0.02\n",
            "iteration: 177200 loss: 0.0034 lr: 0.02\n",
            "iteration: 177300 loss: 0.0037 lr: 0.02\n",
            "iteration: 177400 loss: 0.0037 lr: 0.02\n",
            "iteration: 177500 loss: 0.0030 lr: 0.02\n",
            "iteration: 177600 loss: 0.0035 lr: 0.02\n",
            "iteration: 177700 loss: 0.0034 lr: 0.02\n",
            "iteration: 177800 loss: 0.0039 lr: 0.02\n",
            "iteration: 177900 loss: 0.0037 lr: 0.02\n",
            "iteration: 178000 loss: 0.0044 lr: 0.02\n",
            "iteration: 178100 loss: 0.0039 lr: 0.02\n",
            "iteration: 178200 loss: 0.0036 lr: 0.02\n",
            "iteration: 178300 loss: 0.0036 lr: 0.02\n",
            "iteration: 178400 loss: 0.0041 lr: 0.02\n",
            "iteration: 178500 loss: 0.0033 lr: 0.02\n",
            "iteration: 178600 loss: 0.0034 lr: 0.02\n",
            "iteration: 178700 loss: 0.0033 lr: 0.02\n",
            "iteration: 178800 loss: 0.0039 lr: 0.02\n",
            "iteration: 178900 loss: 0.0041 lr: 0.02\n",
            "iteration: 179000 loss: 0.0035 lr: 0.02\n",
            "iteration: 179100 loss: 0.0030 lr: 0.02\n",
            "iteration: 179200 loss: 0.0030 lr: 0.02\n",
            "iteration: 179300 loss: 0.0035 lr: 0.02\n",
            "iteration: 179400 loss: 0.0039 lr: 0.02\n",
            "iteration: 179500 loss: 0.0036 lr: 0.02\n",
            "iteration: 179600 loss: 0.0033 lr: 0.02\n",
            "iteration: 179700 loss: 0.0035 lr: 0.02\n",
            "iteration: 179800 loss: 0.0040 lr: 0.02\n",
            "iteration: 179900 loss: 0.0035 lr: 0.02\n",
            "iteration: 180000 loss: 0.0034 lr: 0.02\n",
            "iteration: 180100 loss: 0.0035 lr: 0.02\n",
            "iteration: 180200 loss: 0.0032 lr: 0.02\n",
            "iteration: 180300 loss: 0.0033 lr: 0.02\n",
            "iteration: 180400 loss: 0.0036 lr: 0.02\n",
            "iteration: 180500 loss: 0.0031 lr: 0.02\n",
            "iteration: 180600 loss: 0.0038 lr: 0.02\n",
            "iteration: 180700 loss: 0.0036 lr: 0.02\n",
            "iteration: 180800 loss: 0.0039 lr: 0.02\n",
            "iteration: 180900 loss: 0.0037 lr: 0.02\n",
            "iteration: 181000 loss: 0.0034 lr: 0.02\n",
            "iteration: 181100 loss: 0.0034 lr: 0.02\n",
            "iteration: 181200 loss: 0.0040 lr: 0.02\n",
            "iteration: 181300 loss: 0.0042 lr: 0.02\n",
            "iteration: 181400 loss: 0.0036 lr: 0.02\n",
            "iteration: 181500 loss: 0.0035 lr: 0.02\n",
            "iteration: 181600 loss: 0.0033 lr: 0.02\n",
            "iteration: 181700 loss: 0.0033 lr: 0.02\n",
            "iteration: 181800 loss: 0.0037 lr: 0.02\n",
            "iteration: 181900 loss: 0.0034 lr: 0.02\n",
            "iteration: 182000 loss: 0.0032 lr: 0.02\n",
            "iteration: 182100 loss: 0.0040 lr: 0.02\n",
            "iteration: 182200 loss: 0.0039 lr: 0.02\n",
            "iteration: 182300 loss: 0.0036 lr: 0.02\n",
            "iteration: 182400 loss: 0.0039 lr: 0.02\n",
            "iteration: 182500 loss: 0.0037 lr: 0.02\n",
            "iteration: 182600 loss: 0.0037 lr: 0.02\n",
            "iteration: 182700 loss: 0.0038 lr: 0.02\n",
            "iteration: 182800 loss: 0.0031 lr: 0.02\n",
            "iteration: 182900 loss: 0.0037 lr: 0.02\n",
            "iteration: 183000 loss: 0.0037 lr: 0.02\n",
            "iteration: 183100 loss: 0.0031 lr: 0.02\n",
            "iteration: 183200 loss: 0.0035 lr: 0.02\n",
            "iteration: 183300 loss: 0.0034 lr: 0.02\n",
            "iteration: 183400 loss: 0.0034 lr: 0.02\n",
            "iteration: 183500 loss: 0.0034 lr: 0.02\n",
            "iteration: 183600 loss: 0.0035 lr: 0.02\n",
            "iteration: 183700 loss: 0.0032 lr: 0.02\n",
            "iteration: 183800 loss: 0.0036 lr: 0.02\n",
            "iteration: 183900 loss: 0.0033 lr: 0.02\n",
            "iteration: 184000 loss: 0.0029 lr: 0.02\n",
            "iteration: 184100 loss: 0.0041 lr: 0.02\n",
            "iteration: 184200 loss: 0.0032 lr: 0.02\n",
            "iteration: 184300 loss: 0.0037 lr: 0.02\n",
            "iteration: 184400 loss: 0.0035 lr: 0.02\n",
            "iteration: 184500 loss: 0.0032 lr: 0.02\n",
            "iteration: 184600 loss: 0.0039 lr: 0.02\n",
            "iteration: 184700 loss: 0.0036 lr: 0.02\n",
            "iteration: 184800 loss: 0.0034 lr: 0.02\n",
            "iteration: 184900 loss: 0.0034 lr: 0.02\n",
            "iteration: 185000 loss: 0.0037 lr: 0.02\n",
            "iteration: 185100 loss: 0.0034 lr: 0.02\n",
            "iteration: 185200 loss: 0.0034 lr: 0.02\n",
            "iteration: 185300 loss: 0.0030 lr: 0.02\n",
            "iteration: 185400 loss: 0.0038 lr: 0.02\n",
            "iteration: 185500 loss: 0.0037 lr: 0.02\n",
            "iteration: 185600 loss: 0.0032 lr: 0.02\n",
            "iteration: 185700 loss: 0.0040 lr: 0.02\n",
            "iteration: 185800 loss: 0.0039 lr: 0.02\n",
            "iteration: 185900 loss: 0.0031 lr: 0.02\n",
            "iteration: 186000 loss: 0.0030 lr: 0.02\n",
            "iteration: 186100 loss: 0.0031 lr: 0.02\n",
            "iteration: 186200 loss: 0.0039 lr: 0.02\n",
            "iteration: 186300 loss: 0.0031 lr: 0.02\n",
            "iteration: 186400 loss: 0.0034 lr: 0.02\n",
            "iteration: 186500 loss: 0.0036 lr: 0.02\n",
            "iteration: 186600 loss: 0.0031 lr: 0.02\n",
            "iteration: 186700 loss: 0.0031 lr: 0.02\n",
            "iteration: 186800 loss: 0.0032 lr: 0.02\n",
            "iteration: 186900 loss: 0.0033 lr: 0.02\n",
            "iteration: 187000 loss: 0.0034 lr: 0.02\n",
            "iteration: 187100 loss: 0.0040 lr: 0.02\n",
            "iteration: 187200 loss: 0.0036 lr: 0.02\n",
            "iteration: 187300 loss: 0.0045 lr: 0.02\n",
            "iteration: 187400 loss: 0.0030 lr: 0.02\n",
            "iteration: 187500 loss: 0.0036 lr: 0.02\n",
            "iteration: 187600 loss: 0.0034 lr: 0.02\n",
            "iteration: 187700 loss: 0.0034 lr: 0.02\n",
            "iteration: 187800 loss: 0.0033 lr: 0.02\n",
            "iteration: 187900 loss: 0.0031 lr: 0.02\n",
            "iteration: 188000 loss: 0.0035 lr: 0.02\n",
            "iteration: 188100 loss: 0.0038 lr: 0.02\n",
            "iteration: 188200 loss: 0.0041 lr: 0.02\n",
            "iteration: 188300 loss: 0.0038 lr: 0.02\n",
            "iteration: 188400 loss: 0.0040 lr: 0.02\n",
            "iteration: 188500 loss: 0.0039 lr: 0.02\n",
            "iteration: 188600 loss: 0.0036 lr: 0.02\n",
            "iteration: 188700 loss: 0.0029 lr: 0.02\n",
            "iteration: 188800 loss: 0.0036 lr: 0.02\n",
            "iteration: 188900 loss: 0.0041 lr: 0.02\n",
            "iteration: 189000 loss: 0.0031 lr: 0.02\n",
            "iteration: 189100 loss: 0.0030 lr: 0.02\n",
            "iteration: 189200 loss: 0.0030 lr: 0.02\n",
            "iteration: 189300 loss: 0.0038 lr: 0.02\n",
            "iteration: 189400 loss: 0.0029 lr: 0.02\n",
            "iteration: 189500 loss: 0.0037 lr: 0.02\n",
            "iteration: 189600 loss: 0.0036 lr: 0.02\n",
            "iteration: 189700 loss: 0.0038 lr: 0.02\n",
            "iteration: 189800 loss: 0.0036 lr: 0.02\n",
            "iteration: 189900 loss: 0.0037 lr: 0.02\n",
            "iteration: 190000 loss: 0.0040 lr: 0.02\n",
            "iteration: 190100 loss: 0.0031 lr: 0.02\n",
            "iteration: 190200 loss: 0.0034 lr: 0.02\n",
            "iteration: 190300 loss: 0.0037 lr: 0.02\n",
            "iteration: 190400 loss: 0.0038 lr: 0.02\n",
            "iteration: 190500 loss: 0.0031 lr: 0.02\n",
            "iteration: 190600 loss: 0.0035 lr: 0.02\n",
            "iteration: 190700 loss: 0.0042 lr: 0.02\n",
            "iteration: 190800 loss: 0.0038 lr: 0.02\n",
            "iteration: 190900 loss: 0.0039 lr: 0.02\n",
            "iteration: 191000 loss: 0.0045 lr: 0.02\n",
            "iteration: 191100 loss: 0.0035 lr: 0.02\n",
            "iteration: 191200 loss: 0.0034 lr: 0.02\n",
            "iteration: 191300 loss: 0.0036 lr: 0.02\n",
            "iteration: 191400 loss: 0.0028 lr: 0.02\n",
            "iteration: 191500 loss: 0.0035 lr: 0.02\n",
            "iteration: 191600 loss: 0.0041 lr: 0.02\n",
            "iteration: 191700 loss: 0.0033 lr: 0.02\n",
            "iteration: 191800 loss: 0.0033 lr: 0.02\n",
            "iteration: 191900 loss: 0.0036 lr: 0.02\n",
            "iteration: 192000 loss: 0.0040 lr: 0.02\n",
            "iteration: 192100 loss: 0.0033 lr: 0.02\n",
            "iteration: 192200 loss: 0.0036 lr: 0.02\n",
            "iteration: 192300 loss: 0.0035 lr: 0.02\n",
            "iteration: 192400 loss: 0.0033 lr: 0.02\n",
            "iteration: 192500 loss: 0.0033 lr: 0.02\n",
            "iteration: 192600 loss: 0.0035 lr: 0.02\n",
            "iteration: 192700 loss: 0.0035 lr: 0.02\n",
            "iteration: 192800 loss: 0.0036 lr: 0.02\n",
            "iteration: 192900 loss: 0.0038 lr: 0.02\n",
            "iteration: 193000 loss: 0.0036 lr: 0.02\n",
            "iteration: 193100 loss: 0.0038 lr: 0.02\n",
            "iteration: 193200 loss: 0.0032 lr: 0.02\n",
            "iteration: 193300 loss: 0.0031 lr: 0.02\n",
            "iteration: 193400 loss: 0.0032 lr: 0.02\n",
            "iteration: 193500 loss: 0.0034 lr: 0.02\n",
            "iteration: 193600 loss: 0.0032 lr: 0.02\n",
            "iteration: 193700 loss: 0.0033 lr: 0.02\n",
            "iteration: 193800 loss: 0.0029 lr: 0.02\n",
            "iteration: 193900 loss: 0.0036 lr: 0.02\n",
            "iteration: 194000 loss: 0.0029 lr: 0.02\n",
            "iteration: 194100 loss: 0.0040 lr: 0.02\n",
            "iteration: 194200 loss: 0.0037 lr: 0.02\n",
            "iteration: 194300 loss: 0.0029 lr: 0.02\n",
            "iteration: 194400 loss: 0.0034 lr: 0.02\n",
            "iteration: 194500 loss: 0.0029 lr: 0.02\n",
            "iteration: 194600 loss: 0.0033 lr: 0.02\n",
            "iteration: 194700 loss: 0.0035 lr: 0.02\n",
            "iteration: 194800 loss: 0.0031 lr: 0.02\n",
            "iteration: 194900 loss: 0.0038 lr: 0.02\n",
            "iteration: 195000 loss: 0.0031 lr: 0.02\n",
            "iteration: 195100 loss: 0.0032 lr: 0.02\n",
            "iteration: 195200 loss: 0.0034 lr: 0.02\n",
            "iteration: 195300 loss: 0.0038 lr: 0.02\n",
            "iteration: 195400 loss: 0.0032 lr: 0.02\n",
            "iteration: 195500 loss: 0.0036 lr: 0.02\n",
            "iteration: 195600 loss: 0.0033 lr: 0.02\n",
            "iteration: 195700 loss: 0.0044 lr: 0.02\n",
            "iteration: 195800 loss: 0.0033 lr: 0.02\n",
            "iteration: 195900 loss: 0.0032 lr: 0.02\n",
            "iteration: 196000 loss: 0.0034 lr: 0.02\n",
            "iteration: 196100 loss: 0.0035 lr: 0.02\n",
            "iteration: 196200 loss: 0.0030 lr: 0.02\n",
            "iteration: 196300 loss: 0.0032 lr: 0.02\n",
            "iteration: 196400 loss: 0.0033 lr: 0.02\n",
            "iteration: 196500 loss: 0.0031 lr: 0.02\n",
            "iteration: 196600 loss: 0.0036 lr: 0.02\n",
            "iteration: 196700 loss: 0.0030 lr: 0.02\n",
            "iteration: 196800 loss: 0.0037 lr: 0.02\n",
            "iteration: 196900 loss: 0.0038 lr: 0.02\n",
            "iteration: 197000 loss: 0.0031 lr: 0.02\n",
            "iteration: 197100 loss: 0.0035 lr: 0.02\n",
            "iteration: 197200 loss: 0.0036 lr: 0.02\n",
            "iteration: 197300 loss: 0.0033 lr: 0.02\n",
            "iteration: 197400 loss: 0.0036 lr: 0.02\n",
            "iteration: 197500 loss: 0.0034 lr: 0.02\n",
            "iteration: 197600 loss: 0.0030 lr: 0.02\n",
            "iteration: 197700 loss: 0.0033 lr: 0.02\n",
            "iteration: 197800 loss: 0.0035 lr: 0.02\n",
            "iteration: 197900 loss: 0.0031 lr: 0.02\n",
            "iteration: 198000 loss: 0.0033 lr: 0.02\n",
            "iteration: 198100 loss: 0.0034 lr: 0.02\n",
            "iteration: 198200 loss: 0.0033 lr: 0.02\n",
            "iteration: 198300 loss: 0.0034 lr: 0.02\n",
            "iteration: 198400 loss: 0.0036 lr: 0.02\n",
            "iteration: 198500 loss: 0.0037 lr: 0.02\n",
            "iteration: 198600 loss: 0.0038 lr: 0.02\n",
            "iteration: 198700 loss: 0.0035 lr: 0.02\n",
            "iteration: 198800 loss: 0.0038 lr: 0.02\n",
            "iteration: 198900 loss: 0.0031 lr: 0.02\n",
            "iteration: 199000 loss: 0.0032 lr: 0.02\n",
            "iteration: 199100 loss: 0.0031 lr: 0.02\n",
            "iteration: 199200 loss: 0.0034 lr: 0.02\n",
            "iteration: 199300 loss: 0.0037 lr: 0.02\n",
            "iteration: 199400 loss: 0.0038 lr: 0.02\n",
            "iteration: 199500 loss: 0.0032 lr: 0.02\n",
            "iteration: 199600 loss: 0.0041 lr: 0.02\n",
            "iteration: 199700 loss: 0.0035 lr: 0.02\n",
            "iteration: 199800 loss: 0.0038 lr: 0.02\n",
            "iteration: 199900 loss: 0.0040 lr: 0.02\n",
            "iteration: 200000 loss: 0.0032 lr: 0.02\n",
            "iteration: 200100 loss: 0.0035 lr: 0.02\n",
            "iteration: 200200 loss: 0.0033 lr: 0.02\n",
            "iteration: 200300 loss: 0.0035 lr: 0.02\n",
            "iteration: 200400 loss: 0.0032 lr: 0.02\n",
            "iteration: 200500 loss: 0.0035 lr: 0.02\n",
            "iteration: 200600 loss: 0.0033 lr: 0.02\n",
            "iteration: 200700 loss: 0.0033 lr: 0.02\n",
            "iteration: 200800 loss: 0.0032 lr: 0.02\n",
            "iteration: 200900 loss: 0.0033 lr: 0.02\n",
            "iteration: 201000 loss: 0.0029 lr: 0.02\n",
            "iteration: 201100 loss: 0.0034 lr: 0.02\n",
            "iteration: 201200 loss: 0.0030 lr: 0.02\n",
            "iteration: 201300 loss: 0.0035 lr: 0.02\n",
            "iteration: 201400 loss: 0.0029 lr: 0.02\n",
            "iteration: 201500 loss: 0.0034 lr: 0.02\n",
            "iteration: 201600 loss: 0.0035 lr: 0.02\n",
            "iteration: 201700 loss: 0.0032 lr: 0.02\n",
            "iteration: 201800 loss: 0.0038 lr: 0.02\n",
            "iteration: 201900 loss: 0.0033 lr: 0.02\n",
            "iteration: 202000 loss: 0.0037 lr: 0.02\n",
            "iteration: 202100 loss: 0.0034 lr: 0.02\n",
            "iteration: 202200 loss: 0.0032 lr: 0.02\n",
            "iteration: 202300 loss: 0.0036 lr: 0.02\n",
            "iteration: 202400 loss: 0.0028 lr: 0.02\n",
            "iteration: 202500 loss: 0.0034 lr: 0.02\n",
            "iteration: 202600 loss: 0.0038 lr: 0.02\n",
            "iteration: 202700 loss: 0.0033 lr: 0.02\n",
            "iteration: 202800 loss: 0.0031 lr: 0.02\n",
            "iteration: 202900 loss: 0.0032 lr: 0.02\n",
            "iteration: 203000 loss: 0.0039 lr: 0.02\n",
            "iteration: 203100 loss: 0.0036 lr: 0.02\n",
            "iteration: 203200 loss: 0.0035 lr: 0.02\n",
            "iteration: 203300 loss: 0.0041 lr: 0.02\n",
            "iteration: 203400 loss: 0.0034 lr: 0.02\n",
            "iteration: 203500 loss: 0.0035 lr: 0.02\n",
            "iteration: 203600 loss: 0.0032 lr: 0.02\n",
            "iteration: 203700 loss: 0.0034 lr: 0.02\n",
            "iteration: 203800 loss: 0.0035 lr: 0.02\n",
            "iteration: 203900 loss: 0.0036 lr: 0.02\n",
            "iteration: 204000 loss: 0.0031 lr: 0.02\n",
            "iteration: 204100 loss: 0.0038 lr: 0.02\n",
            "iteration: 204200 loss: 0.0033 lr: 0.02\n",
            "iteration: 204300 loss: 0.0036 lr: 0.02\n",
            "iteration: 204400 loss: 0.0045 lr: 0.02\n",
            "iteration: 204500 loss: 0.0036 lr: 0.02\n",
            "iteration: 204600 loss: 0.0034 lr: 0.02\n",
            "iteration: 204700 loss: 0.0034 lr: 0.02\n",
            "iteration: 204800 loss: 0.0036 lr: 0.02\n",
            "iteration: 204900 loss: 0.0036 lr: 0.02\n",
            "iteration: 205000 loss: 0.0036 lr: 0.02\n",
            "iteration: 205100 loss: 0.0037 lr: 0.02\n",
            "iteration: 205200 loss: 0.0034 lr: 0.02\n",
            "iteration: 205300 loss: 0.0035 lr: 0.02\n",
            "iteration: 205400 loss: 0.0044 lr: 0.02\n",
            "iteration: 205500 loss: 0.0047 lr: 0.02\n",
            "iteration: 205600 loss: 0.0031 lr: 0.02\n",
            "iteration: 205700 loss: 0.0035 lr: 0.02\n",
            "iteration: 205800 loss: 0.0029 lr: 0.02\n",
            "iteration: 205900 loss: 0.0036 lr: 0.02\n",
            "iteration: 206000 loss: 0.0033 lr: 0.02\n",
            "iteration: 206100 loss: 0.0035 lr: 0.02\n",
            "iteration: 206200 loss: 0.0033 lr: 0.02\n",
            "iteration: 206300 loss: 0.0036 lr: 0.02\n",
            "iteration: 206400 loss: 0.0034 lr: 0.02\n",
            "iteration: 206500 loss: 0.0031 lr: 0.02\n",
            "iteration: 206600 loss: 0.0029 lr: 0.02\n",
            "iteration: 206700 loss: 0.0038 lr: 0.02\n",
            "iteration: 206800 loss: 0.0029 lr: 0.02\n",
            "iteration: 206900 loss: 0.0033 lr: 0.02\n",
            "iteration: 207000 loss: 0.0030 lr: 0.02\n",
            "iteration: 207100 loss: 0.0033 lr: 0.02\n",
            "iteration: 207200 loss: 0.0040 lr: 0.02\n",
            "iteration: 207300 loss: 0.0034 lr: 0.02\n",
            "iteration: 207400 loss: 0.0030 lr: 0.02\n",
            "iteration: 207500 loss: 0.0031 lr: 0.02\n",
            "iteration: 207600 loss: 0.0032 lr: 0.02\n",
            "iteration: 207700 loss: 0.0039 lr: 0.02\n",
            "iteration: 207800 loss: 0.0030 lr: 0.02\n",
            "iteration: 207900 loss: 0.0032 lr: 0.02\n",
            "iteration: 208000 loss: 0.0029 lr: 0.02\n",
            "iteration: 208100 loss: 0.0032 lr: 0.02\n",
            "iteration: 208200 loss: 0.0034 lr: 0.02\n",
            "iteration: 208300 loss: 0.0033 lr: 0.02\n",
            "iteration: 208400 loss: 0.0034 lr: 0.02\n",
            "iteration: 208500 loss: 0.0035 lr: 0.02\n",
            "iteration: 208600 loss: 0.0033 lr: 0.02\n",
            "iteration: 208700 loss: 0.0031 lr: 0.02\n",
            "iteration: 208800 loss: 0.0037 lr: 0.02\n",
            "iteration: 208900 loss: 0.0033 lr: 0.02\n",
            "iteration: 209000 loss: 0.0031 lr: 0.02\n",
            "iteration: 209100 loss: 0.0033 lr: 0.02\n",
            "iteration: 209200 loss: 0.0039 lr: 0.02\n",
            "iteration: 209300 loss: 0.0035 lr: 0.02\n",
            "iteration: 209400 loss: 0.0030 lr: 0.02\n",
            "iteration: 209500 loss: 0.0034 lr: 0.02\n",
            "iteration: 209600 loss: 0.0032 lr: 0.02\n",
            "iteration: 209700 loss: 0.0030 lr: 0.02\n",
            "iteration: 209800 loss: 0.0035 lr: 0.02\n",
            "iteration: 209900 loss: 0.0036 lr: 0.02\n",
            "iteration: 210000 loss: 0.0037 lr: 0.02\n",
            "iteration: 210100 loss: 0.0032 lr: 0.02\n",
            "iteration: 210200 loss: 0.0029 lr: 0.02\n",
            "iteration: 210300 loss: 0.0031 lr: 0.02\n",
            "iteration: 210400 loss: 0.0030 lr: 0.02\n",
            "iteration: 210500 loss: 0.0029 lr: 0.02\n",
            "iteration: 210600 loss: 0.0029 lr: 0.02\n",
            "iteration: 210700 loss: 0.0029 lr: 0.02\n",
            "iteration: 210800 loss: 0.0042 lr: 0.02\n",
            "iteration: 210900 loss: 0.0034 lr: 0.02\n",
            "iteration: 211000 loss: 0.0037 lr: 0.02\n",
            "iteration: 211100 loss: 0.0035 lr: 0.02\n",
            "iteration: 211200 loss: 0.0034 lr: 0.02\n",
            "iteration: 211300 loss: 0.0032 lr: 0.02\n",
            "iteration: 211400 loss: 0.0031 lr: 0.02\n",
            "iteration: 211500 loss: 0.0035 lr: 0.02\n",
            "iteration: 211600 loss: 0.0030 lr: 0.02\n",
            "iteration: 211700 loss: 0.0036 lr: 0.02\n",
            "iteration: 211800 loss: 0.0035 lr: 0.02\n",
            "iteration: 211900 loss: 0.0029 lr: 0.02\n",
            "iteration: 212000 loss: 0.0032 lr: 0.02\n",
            "iteration: 212100 loss: 0.0033 lr: 0.02\n",
            "iteration: 212200 loss: 0.0030 lr: 0.02\n",
            "iteration: 212300 loss: 0.0030 lr: 0.02\n",
            "iteration: 212400 loss: 0.0038 lr: 0.02\n",
            "iteration: 212500 loss: 0.0035 lr: 0.02\n",
            "iteration: 212600 loss: 0.0032 lr: 0.02\n",
            "iteration: 212700 loss: 0.0028 lr: 0.02\n",
            "iteration: 212800 loss: 0.0033 lr: 0.02\n",
            "iteration: 212900 loss: 0.0032 lr: 0.02\n",
            "iteration: 213000 loss: 0.0033 lr: 0.02\n",
            "iteration: 213100 loss: 0.0039 lr: 0.02\n",
            "iteration: 213200 loss: 0.0033 lr: 0.02\n",
            "iteration: 213300 loss: 0.0035 lr: 0.02\n",
            "iteration: 213400 loss: 0.0040 lr: 0.02\n",
            "iteration: 213500 loss: 0.0039 lr: 0.02\n",
            "iteration: 213600 loss: 0.0036 lr: 0.02\n",
            "iteration: 213700 loss: 0.0030 lr: 0.02\n",
            "iteration: 213800 loss: 0.0030 lr: 0.02\n",
            "iteration: 213900 loss: 0.0027 lr: 0.02\n",
            "iteration: 214000 loss: 0.0033 lr: 0.02\n",
            "iteration: 214100 loss: 0.0030 lr: 0.02\n",
            "iteration: 214200 loss: 0.0029 lr: 0.02\n",
            "iteration: 214300 loss: 0.0030 lr: 0.02\n",
            "iteration: 214400 loss: 0.0034 lr: 0.02\n",
            "iteration: 214500 loss: 0.0029 lr: 0.02\n",
            "iteration: 214600 loss: 0.0029 lr: 0.02\n",
            "iteration: 214700 loss: 0.0033 lr: 0.02\n",
            "iteration: 214800 loss: 0.0033 lr: 0.02\n",
            "iteration: 214900 loss: 0.0031 lr: 0.02\n",
            "iteration: 215000 loss: 0.0032 lr: 0.02\n",
            "iteration: 215100 loss: 0.0033 lr: 0.02\n",
            "iteration: 215200 loss: 0.0040 lr: 0.02\n",
            "iteration: 215300 loss: 0.0031 lr: 0.02\n",
            "iteration: 215400 loss: 0.0034 lr: 0.02\n",
            "iteration: 215500 loss: 0.0033 lr: 0.02\n",
            "iteration: 215600 loss: 0.0035 lr: 0.02\n",
            "iteration: 215700 loss: 0.0037 lr: 0.02\n",
            "iteration: 215800 loss: 0.0035 lr: 0.02\n",
            "iteration: 215900 loss: 0.0036 lr: 0.02\n",
            "iteration: 216000 loss: 0.0034 lr: 0.02\n",
            "iteration: 216100 loss: 0.0031 lr: 0.02\n",
            "iteration: 216200 loss: 0.0033 lr: 0.02\n",
            "iteration: 216300 loss: 0.0028 lr: 0.02\n",
            "iteration: 216400 loss: 0.0035 lr: 0.02\n",
            "iteration: 216500 loss: 0.0031 lr: 0.02\n",
            "iteration: 216600 loss: 0.0037 lr: 0.02\n",
            "iteration: 216700 loss: 0.0033 lr: 0.02\n",
            "iteration: 216800 loss: 0.0031 lr: 0.02\n",
            "iteration: 216900 loss: 0.0035 lr: 0.02\n",
            "iteration: 217000 loss: 0.0029 lr: 0.02\n",
            "iteration: 217100 loss: 0.0035 lr: 0.02\n",
            "iteration: 217200 loss: 0.0036 lr: 0.02\n",
            "iteration: 217300 loss: 0.0034 lr: 0.02\n",
            "iteration: 217400 loss: 0.0032 lr: 0.02\n",
            "iteration: 217500 loss: 0.0032 lr: 0.02\n",
            "iteration: 217600 loss: 0.0034 lr: 0.02\n",
            "iteration: 217700 loss: 0.0033 lr: 0.02\n",
            "iteration: 217800 loss: 0.0032 lr: 0.02\n",
            "iteration: 217900 loss: 0.0032 lr: 0.02\n",
            "iteration: 218000 loss: 0.0031 lr: 0.02\n",
            "iteration: 218100 loss: 0.0037 lr: 0.02\n",
            "iteration: 218200 loss: 0.0030 lr: 0.02\n",
            "iteration: 218300 loss: 0.0027 lr: 0.02\n",
            "iteration: 218400 loss: 0.0029 lr: 0.02\n",
            "iteration: 218500 loss: 0.0034 lr: 0.02\n",
            "iteration: 218600 loss: 0.0030 lr: 0.02\n",
            "iteration: 218700 loss: 0.0035 lr: 0.02\n",
            "iteration: 218800 loss: 0.0029 lr: 0.02\n",
            "iteration: 218900 loss: 0.0032 lr: 0.02\n",
            "iteration: 219000 loss: 0.0029 lr: 0.02\n",
            "iteration: 219100 loss: 0.0034 lr: 0.02\n",
            "iteration: 219200 loss: 0.0033 lr: 0.02\n",
            "iteration: 219300 loss: 0.0033 lr: 0.02\n",
            "iteration: 219400 loss: 0.0028 lr: 0.02\n",
            "iteration: 219500 loss: 0.0036 lr: 0.02\n",
            "iteration: 219600 loss: 0.0038 lr: 0.02\n",
            "iteration: 219700 loss: 0.0033 lr: 0.02\n",
            "iteration: 219800 loss: 0.0031 lr: 0.02\n",
            "iteration: 219900 loss: 0.0031 lr: 0.02\n",
            "iteration: 220000 loss: 0.0033 lr: 0.02\n",
            "iteration: 220100 loss: 0.0029 lr: 0.02\n",
            "iteration: 220200 loss: 0.0035 lr: 0.02\n",
            "iteration: 220300 loss: 0.0031 lr: 0.02\n",
            "iteration: 220400 loss: 0.0030 lr: 0.02\n",
            "iteration: 220500 loss: 0.0031 lr: 0.02\n",
            "iteration: 220600 loss: 0.0031 lr: 0.02\n",
            "iteration: 220700 loss: 0.0035 lr: 0.02\n",
            "iteration: 220800 loss: 0.0031 lr: 0.02\n",
            "iteration: 220900 loss: 0.0029 lr: 0.02\n",
            "iteration: 221000 loss: 0.0034 lr: 0.02\n",
            "iteration: 221100 loss: 0.0034 lr: 0.02\n",
            "iteration: 221200 loss: 0.0036 lr: 0.02\n",
            "iteration: 221300 loss: 0.0035 lr: 0.02\n",
            "iteration: 221400 loss: 0.0031 lr: 0.02\n",
            "iteration: 221500 loss: 0.0030 lr: 0.02\n",
            "iteration: 221600 loss: 0.0030 lr: 0.02\n",
            "iteration: 221700 loss: 0.0036 lr: 0.02\n",
            "iteration: 221800 loss: 0.0032 lr: 0.02\n",
            "iteration: 221900 loss: 0.0030 lr: 0.02\n",
            "iteration: 222000 loss: 0.0035 lr: 0.02\n",
            "iteration: 222100 loss: 0.0038 lr: 0.02\n",
            "iteration: 222200 loss: 0.0034 lr: 0.02\n",
            "iteration: 222300 loss: 0.0026 lr: 0.02\n",
            "iteration: 222400 loss: 0.0034 lr: 0.02\n",
            "iteration: 222500 loss: 0.0031 lr: 0.02\n",
            "iteration: 222600 loss: 0.0030 lr: 0.02\n",
            "iteration: 222700 loss: 0.0034 lr: 0.02\n",
            "iteration: 222800 loss: 0.0026 lr: 0.02\n",
            "iteration: 222900 loss: 0.0031 lr: 0.02\n",
            "iteration: 223000 loss: 0.0030 lr: 0.02\n",
            "iteration: 223100 loss: 0.0033 lr: 0.02\n",
            "iteration: 223200 loss: 0.0035 lr: 0.02\n",
            "iteration: 223300 loss: 0.0033 lr: 0.02\n",
            "iteration: 223400 loss: 0.0027 lr: 0.02\n",
            "iteration: 223500 loss: 0.0029 lr: 0.02\n",
            "iteration: 223600 loss: 0.0029 lr: 0.02\n",
            "iteration: 223700 loss: 0.0031 lr: 0.02\n",
            "iteration: 223800 loss: 0.0026 lr: 0.02\n",
            "iteration: 223900 loss: 0.0027 lr: 0.02\n",
            "iteration: 224000 loss: 0.0030 lr: 0.02\n",
            "iteration: 224100 loss: 0.0032 lr: 0.02\n",
            "iteration: 224200 loss: 0.0036 lr: 0.02\n",
            "iteration: 224300 loss: 0.0032 lr: 0.02\n",
            "iteration: 224400 loss: 0.0035 lr: 0.02\n",
            "iteration: 224500 loss: 0.0033 lr: 0.02\n",
            "iteration: 224600 loss: 0.0030 lr: 0.02\n",
            "iteration: 224700 loss: 0.0031 lr: 0.02\n",
            "iteration: 224800 loss: 0.0031 lr: 0.02\n",
            "iteration: 224900 loss: 0.0031 lr: 0.02\n",
            "iteration: 225000 loss: 0.0028 lr: 0.02\n",
            "iteration: 225100 loss: 0.0034 lr: 0.02\n",
            "iteration: 225200 loss: 0.0035 lr: 0.02\n",
            "iteration: 225300 loss: 0.0033 lr: 0.02\n",
            "iteration: 225400 loss: 0.0028 lr: 0.02\n",
            "iteration: 225500 loss: 0.0029 lr: 0.02\n",
            "iteration: 225600 loss: 0.0031 lr: 0.02\n",
            "iteration: 225700 loss: 0.0034 lr: 0.02\n",
            "iteration: 225800 loss: 0.0037 lr: 0.02\n",
            "iteration: 225900 loss: 0.0033 lr: 0.02\n",
            "iteration: 226000 loss: 0.0032 lr: 0.02\n",
            "iteration: 226100 loss: 0.0032 lr: 0.02\n",
            "iteration: 226200 loss: 0.0035 lr: 0.02\n",
            "iteration: 226300 loss: 0.0032 lr: 0.02\n",
            "iteration: 226400 loss: 0.0037 lr: 0.02\n",
            "iteration: 226500 loss: 0.0033 lr: 0.02\n",
            "iteration: 226600 loss: 0.0028 lr: 0.02\n",
            "iteration: 226700 loss: 0.0031 lr: 0.02\n",
            "iteration: 226800 loss: 0.0033 lr: 0.02\n",
            "iteration: 226900 loss: 0.0032 lr: 0.02\n",
            "iteration: 227000 loss: 0.0030 lr: 0.02\n",
            "iteration: 227100 loss: 0.0029 lr: 0.02\n",
            "iteration: 227200 loss: 0.0036 lr: 0.02\n",
            "iteration: 227300 loss: 0.0034 lr: 0.02\n",
            "iteration: 227400 loss: 0.0038 lr: 0.02\n",
            "iteration: 227500 loss: 0.0033 lr: 0.02\n",
            "iteration: 227600 loss: 0.0031 lr: 0.02\n",
            "iteration: 227700 loss: 0.0031 lr: 0.02\n",
            "iteration: 227800 loss: 0.0034 lr: 0.02\n",
            "iteration: 227900 loss: 0.0031 lr: 0.02\n",
            "iteration: 228000 loss: 0.0029 lr: 0.02\n",
            "iteration: 228100 loss: 0.0034 lr: 0.02\n",
            "iteration: 228200 loss: 0.0031 lr: 0.02\n",
            "iteration: 228300 loss: 0.0028 lr: 0.02\n",
            "iteration: 228400 loss: 0.0025 lr: 0.02\n",
            "iteration: 228500 loss: 0.0033 lr: 0.02\n",
            "iteration: 228600 loss: 0.0031 lr: 0.02\n",
            "iteration: 228700 loss: 0.0033 lr: 0.02\n",
            "iteration: 228800 loss: 0.0033 lr: 0.02\n",
            "iteration: 228900 loss: 0.0034 lr: 0.02\n",
            "iteration: 229000 loss: 0.0031 lr: 0.02\n",
            "iteration: 229100 loss: 0.0034 lr: 0.02\n",
            "iteration: 229200 loss: 0.0034 lr: 0.02\n",
            "iteration: 229300 loss: 0.0036 lr: 0.02\n",
            "iteration: 229400 loss: 0.0030 lr: 0.02\n",
            "iteration: 229500 loss: 0.0033 lr: 0.02\n",
            "iteration: 229600 loss: 0.0027 lr: 0.02\n",
            "iteration: 229700 loss: 0.0033 lr: 0.02\n",
            "iteration: 229800 loss: 0.0032 lr: 0.02\n",
            "iteration: 229900 loss: 0.0031 lr: 0.02\n",
            "iteration: 230000 loss: 0.0028 lr: 0.02\n",
            "iteration: 230100 loss: 0.0028 lr: 0.02\n",
            "iteration: 230200 loss: 0.0029 lr: 0.02\n",
            "iteration: 230300 loss: 0.0040 lr: 0.02\n",
            "iteration: 230400 loss: 0.0034 lr: 0.02\n",
            "iteration: 230500 loss: 0.0037 lr: 0.02\n",
            "iteration: 230600 loss: 0.0033 lr: 0.02\n",
            "iteration: 230700 loss: 0.0030 lr: 0.02\n",
            "iteration: 230800 loss: 0.0030 lr: 0.02\n",
            "iteration: 230900 loss: 0.0030 lr: 0.02\n",
            "iteration: 231000 loss: 0.0028 lr: 0.02\n",
            "iteration: 231100 loss: 0.0030 lr: 0.02\n",
            "iteration: 231200 loss: 0.0029 lr: 0.02\n",
            "iteration: 231300 loss: 0.0034 lr: 0.02\n",
            "iteration: 231400 loss: 0.0037 lr: 0.02\n",
            "iteration: 231500 loss: 0.0029 lr: 0.02\n",
            "iteration: 231600 loss: 0.0031 lr: 0.02\n",
            "iteration: 231700 loss: 0.0031 lr: 0.02\n",
            "iteration: 231800 loss: 0.0030 lr: 0.02\n",
            "iteration: 231900 loss: 0.0030 lr: 0.02\n",
            "iteration: 232000 loss: 0.0029 lr: 0.02\n",
            "iteration: 232100 loss: 0.0031 lr: 0.02\n",
            "iteration: 232200 loss: 0.0033 lr: 0.02\n",
            "iteration: 232300 loss: 0.0034 lr: 0.02\n",
            "iteration: 232400 loss: 0.0033 lr: 0.02\n",
            "iteration: 232500 loss: 0.0035 lr: 0.02\n",
            "iteration: 232600 loss: 0.0031 lr: 0.02\n",
            "iteration: 232700 loss: 0.0041 lr: 0.02\n",
            "iteration: 232800 loss: 0.0037 lr: 0.02\n",
            "iteration: 232900 loss: 0.0033 lr: 0.02\n",
            "iteration: 233000 loss: 0.0033 lr: 0.02\n",
            "iteration: 233100 loss: 0.0030 lr: 0.02\n",
            "iteration: 233200 loss: 0.0030 lr: 0.02\n",
            "iteration: 233300 loss: 0.0034 lr: 0.02\n",
            "iteration: 233400 loss: 0.0032 lr: 0.02\n",
            "iteration: 233500 loss: 0.0033 lr: 0.02\n",
            "iteration: 233600 loss: 0.0035 lr: 0.02\n",
            "iteration: 233700 loss: 0.0034 lr: 0.02\n",
            "iteration: 233800 loss: 0.0035 lr: 0.02\n",
            "iteration: 233900 loss: 0.0029 lr: 0.02\n",
            "iteration: 234000 loss: 0.0034 lr: 0.02\n",
            "iteration: 234100 loss: 0.0030 lr: 0.02\n",
            "iteration: 234200 loss: 0.0036 lr: 0.02\n",
            "iteration: 234300 loss: 0.0032 lr: 0.02\n",
            "iteration: 234400 loss: 0.0031 lr: 0.02\n",
            "iteration: 234500 loss: 0.0028 lr: 0.02\n",
            "iteration: 234600 loss: 0.0034 lr: 0.02\n",
            "iteration: 234700 loss: 0.0028 lr: 0.02\n",
            "iteration: 234800 loss: 0.0031 lr: 0.02\n",
            "iteration: 234900 loss: 0.0028 lr: 0.02\n",
            "iteration: 235000 loss: 0.0039 lr: 0.02\n",
            "iteration: 235100 loss: 0.0030 lr: 0.02\n",
            "iteration: 235200 loss: 0.0033 lr: 0.02\n",
            "iteration: 235300 loss: 0.0037 lr: 0.02\n",
            "iteration: 235400 loss: 0.0030 lr: 0.02\n",
            "iteration: 235500 loss: 0.0029 lr: 0.02\n",
            "iteration: 235600 loss: 0.0032 lr: 0.02\n",
            "iteration: 235700 loss: 0.0032 lr: 0.02\n",
            "iteration: 235800 loss: 0.0027 lr: 0.02\n",
            "iteration: 235900 loss: 0.0035 lr: 0.02\n",
            "iteration: 236000 loss: 0.0034 lr: 0.02\n",
            "iteration: 236100 loss: 0.0031 lr: 0.02\n",
            "iteration: 236200 loss: 0.0029 lr: 0.02\n",
            "iteration: 236300 loss: 0.0032 lr: 0.02\n",
            "iteration: 236400 loss: 0.0029 lr: 0.02\n",
            "iteration: 236500 loss: 0.0029 lr: 0.02\n",
            "iteration: 236600 loss: 0.0028 lr: 0.02\n",
            "iteration: 236700 loss: 0.0034 lr: 0.02\n",
            "iteration: 236800 loss: 0.0031 lr: 0.02\n",
            "iteration: 236900 loss: 0.0036 lr: 0.02\n",
            "iteration: 237000 loss: 0.0031 lr: 0.02\n",
            "iteration: 237100 loss: 0.0030 lr: 0.02\n",
            "iteration: 237200 loss: 0.0033 lr: 0.02\n",
            "iteration: 237300 loss: 0.0031 lr: 0.02\n",
            "iteration: 237400 loss: 0.0026 lr: 0.02\n",
            "iteration: 237500 loss: 0.0029 lr: 0.02\n",
            "iteration: 237600 loss: 0.0033 lr: 0.02\n",
            "iteration: 237700 loss: 0.0035 lr: 0.02\n",
            "iteration: 237800 loss: 0.0034 lr: 0.02\n",
            "iteration: 237900 loss: 0.0041 lr: 0.02\n",
            "iteration: 238000 loss: 0.0033 lr: 0.02\n",
            "iteration: 238100 loss: 0.0034 lr: 0.02\n",
            "iteration: 238200 loss: 0.0028 lr: 0.02\n",
            "iteration: 238300 loss: 0.0032 lr: 0.02\n",
            "iteration: 238400 loss: 0.0035 lr: 0.02\n",
            "iteration: 238500 loss: 0.0033 lr: 0.02\n",
            "iteration: 238600 loss: 0.0032 lr: 0.02\n",
            "iteration: 238700 loss: 0.0027 lr: 0.02\n",
            "iteration: 238800 loss: 0.0038 lr: 0.02\n",
            "iteration: 238900 loss: 0.0031 lr: 0.02\n",
            "iteration: 239000 loss: 0.0030 lr: 0.02\n",
            "iteration: 239100 loss: 0.0032 lr: 0.02\n",
            "iteration: 239200 loss: 0.0030 lr: 0.02\n",
            "iteration: 239300 loss: 0.0035 lr: 0.02\n",
            "iteration: 239400 loss: 0.0029 lr: 0.02\n",
            "iteration: 239500 loss: 0.0028 lr: 0.02\n",
            "iteration: 239600 loss: 0.0036 lr: 0.02\n",
            "iteration: 239700 loss: 0.0032 lr: 0.02\n",
            "iteration: 239800 loss: 0.0029 lr: 0.02\n",
            "iteration: 239900 loss: 0.0032 lr: 0.02\n",
            "iteration: 240000 loss: 0.0028 lr: 0.02\n",
            "iteration: 240100 loss: 0.0030 lr: 0.02\n",
            "iteration: 240200 loss: 0.0030 lr: 0.02\n",
            "iteration: 240300 loss: 0.0035 lr: 0.02\n",
            "iteration: 240400 loss: 0.0029 lr: 0.02\n",
            "iteration: 240500 loss: 0.0035 lr: 0.02\n",
            "iteration: 240600 loss: 0.0026 lr: 0.02\n",
            "iteration: 240700 loss: 0.0032 lr: 0.02\n",
            "iteration: 240800 loss: 0.0032 lr: 0.02\n",
            "iteration: 240900 loss: 0.0032 lr: 0.02\n",
            "iteration: 241000 loss: 0.0032 lr: 0.02\n",
            "iteration: 241100 loss: 0.0032 lr: 0.02\n",
            "iteration: 241200 loss: 0.0029 lr: 0.02\n",
            "iteration: 241300 loss: 0.0030 lr: 0.02\n",
            "iteration: 241400 loss: 0.0027 lr: 0.02\n",
            "iteration: 241500 loss: 0.0036 lr: 0.02\n",
            "iteration: 241600 loss: 0.0033 lr: 0.02\n",
            "iteration: 241700 loss: 0.0033 lr: 0.02\n",
            "iteration: 241800 loss: 0.0031 lr: 0.02\n",
            "iteration: 241900 loss: 0.0026 lr: 0.02\n",
            "iteration: 242000 loss: 0.0031 lr: 0.02\n",
            "iteration: 242100 loss: 0.0027 lr: 0.02\n",
            "iteration: 242200 loss: 0.0036 lr: 0.02\n",
            "iteration: 242300 loss: 0.0029 lr: 0.02\n",
            "iteration: 242400 loss: 0.0032 lr: 0.02\n",
            "iteration: 242500 loss: 0.0029 lr: 0.02\n",
            "iteration: 242600 loss: 0.0033 lr: 0.02\n",
            "iteration: 242700 loss: 0.0029 lr: 0.02\n",
            "iteration: 242800 loss: 0.0034 lr: 0.02\n",
            "iteration: 242900 loss: 0.0033 lr: 0.02\n",
            "iteration: 243000 loss: 0.0030 lr: 0.02\n",
            "iteration: 243100 loss: 0.0031 lr: 0.02\n",
            "iteration: 243200 loss: 0.0029 lr: 0.02\n",
            "iteration: 243300 loss: 0.0037 lr: 0.02\n",
            "iteration: 243400 loss: 0.0029 lr: 0.02\n",
            "iteration: 243500 loss: 0.0029 lr: 0.02\n",
            "iteration: 243600 loss: 0.0034 lr: 0.02\n",
            "iteration: 243700 loss: 0.0031 lr: 0.02\n",
            "iteration: 243800 loss: 0.0032 lr: 0.02\n",
            "iteration: 243900 loss: 0.0026 lr: 0.02\n",
            "iteration: 244000 loss: 0.0029 lr: 0.02\n",
            "iteration: 244100 loss: 0.0031 lr: 0.02\n",
            "iteration: 244200 loss: 0.0031 lr: 0.02\n",
            "iteration: 244300 loss: 0.0030 lr: 0.02\n",
            "iteration: 244400 loss: 0.0036 lr: 0.02\n",
            "iteration: 244500 loss: 0.0029 lr: 0.02\n",
            "iteration: 244600 loss: 0.0030 lr: 0.02\n",
            "iteration: 244700 loss: 0.0032 lr: 0.02\n",
            "iteration: 244800 loss: 0.0031 lr: 0.02\n",
            "iteration: 244900 loss: 0.0030 lr: 0.02\n",
            "iteration: 245000 loss: 0.0033 lr: 0.02\n",
            "iteration: 245100 loss: 0.0032 lr: 0.02\n",
            "iteration: 245200 loss: 0.0030 lr: 0.02\n",
            "iteration: 245300 loss: 0.0029 lr: 0.02\n",
            "iteration: 245400 loss: 0.0028 lr: 0.02\n",
            "iteration: 245500 loss: 0.0028 lr: 0.02\n",
            "iteration: 245600 loss: 0.0034 lr: 0.02\n",
            "iteration: 245700 loss: 0.0029 lr: 0.02\n",
            "iteration: 245800 loss: 0.0030 lr: 0.02\n",
            "iteration: 245900 loss: 0.0035 lr: 0.02\n",
            "iteration: 246000 loss: 0.0034 lr: 0.02\n",
            "iteration: 246100 loss: 0.0028 lr: 0.02\n",
            "iteration: 246200 loss: 0.0032 lr: 0.02\n",
            "iteration: 246300 loss: 0.0031 lr: 0.02\n",
            "iteration: 246400 loss: 0.0031 lr: 0.02\n",
            "iteration: 246500 loss: 0.0030 lr: 0.02\n",
            "iteration: 246600 loss: 0.0029 lr: 0.02\n",
            "iteration: 246700 loss: 0.0028 lr: 0.02\n",
            "iteration: 246800 loss: 0.0025 lr: 0.02\n",
            "iteration: 246900 loss: 0.0030 lr: 0.02\n",
            "iteration: 247000 loss: 0.0031 lr: 0.02\n",
            "iteration: 247100 loss: 0.0029 lr: 0.02\n",
            "iteration: 247200 loss: 0.0031 lr: 0.02\n",
            "iteration: 247300 loss: 0.0031 lr: 0.02\n",
            "iteration: 247400 loss: 0.0032 lr: 0.02\n",
            "iteration: 247500 loss: 0.0032 lr: 0.02\n",
            "iteration: 247600 loss: 0.0032 lr: 0.02\n",
            "iteration: 247700 loss: 0.0028 lr: 0.02\n",
            "iteration: 247800 loss: 0.0034 lr: 0.02\n",
            "iteration: 247900 loss: 0.0033 lr: 0.02\n",
            "iteration: 248000 loss: 0.0029 lr: 0.02\n",
            "iteration: 248100 loss: 0.0031 lr: 0.02\n",
            "iteration: 248200 loss: 0.0034 lr: 0.02\n",
            "iteration: 248300 loss: 0.0035 lr: 0.02\n",
            "iteration: 248400 loss: 0.0037 lr: 0.02\n",
            "iteration: 248500 loss: 0.0030 lr: 0.02\n",
            "iteration: 248600 loss: 0.0028 lr: 0.02\n",
            "iteration: 248700 loss: 0.0029 lr: 0.02\n",
            "iteration: 248800 loss: 0.0029 lr: 0.02\n",
            "iteration: 248900 loss: 0.0030 lr: 0.02\n",
            "iteration: 249000 loss: 0.0029 lr: 0.02\n",
            "iteration: 249100 loss: 0.0032 lr: 0.02\n",
            "iteration: 249200 loss: 0.0032 lr: 0.02\n",
            "iteration: 249300 loss: 0.0031 lr: 0.02\n",
            "iteration: 249400 loss: 0.0029 lr: 0.02\n",
            "iteration: 249500 loss: 0.0034 lr: 0.02\n",
            "iteration: 249600 loss: 0.0041 lr: 0.02\n",
            "iteration: 249700 loss: 0.0028 lr: 0.02\n",
            "iteration: 249800 loss: 0.0028 lr: 0.02\n",
            "iteration: 249900 loss: 0.0028 lr: 0.02\n",
            "iteration: 250000 loss: 0.0037 lr: 0.02\n",
            "iteration: 250100 loss: 0.0027 lr: 0.02\n",
            "iteration: 250200 loss: 0.0028 lr: 0.02\n",
            "iteration: 250300 loss: 0.0030 lr: 0.02\n",
            "iteration: 250400 loss: 0.0034 lr: 0.02\n",
            "iteration: 250500 loss: 0.0029 lr: 0.02\n",
            "iteration: 250600 loss: 0.0029 lr: 0.02\n",
            "iteration: 250700 loss: 0.0028 lr: 0.02\n",
            "iteration: 250800 loss: 0.0032 lr: 0.02\n",
            "iteration: 250900 loss: 0.0029 lr: 0.02\n",
            "iteration: 251000 loss: 0.0029 lr: 0.02\n",
            "iteration: 251100 loss: 0.0033 lr: 0.02\n",
            "iteration: 251200 loss: 0.0027 lr: 0.02\n",
            "iteration: 251300 loss: 0.0029 lr: 0.02\n",
            "iteration: 251400 loss: 0.0028 lr: 0.02\n",
            "iteration: 251500 loss: 0.0031 lr: 0.02\n",
            "iteration: 251600 loss: 0.0031 lr: 0.02\n",
            "iteration: 251700 loss: 0.0031 lr: 0.02\n",
            "iteration: 251800 loss: 0.0034 lr: 0.02\n",
            "iteration: 251900 loss: 0.0035 lr: 0.02\n",
            "iteration: 252000 loss: 0.0031 lr: 0.02\n",
            "iteration: 252100 loss: 0.0030 lr: 0.02\n",
            "iteration: 252200 loss: 0.0030 lr: 0.02\n",
            "iteration: 252300 loss: 0.0029 lr: 0.02\n",
            "iteration: 252400 loss: 0.0028 lr: 0.02\n",
            "iteration: 252500 loss: 0.0029 lr: 0.02\n",
            "iteration: 252600 loss: 0.0027 lr: 0.02\n",
            "iteration: 252700 loss: 0.0029 lr: 0.02\n",
            "iteration: 252800 loss: 0.0028 lr: 0.02\n",
            "iteration: 252900 loss: 0.0029 lr: 0.02\n",
            "iteration: 253000 loss: 0.0031 lr: 0.02\n",
            "iteration: 253100 loss: 0.0035 lr: 0.02\n",
            "iteration: 253200 loss: 0.0030 lr: 0.02\n",
            "iteration: 253300 loss: 0.0031 lr: 0.02\n",
            "iteration: 253400 loss: 0.0032 lr: 0.02\n",
            "iteration: 253500 loss: 0.0029 lr: 0.02\n",
            "iteration: 253600 loss: 0.0036 lr: 0.02\n",
            "iteration: 253700 loss: 0.0031 lr: 0.02\n",
            "iteration: 253800 loss: 0.0032 lr: 0.02\n",
            "iteration: 253900 loss: 0.0028 lr: 0.02\n",
            "iteration: 254000 loss: 0.0030 lr: 0.02\n",
            "iteration: 254100 loss: 0.0034 lr: 0.02\n",
            "iteration: 254200 loss: 0.0032 lr: 0.02\n",
            "iteration: 254300 loss: 0.0029 lr: 0.02\n",
            "iteration: 254400 loss: 0.0028 lr: 0.02\n",
            "iteration: 254500 loss: 0.0028 lr: 0.02\n",
            "iteration: 254600 loss: 0.0027 lr: 0.02\n",
            "iteration: 254700 loss: 0.0028 lr: 0.02\n",
            "iteration: 254800 loss: 0.0031 lr: 0.02\n",
            "iteration: 254900 loss: 0.0027 lr: 0.02\n",
            "iteration: 255000 loss: 0.0030 lr: 0.02\n",
            "iteration: 255100 loss: 0.0029 lr: 0.02\n",
            "iteration: 255200 loss: 0.0031 lr: 0.02\n",
            "iteration: 255300 loss: 0.0028 lr: 0.02\n",
            "iteration: 255400 loss: 0.0030 lr: 0.02\n",
            "iteration: 255500 loss: 0.0028 lr: 0.02\n",
            "iteration: 255600 loss: 0.0029 lr: 0.02\n",
            "iteration: 255700 loss: 0.0031 lr: 0.02\n",
            "iteration: 255800 loss: 0.0030 lr: 0.02\n",
            "iteration: 255900 loss: 0.0030 lr: 0.02\n",
            "iteration: 256000 loss: 0.0030 lr: 0.02\n",
            "iteration: 256100 loss: 0.0039 lr: 0.02\n",
            "iteration: 256200 loss: 0.0030 lr: 0.02\n",
            "iteration: 256300 loss: 0.0032 lr: 0.02\n",
            "iteration: 256400 loss: 0.0032 lr: 0.02\n",
            "iteration: 256500 loss: 0.0030 lr: 0.02\n",
            "iteration: 256600 loss: 0.0031 lr: 0.02\n",
            "iteration: 256700 loss: 0.0031 lr: 0.02\n",
            "iteration: 256800 loss: 0.0030 lr: 0.02\n",
            "iteration: 256900 loss: 0.0029 lr: 0.02\n",
            "iteration: 257000 loss: 0.0028 lr: 0.02\n",
            "iteration: 257100 loss: 0.0035 lr: 0.02\n",
            "iteration: 257200 loss: 0.0032 lr: 0.02\n",
            "iteration: 257300 loss: 0.0026 lr: 0.02\n",
            "iteration: 257400 loss: 0.0031 lr: 0.02\n",
            "iteration: 257500 loss: 0.0030 lr: 0.02\n",
            "iteration: 257600 loss: 0.0033 lr: 0.02\n",
            "iteration: 257700 loss: 0.0030 lr: 0.02\n",
            "iteration: 257800 loss: 0.0041 lr: 0.02\n",
            "iteration: 257900 loss: 0.0033 lr: 0.02\n",
            "iteration: 258000 loss: 0.0030 lr: 0.02\n",
            "iteration: 258100 loss: 0.0031 lr: 0.02\n",
            "iteration: 258200 loss: 0.0030 lr: 0.02\n",
            "iteration: 258300 loss: 0.0029 lr: 0.02\n",
            "iteration: 258400 loss: 0.0030 lr: 0.02\n",
            "iteration: 258500 loss: 0.0034 lr: 0.02\n",
            "iteration: 258600 loss: 0.0027 lr: 0.02\n",
            "iteration: 258700 loss: 0.0030 lr: 0.02\n",
            "iteration: 258800 loss: 0.0034 lr: 0.02\n",
            "iteration: 258900 loss: 0.0029 lr: 0.02\n",
            "iteration: 259000 loss: 0.0032 lr: 0.02\n",
            "iteration: 259100 loss: 0.0030 lr: 0.02\n",
            "iteration: 259200 loss: 0.0033 lr: 0.02\n",
            "iteration: 259300 loss: 0.0029 lr: 0.02\n",
            "iteration: 259400 loss: 0.0032 lr: 0.02\n",
            "iteration: 259500 loss: 0.0032 lr: 0.02\n",
            "iteration: 259600 loss: 0.0026 lr: 0.02\n",
            "iteration: 259700 loss: 0.0027 lr: 0.02\n",
            "iteration: 259800 loss: 0.0023 lr: 0.02\n",
            "iteration: 259900 loss: 0.0028 lr: 0.02\n",
            "iteration: 260000 loss: 0.0030 lr: 0.02\n",
            "iteration: 260100 loss: 0.0032 lr: 0.02\n",
            "iteration: 260200 loss: 0.0032 lr: 0.02\n",
            "iteration: 260300 loss: 0.0026 lr: 0.02\n",
            "iteration: 260400 loss: 0.0031 lr: 0.02\n",
            "iteration: 260500 loss: 0.0028 lr: 0.02\n",
            "iteration: 260600 loss: 0.0026 lr: 0.02\n",
            "iteration: 260700 loss: 0.0027 lr: 0.02\n",
            "iteration: 260800 loss: 0.0029 lr: 0.02\n",
            "iteration: 260900 loss: 0.0028 lr: 0.02\n",
            "iteration: 261000 loss: 0.0033 lr: 0.02\n",
            "iteration: 261100 loss: 0.0036 lr: 0.02\n",
            "iteration: 261200 loss: 0.0032 lr: 0.02\n",
            "iteration: 261300 loss: 0.0031 lr: 0.02\n",
            "iteration: 261400 loss: 0.0027 lr: 0.02\n",
            "iteration: 261500 loss: 0.0030 lr: 0.02\n",
            "iteration: 261600 loss: 0.0031 lr: 0.02\n",
            "iteration: 261700 loss: 0.0031 lr: 0.02\n",
            "iteration: 261800 loss: 0.0028 lr: 0.02\n",
            "iteration: 261900 loss: 0.0028 lr: 0.02\n",
            "iteration: 262000 loss: 0.0030 lr: 0.02\n",
            "iteration: 262100 loss: 0.0034 lr: 0.02\n",
            "iteration: 262200 loss: 0.0029 lr: 0.02\n",
            "iteration: 262300 loss: 0.0034 lr: 0.02\n",
            "iteration: 262400 loss: 0.0030 lr: 0.02\n",
            "iteration: 262500 loss: 0.0030 lr: 0.02\n",
            "iteration: 262600 loss: 0.0027 lr: 0.02\n",
            "iteration: 262700 loss: 0.0029 lr: 0.02\n",
            "iteration: 262800 loss: 0.0028 lr: 0.02\n",
            "iteration: 262900 loss: 0.0030 lr: 0.02\n",
            "iteration: 263000 loss: 0.0035 lr: 0.02\n",
            "iteration: 263100 loss: 0.0029 lr: 0.02\n",
            "iteration: 263200 loss: 0.0029 lr: 0.02\n",
            "iteration: 263300 loss: 0.0029 lr: 0.02\n",
            "iteration: 263400 loss: 0.0030 lr: 0.02\n",
            "iteration: 263500 loss: 0.0030 lr: 0.02\n",
            "iteration: 263600 loss: 0.0031 lr: 0.02\n",
            "iteration: 263700 loss: 0.0031 lr: 0.02\n",
            "iteration: 263800 loss: 0.0033 lr: 0.02\n",
            "iteration: 263900 loss: 0.0029 lr: 0.02\n",
            "iteration: 264000 loss: 0.0027 lr: 0.02\n",
            "iteration: 264100 loss: 0.0029 lr: 0.02\n",
            "iteration: 264200 loss: 0.0032 lr: 0.02\n",
            "iteration: 264300 loss: 0.0033 lr: 0.02\n",
            "iteration: 264400 loss: 0.0033 lr: 0.02\n",
            "iteration: 264500 loss: 0.0033 lr: 0.02\n",
            "iteration: 264600 loss: 0.0028 lr: 0.02\n",
            "iteration: 264700 loss: 0.0029 lr: 0.02\n",
            "iteration: 264800 loss: 0.0027 lr: 0.02\n",
            "iteration: 264900 loss: 0.0030 lr: 0.02\n",
            "iteration: 265000 loss: 0.0033 lr: 0.02\n",
            "iteration: 265100 loss: 0.0034 lr: 0.02\n",
            "iteration: 265200 loss: 0.0030 lr: 0.02\n",
            "iteration: 265300 loss: 0.0029 lr: 0.02\n",
            "iteration: 265400 loss: 0.0033 lr: 0.02\n",
            "iteration: 265500 loss: 0.0032 lr: 0.02\n",
            "iteration: 265600 loss: 0.0028 lr: 0.02\n",
            "iteration: 265700 loss: 0.0031 lr: 0.02\n",
            "iteration: 265800 loss: 0.0031 lr: 0.02\n",
            "iteration: 265900 loss: 0.0036 lr: 0.02\n",
            "iteration: 266000 loss: 0.0028 lr: 0.02\n",
            "iteration: 266100 loss: 0.0023 lr: 0.02\n",
            "iteration: 266200 loss: 0.0032 lr: 0.02\n",
            "iteration: 266300 loss: 0.0026 lr: 0.02\n",
            "iteration: 266400 loss: 0.0028 lr: 0.02\n",
            "iteration: 266500 loss: 0.0035 lr: 0.02\n",
            "iteration: 266600 loss: 0.0035 lr: 0.02\n",
            "iteration: 266700 loss: 0.0032 lr: 0.02\n",
            "iteration: 266800 loss: 0.0028 lr: 0.02\n",
            "iteration: 266900 loss: 0.0029 lr: 0.02\n",
            "iteration: 267000 loss: 0.0027 lr: 0.02\n",
            "iteration: 267100 loss: 0.0026 lr: 0.02\n",
            "iteration: 267200 loss: 0.0028 lr: 0.02\n",
            "iteration: 267300 loss: 0.0028 lr: 0.02\n",
            "iteration: 267400 loss: 0.0029 lr: 0.02\n",
            "iteration: 267500 loss: 0.0036 lr: 0.02\n",
            "iteration: 267600 loss: 0.0031 lr: 0.02\n",
            "iteration: 267700 loss: 0.0030 lr: 0.02\n",
            "iteration: 267800 loss: 0.0029 lr: 0.02\n",
            "iteration: 267900 loss: 0.0030 lr: 0.02\n",
            "iteration: 268000 loss: 0.0030 lr: 0.02\n",
            "iteration: 268100 loss: 0.0031 lr: 0.02\n",
            "iteration: 268200 loss: 0.0030 lr: 0.02\n",
            "iteration: 268300 loss: 0.0028 lr: 0.02\n",
            "iteration: 268400 loss: 0.0033 lr: 0.02\n",
            "iteration: 268500 loss: 0.0030 lr: 0.02\n",
            "iteration: 268600 loss: 0.0034 lr: 0.02\n",
            "iteration: 268700 loss: 0.0031 lr: 0.02\n",
            "iteration: 268800 loss: 0.0031 lr: 0.02\n",
            "iteration: 268900 loss: 0.0025 lr: 0.02\n",
            "iteration: 269000 loss: 0.0029 lr: 0.02\n",
            "iteration: 269100 loss: 0.0035 lr: 0.02\n",
            "iteration: 269200 loss: 0.0031 lr: 0.02\n",
            "iteration: 269300 loss: 0.0026 lr: 0.02\n",
            "iteration: 269400 loss: 0.0032 lr: 0.02\n",
            "iteration: 269500 loss: 0.0030 lr: 0.02\n",
            "iteration: 269600 loss: 0.0030 lr: 0.02\n",
            "iteration: 269700 loss: 0.0031 lr: 0.02\n",
            "iteration: 269800 loss: 0.0027 lr: 0.02\n",
            "iteration: 269900 loss: 0.0030 lr: 0.02\n",
            "iteration: 270000 loss: 0.0028 lr: 0.02\n",
            "iteration: 270100 loss: 0.0031 lr: 0.02\n",
            "iteration: 270200 loss: 0.0031 lr: 0.02\n",
            "iteration: 270300 loss: 0.0033 lr: 0.02\n",
            "iteration: 270400 loss: 0.0030 lr: 0.02\n",
            "iteration: 270500 loss: 0.0032 lr: 0.02\n",
            "iteration: 270600 loss: 0.0030 lr: 0.02\n",
            "iteration: 270700 loss: 0.0028 lr: 0.02\n",
            "iteration: 270800 loss: 0.0027 lr: 0.02\n",
            "iteration: 270900 loss: 0.0029 lr: 0.02\n",
            "iteration: 271000 loss: 0.0028 lr: 0.02\n",
            "iteration: 271100 loss: 0.0028 lr: 0.02\n",
            "iteration: 271200 loss: 0.0029 lr: 0.02\n",
            "iteration: 271300 loss: 0.0028 lr: 0.02\n",
            "iteration: 271400 loss: 0.0032 lr: 0.02\n",
            "iteration: 271500 loss: 0.0026 lr: 0.02\n",
            "iteration: 271600 loss: 0.0031 lr: 0.02\n",
            "iteration: 271700 loss: 0.0033 lr: 0.02\n",
            "iteration: 271800 loss: 0.0034 lr: 0.02\n",
            "iteration: 271900 loss: 0.0028 lr: 0.02\n",
            "iteration: 272000 loss: 0.0027 lr: 0.02\n",
            "iteration: 272100 loss: 0.0027 lr: 0.02\n",
            "iteration: 272200 loss: 0.0029 lr: 0.02\n",
            "iteration: 272300 loss: 0.0031 lr: 0.02\n",
            "iteration: 272400 loss: 0.0028 lr: 0.02\n",
            "iteration: 272500 loss: 0.0024 lr: 0.02\n",
            "iteration: 272600 loss: 0.0029 lr: 0.02\n",
            "iteration: 272700 loss: 0.0030 lr: 0.02\n",
            "iteration: 272800 loss: 0.0029 lr: 0.02\n",
            "iteration: 272900 loss: 0.0029 lr: 0.02\n",
            "iteration: 273000 loss: 0.0029 lr: 0.02\n",
            "iteration: 273100 loss: 0.0029 lr: 0.02\n",
            "iteration: 273200 loss: 0.0030 lr: 0.02\n",
            "iteration: 273300 loss: 0.0028 lr: 0.02\n",
            "iteration: 273400 loss: 0.0028 lr: 0.02\n",
            "iteration: 273500 loss: 0.0029 lr: 0.02\n",
            "iteration: 273600 loss: 0.0030 lr: 0.02\n",
            "iteration: 273700 loss: 0.0031 lr: 0.02\n",
            "iteration: 273800 loss: 0.0029 lr: 0.02\n",
            "iteration: 273900 loss: 0.0027 lr: 0.02\n",
            "iteration: 274000 loss: 0.0031 lr: 0.02\n",
            "iteration: 274100 loss: 0.0030 lr: 0.02\n",
            "iteration: 274200 loss: 0.0036 lr: 0.02\n",
            "iteration: 274300 loss: 0.0028 lr: 0.02\n",
            "iteration: 274400 loss: 0.0031 lr: 0.02\n",
            "iteration: 274500 loss: 0.0027 lr: 0.02\n",
            "iteration: 274600 loss: 0.0028 lr: 0.02\n",
            "iteration: 274700 loss: 0.0031 lr: 0.02\n",
            "iteration: 274800 loss: 0.0024 lr: 0.02\n",
            "iteration: 274900 loss: 0.0026 lr: 0.02\n",
            "iteration: 275000 loss: 0.0032 lr: 0.02\n",
            "iteration: 275100 loss: 0.0034 lr: 0.02\n",
            "iteration: 275200 loss: 0.0028 lr: 0.02\n",
            "iteration: 275300 loss: 0.0031 lr: 0.02\n",
            "iteration: 275400 loss: 0.0029 lr: 0.02\n",
            "iteration: 275500 loss: 0.0028 lr: 0.02\n",
            "iteration: 275600 loss: 0.0032 lr: 0.02\n",
            "iteration: 275700 loss: 0.0026 lr: 0.02\n",
            "iteration: 275800 loss: 0.0033 lr: 0.02\n",
            "iteration: 275900 loss: 0.0027 lr: 0.02\n",
            "iteration: 276000 loss: 0.0030 lr: 0.02\n",
            "iteration: 276100 loss: 0.0031 lr: 0.02\n",
            "iteration: 276200 loss: 0.0028 lr: 0.02\n",
            "iteration: 276300 loss: 0.0027 lr: 0.02\n",
            "iteration: 276400 loss: 0.0031 lr: 0.02\n",
            "iteration: 276500 loss: 0.0027 lr: 0.02\n",
            "iteration: 276600 loss: 0.0029 lr: 0.02\n",
            "iteration: 276700 loss: 0.0029 lr: 0.02\n",
            "iteration: 276800 loss: 0.0028 lr: 0.02\n",
            "iteration: 276900 loss: 0.0029 lr: 0.02\n",
            "iteration: 277000 loss: 0.0028 lr: 0.02\n",
            "iteration: 277100 loss: 0.0035 lr: 0.02\n",
            "iteration: 277200 loss: 0.0030 lr: 0.02\n",
            "iteration: 277300 loss: 0.0028 lr: 0.02\n",
            "iteration: 277400 loss: 0.0032 lr: 0.02\n",
            "iteration: 277500 loss: 0.0028 lr: 0.02\n",
            "iteration: 277600 loss: 0.0027 lr: 0.02\n",
            "iteration: 277700 loss: 0.0031 lr: 0.02\n",
            "iteration: 277800 loss: 0.0027 lr: 0.02\n",
            "iteration: 277900 loss: 0.0030 lr: 0.02\n",
            "iteration: 278000 loss: 0.0027 lr: 0.02\n",
            "iteration: 278100 loss: 0.0027 lr: 0.02\n",
            "iteration: 278200 loss: 0.0031 lr: 0.02\n",
            "iteration: 278300 loss: 0.0028 lr: 0.02\n",
            "iteration: 278400 loss: 0.0035 lr: 0.02\n",
            "iteration: 278500 loss: 0.0034 lr: 0.02\n",
            "iteration: 278600 loss: 0.0030 lr: 0.02\n",
            "iteration: 278700 loss: 0.0028 lr: 0.02\n",
            "iteration: 278800 loss: 0.0033 lr: 0.02\n",
            "iteration: 278900 loss: 0.0031 lr: 0.02\n",
            "iteration: 279000 loss: 0.0025 lr: 0.02\n",
            "iteration: 279100 loss: 0.0026 lr: 0.02\n",
            "iteration: 279200 loss: 0.0027 lr: 0.02\n",
            "iteration: 279300 loss: 0.0030 lr: 0.02\n",
            "iteration: 279400 loss: 0.0024 lr: 0.02\n",
            "iteration: 279500 loss: 0.0029 lr: 0.02\n",
            "iteration: 279600 loss: 0.0030 lr: 0.02\n",
            "iteration: 279700 loss: 0.0028 lr: 0.02\n",
            "iteration: 279800 loss: 0.0029 lr: 0.02\n",
            "iteration: 279900 loss: 0.0025 lr: 0.02\n",
            "iteration: 280000 loss: 0.0026 lr: 0.02\n",
            "iteration: 280100 loss: 0.0028 lr: 0.02\n",
            "iteration: 280200 loss: 0.0028 lr: 0.02\n",
            "iteration: 280300 loss: 0.0027 lr: 0.02\n",
            "iteration: 280400 loss: 0.0033 lr: 0.02\n",
            "iteration: 280500 loss: 0.0030 lr: 0.02\n",
            "iteration: 280600 loss: 0.0030 lr: 0.02\n",
            "iteration: 280700 loss: 0.0024 lr: 0.02\n",
            "iteration: 280800 loss: 0.0029 lr: 0.02\n",
            "iteration: 280900 loss: 0.0032 lr: 0.02\n",
            "iteration: 281000 loss: 0.0027 lr: 0.02\n",
            "iteration: 281100 loss: 0.0031 lr: 0.02\n",
            "iteration: 281200 loss: 0.0030 lr: 0.02\n",
            "iteration: 281300 loss: 0.0028 lr: 0.02\n",
            "iteration: 281400 loss: 0.0028 lr: 0.02\n",
            "iteration: 281500 loss: 0.0030 lr: 0.02\n",
            "iteration: 281600 loss: 0.0030 lr: 0.02\n",
            "iteration: 281700 loss: 0.0031 lr: 0.02\n",
            "iteration: 281800 loss: 0.0029 lr: 0.02\n",
            "iteration: 281900 loss: 0.0032 lr: 0.02\n",
            "iteration: 282000 loss: 0.0032 lr: 0.02\n",
            "iteration: 282100 loss: 0.0027 lr: 0.02\n",
            "iteration: 282200 loss: 0.0029 lr: 0.02\n",
            "iteration: 282300 loss: 0.0030 lr: 0.02\n",
            "iteration: 282400 loss: 0.0027 lr: 0.02\n",
            "iteration: 282500 loss: 0.0025 lr: 0.02\n",
            "iteration: 282600 loss: 0.0025 lr: 0.02\n",
            "iteration: 282700 loss: 0.0037 lr: 0.02\n",
            "iteration: 282800 loss: 0.0030 lr: 0.02\n",
            "iteration: 282900 loss: 0.0028 lr: 0.02\n",
            "iteration: 283000 loss: 0.0032 lr: 0.02\n",
            "iteration: 283100 loss: 0.0030 lr: 0.02\n",
            "iteration: 283200 loss: 0.0027 lr: 0.02\n",
            "iteration: 283300 loss: 0.0028 lr: 0.02\n",
            "iteration: 283400 loss: 0.0029 lr: 0.02\n",
            "iteration: 283500 loss: 0.0029 lr: 0.02\n",
            "iteration: 283600 loss: 0.0028 lr: 0.02\n",
            "iteration: 283700 loss: 0.0028 lr: 0.02\n",
            "iteration: 283800 loss: 0.0031 lr: 0.02\n",
            "iteration: 283900 loss: 0.0026 lr: 0.02\n",
            "iteration: 284000 loss: 0.0029 lr: 0.02\n",
            "iteration: 284100 loss: 0.0033 lr: 0.02\n",
            "iteration: 284200 loss: 0.0026 lr: 0.02\n",
            "iteration: 284300 loss: 0.0029 lr: 0.02\n",
            "iteration: 284400 loss: 0.0029 lr: 0.02\n",
            "iteration: 284500 loss: 0.0030 lr: 0.02\n",
            "iteration: 284600 loss: 0.0027 lr: 0.02\n",
            "iteration: 284700 loss: 0.0030 lr: 0.02\n",
            "iteration: 284800 loss: 0.0031 lr: 0.02\n",
            "iteration: 284900 loss: 0.0030 lr: 0.02\n",
            "iteration: 285000 loss: 0.0027 lr: 0.02\n",
            "iteration: 285100 loss: 0.0027 lr: 0.02\n",
            "iteration: 285200 loss: 0.0026 lr: 0.02\n",
            "iteration: 285300 loss: 0.0028 lr: 0.02\n",
            "iteration: 285400 loss: 0.0030 lr: 0.02\n",
            "iteration: 285500 loss: 0.0028 lr: 0.02\n",
            "iteration: 285600 loss: 0.0027 lr: 0.02\n",
            "iteration: 285700 loss: 0.0029 lr: 0.02\n",
            "iteration: 285800 loss: 0.0026 lr: 0.02\n",
            "iteration: 285900 loss: 0.0034 lr: 0.02\n",
            "iteration: 286000 loss: 0.0028 lr: 0.02\n",
            "iteration: 286100 loss: 0.0030 lr: 0.02\n",
            "iteration: 286200 loss: 0.0025 lr: 0.02\n",
            "iteration: 286300 loss: 0.0033 lr: 0.02\n",
            "iteration: 286400 loss: 0.0029 lr: 0.02\n",
            "iteration: 286500 loss: 0.0026 lr: 0.02\n",
            "iteration: 286600 loss: 0.0038 lr: 0.02\n",
            "iteration: 286700 loss: 0.0029 lr: 0.02\n",
            "iteration: 286800 loss: 0.0032 lr: 0.02\n",
            "iteration: 286900 loss: 0.0028 lr: 0.02\n",
            "iteration: 287000 loss: 0.0029 lr: 0.02\n",
            "iteration: 287100 loss: 0.0029 lr: 0.02\n",
            "iteration: 287200 loss: 0.0027 lr: 0.02\n",
            "iteration: 287300 loss: 0.0030 lr: 0.02\n",
            "iteration: 287400 loss: 0.0027 lr: 0.02\n",
            "iteration: 287500 loss: 0.0031 lr: 0.02\n",
            "iteration: 287600 loss: 0.0029 lr: 0.02\n",
            "iteration: 287700 loss: 0.0032 lr: 0.02\n",
            "iteration: 287800 loss: 0.0032 lr: 0.02\n",
            "iteration: 287900 loss: 0.0038 lr: 0.02\n",
            "iteration: 288000 loss: 0.0036 lr: 0.02\n",
            "iteration: 288100 loss: 0.0039 lr: 0.02\n",
            "iteration: 288200 loss: 0.0029 lr: 0.02\n",
            "iteration: 288300 loss: 0.0031 lr: 0.02\n",
            "iteration: 288400 loss: 0.0029 lr: 0.02\n",
            "iteration: 288500 loss: 0.0030 lr: 0.02\n",
            "iteration: 288600 loss: 0.0034 lr: 0.02\n",
            "iteration: 288700 loss: 0.0031 lr: 0.02\n",
            "iteration: 288800 loss: 0.0028 lr: 0.02\n",
            "iteration: 288900 loss: 0.0032 lr: 0.02\n",
            "iteration: 289000 loss: 0.0029 lr: 0.02\n",
            "iteration: 289100 loss: 0.0026 lr: 0.02\n",
            "iteration: 289200 loss: 0.0033 lr: 0.02\n",
            "iteration: 289300 loss: 0.0025 lr: 0.02\n",
            "iteration: 289400 loss: 0.0027 lr: 0.02\n",
            "iteration: 289500 loss: 0.0028 lr: 0.02\n",
            "iteration: 289600 loss: 0.0026 lr: 0.02\n",
            "iteration: 289700 loss: 0.0032 lr: 0.02\n",
            "iteration: 289800 loss: 0.0032 lr: 0.02\n",
            "iteration: 289900 loss: 0.0032 lr: 0.02\n",
            "iteration: 290000 loss: 0.0033 lr: 0.02\n",
            "iteration: 290100 loss: 0.0026 lr: 0.02\n",
            "iteration: 290200 loss: 0.0029 lr: 0.02\n",
            "iteration: 290300 loss: 0.0033 lr: 0.02\n",
            "iteration: 290400 loss: 0.0026 lr: 0.02\n",
            "iteration: 290500 loss: 0.0029 lr: 0.02\n",
            "iteration: 290600 loss: 0.0029 lr: 0.02\n",
            "iteration: 290700 loss: 0.0025 lr: 0.02\n",
            "iteration: 290800 loss: 0.0025 lr: 0.02\n",
            "iteration: 290900 loss: 0.0034 lr: 0.02\n",
            "iteration: 291000 loss: 0.0024 lr: 0.02\n",
            "iteration: 291100 loss: 0.0033 lr: 0.02\n",
            "iteration: 291200 loss: 0.0028 lr: 0.02\n",
            "iteration: 291300 loss: 0.0028 lr: 0.02\n",
            "iteration: 291400 loss: 0.0028 lr: 0.02\n",
            "iteration: 291500 loss: 0.0033 lr: 0.02\n",
            "iteration: 291600 loss: 0.0031 lr: 0.02\n",
            "iteration: 291700 loss: 0.0035 lr: 0.02\n",
            "iteration: 291800 loss: 0.0029 lr: 0.02\n",
            "iteration: 291900 loss: 0.0025 lr: 0.02\n",
            "iteration: 292000 loss: 0.0029 lr: 0.02\n",
            "iteration: 292100 loss: 0.0029 lr: 0.02\n",
            "iteration: 292200 loss: 0.0032 lr: 0.02\n",
            "iteration: 292300 loss: 0.0025 lr: 0.02\n",
            "iteration: 292400 loss: 0.0031 lr: 0.02\n",
            "iteration: 292500 loss: 0.0031 lr: 0.02\n",
            "iteration: 292600 loss: 0.0030 lr: 0.02\n",
            "iteration: 292700 loss: 0.0032 lr: 0.02\n",
            "iteration: 292800 loss: 0.0029 lr: 0.02\n",
            "iteration: 292900 loss: 0.0032 lr: 0.02\n",
            "iteration: 293000 loss: 0.0029 lr: 0.02\n",
            "iteration: 293100 loss: 0.0029 lr: 0.02\n",
            "iteration: 293200 loss: 0.0031 lr: 0.02\n",
            "iteration: 293300 loss: 0.0031 lr: 0.02\n",
            "iteration: 293400 loss: 0.0028 lr: 0.02\n",
            "iteration: 293500 loss: 0.0034 lr: 0.02\n",
            "iteration: 293600 loss: 0.0027 lr: 0.02\n",
            "iteration: 293700 loss: 0.0029 lr: 0.02\n",
            "iteration: 293800 loss: 0.0029 lr: 0.02\n",
            "iteration: 293900 loss: 0.0026 lr: 0.02\n",
            "iteration: 294000 loss: 0.0026 lr: 0.02\n",
            "iteration: 294100 loss: 0.0031 lr: 0.02\n",
            "iteration: 294200 loss: 0.0024 lr: 0.02\n",
            "iteration: 294300 loss: 0.0026 lr: 0.02\n",
            "iteration: 294400 loss: 0.0025 lr: 0.02\n",
            "iteration: 294500 loss: 0.0027 lr: 0.02\n",
            "iteration: 294600 loss: 0.0028 lr: 0.02\n",
            "iteration: 294700 loss: 0.0029 lr: 0.02\n",
            "iteration: 294800 loss: 0.0027 lr: 0.02\n",
            "iteration: 294900 loss: 0.0027 lr: 0.02\n",
            "iteration: 295000 loss: 0.0029 lr: 0.02\n",
            "iteration: 295100 loss: 0.0031 lr: 0.02\n",
            "iteration: 295200 loss: 0.0028 lr: 0.02\n",
            "iteration: 295300 loss: 0.0026 lr: 0.02\n",
            "iteration: 295400 loss: 0.0031 lr: 0.02\n",
            "iteration: 295500 loss: 0.0031 lr: 0.02\n",
            "iteration: 295600 loss: 0.0026 lr: 0.02\n",
            "iteration: 295700 loss: 0.0029 lr: 0.02\n",
            "iteration: 295800 loss: 0.0030 lr: 0.02\n",
            "iteration: 295900 loss: 0.0030 lr: 0.02\n",
            "iteration: 296000 loss: 0.0029 lr: 0.02\n",
            "iteration: 296100 loss: 0.0029 lr: 0.02\n",
            "iteration: 296200 loss: 0.0024 lr: 0.02\n",
            "iteration: 296300 loss: 0.0029 lr: 0.02\n",
            "iteration: 296400 loss: 0.0028 lr: 0.02\n",
            "iteration: 296500 loss: 0.0031 lr: 0.02\n",
            "iteration: 296600 loss: 0.0035 lr: 0.02\n",
            "iteration: 296700 loss: 0.0027 lr: 0.02\n",
            "iteration: 296800 loss: 0.0025 lr: 0.02\n",
            "iteration: 296900 loss: 0.0031 lr: 0.02\n",
            "iteration: 297000 loss: 0.0028 lr: 0.02\n",
            "iteration: 297100 loss: 0.0029 lr: 0.02\n",
            "iteration: 297200 loss: 0.0035 lr: 0.02\n",
            "iteration: 297300 loss: 0.0030 lr: 0.02\n",
            "iteration: 297400 loss: 0.0031 lr: 0.02\n",
            "iteration: 297500 loss: 0.0027 lr: 0.02\n",
            "iteration: 297600 loss: 0.0033 lr: 0.02\n",
            "iteration: 297700 loss: 0.0028 lr: 0.02\n",
            "iteration: 297800 loss: 0.0032 lr: 0.02\n",
            "iteration: 297900 loss: 0.0026 lr: 0.02\n",
            "iteration: 298000 loss: 0.0028 lr: 0.02\n",
            "iteration: 298100 loss: 0.0031 lr: 0.02\n",
            "iteration: 298200 loss: 0.0027 lr: 0.02\n",
            "iteration: 298300 loss: 0.0029 lr: 0.02\n",
            "iteration: 298400 loss: 0.0032 lr: 0.02\n",
            "iteration: 298500 loss: 0.0033 lr: 0.02\n",
            "iteration: 298600 loss: 0.0029 lr: 0.02\n",
            "iteration: 298700 loss: 0.0026 lr: 0.02\n",
            "iteration: 298800 loss: 0.0028 lr: 0.02\n",
            "iteration: 298900 loss: 0.0030 lr: 0.02\n",
            "iteration: 299000 loss: 0.0031 lr: 0.02\n",
            "iteration: 299100 loss: 0.0028 lr: 0.02\n",
            "iteration: 299200 loss: 0.0025 lr: 0.02\n",
            "iteration: 299300 loss: 0.0031 lr: 0.02\n",
            "iteration: 299400 loss: 0.0028 lr: 0.02\n",
            "iteration: 299500 loss: 0.0033 lr: 0.02\n",
            "iteration: 299600 loss: 0.0034 lr: 0.02\n",
            "iteration: 299700 loss: 0.0034 lr: 0.02\n",
            "iteration: 299800 loss: 0.0028 lr: 0.02\n",
            "iteration: 299900 loss: 0.0029 lr: 0.02\n",
            "iteration: 300000 loss: 0.0025 lr: 0.02\n",
            "iteration: 300100 loss: 0.0028 lr: 0.02\n",
            "iteration: 300200 loss: 0.0027 lr: 0.02\n",
            "iteration: 300300 loss: 0.0030 lr: 0.02\n",
            "iteration: 300400 loss: 0.0026 lr: 0.02\n",
            "iteration: 300500 loss: 0.0026 lr: 0.02\n",
            "iteration: 300600 loss: 0.0031 lr: 0.02\n",
            "iteration: 300700 loss: 0.0028 lr: 0.02\n",
            "iteration: 300800 loss: 0.0028 lr: 0.02\n",
            "iteration: 300900 loss: 0.0030 lr: 0.02\n",
            "iteration: 301000 loss: 0.0030 lr: 0.02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8ef3aacc5628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )  # pass on path and file name for pose_cfg.yaml!\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         [_, loss_val, summary] = sess.run(\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         )\n\u001b[1;32m    277\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "BShmX3QhKuyT",
        "outputId": "cebfade3-b314-477f-e09f-add80dd71814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_mobnet_100_largewithaugDec20shuffle1_301000  with # of training iterations: 301000\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "256it [01:08,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-301000\n",
            "Results for 301000  training iterations: 80 1 train error: 4.26 pixels. Test error: 21.22  pixels.\n",
            "With pcutoff of 0.6  train error: 4.26 pixels. Test error: 18.8 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "Plotting...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support. ' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option);\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        // select the cell after this one\n",
              "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
              "        IPython.notebook.select(index + 1);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div id='63816495-7acf-4030-b5af-e6fa5dda6556'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 256/256 [03:10<00:00,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#300k\n",
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mu6AJsWLbsf",
        "outputId": "2093b158-989c-4ce6-f16a-0d0164fc04d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_mobnet_100_largewithaugDec20shuffle1_300000  with # of training iterations: 300000\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "256it [00:19, 13.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-300000\n",
            "Results for 300000  training iterations: 80 1 train error: 3.82 pixels. Test error: 25.01  pixels.\n",
            "With pcutoff of 0.6  train error: 3.82 pixels. Test error: 18.35 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#200k\n",
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-iDqO_OLlv7",
        "outputId": "4d80e54c-992d-4e4f-bead-16a53dea539b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_mobnet_100_largewithaugDec20shuffle1_200000  with # of training iterations: 200000\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "256it [00:19, 13.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-200000\n",
            "Results for 200000  training iterations: 80 1 train error: 5.15 pixels. Test error: 28.15  pixels.\n",
            "With pcutoff of 0.6  train error: 5.1 pixels. Test error: 26.44 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#300,0400\n",
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItAfWKStNztH",
        "outputId": "cc401537-5e82-4d7a-966b-dc4dafaf6c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_mobnet_100_largewithaugDec20shuffle1_300400  with # of training iterations: 300400\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "256it [00:19, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-300400\n",
            "Results for 300400  training iterations: 80 1 train error: 3.91 pixels. Test error: 22.38  pixels.\n",
            "With pcutoff of 0.6  train error: 3.91 pixels. Test error: 18.86 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#100k\n",
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQWf8f9sLiqu",
        "outputId": "8cdd521e-ff2c-4d45-89bf-60b432d0bcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4]],\n",
            " 'all_joints_names': ['center', 'head', 'tail', 'leftleg', 'rightleg'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_largewithaugDec20/largewithaug_amir80shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'mobilenet_v2_1.0',\n",
            " 'num_joints': 5,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/MyDrive/largewithaug-amir-2021-12-20/dlc-models/iteration-0/largewithaugDec20-trainset80shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_mobnet_100_largewithaugDec20shuffle1_100000  with # of training iterations: 100000\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "256it [00:19, 13.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-100000\n",
            "Results for 100000  training iterations: 80 1 train error: 9.45 pixels. Test error: 40.53  pixels.\n",
            "With pcutoff of 0.6  train error: 8.22 pixels. Test error: 32.54 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        }
      ]
    }
  ]
}